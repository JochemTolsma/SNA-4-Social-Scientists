[["index.html", "SNASS Social Network Analysis for Social Scientists Welcome About the authors", " SNASS Social Network Analysis for Social Scientists 2021-09-23 Welcome This course is not an introduction to Sociology, Social Networks or Social Network Analysis. It is also not a course in R. This course will teach you how to apply a social network perspective to your own research on topics like inequality, cohesion and diversity. Individuals (or social agents more generally) are interconnected in social networks. This has theoretical consequences, methodological consequences and consequences for data collection and data wrangling. There are many different types of networks. Networks may be very small (e.g. couples) and very large (e.g. citation networks), they may be centered around a specific person (an egonet) or capture many individuals bounded in a specific context (a socionet). You may wish to understand how networks come about (i.e. the upstream causes) or want to understand their impact on individuals, groups and society (i.e. the downstream consequences). This is why I tried to structure this course along three different dimensions: Types of Social Network: dyads, egonets, and socionets; Causes or Consequences of social networks; Implications of applying a social network perspective: for Theory, Data and Methodology. What will you learn in this course? You will learn that many of the societal problems that the social sciences address  including, but not limited to, research questions rooted in anthropology, communication science, development studies, economics, organisation science, political science, and sociology  can and should often be studied from a social network perspective. In addition, you will systematically learn (how) to learn more about social network theories, data, and methods. Enjoy!! About the authors Jochem Tolsma (www.jochemtolsma.nl) is professor by special appointment of Social Divisions between Groups in the Department of Sociology at the University of Groningen, associate professor in the Sociology Department of Radboud University Nijmegen and the program director of the Research Master Social and Cultural Science of Radboud University. In his research he focuses on social divisions between groups; the interplay of segregation, inequality and polarization. His particular areas of interest are the causes and consequences of segregation in neighborhoods, schools and friendship networks. He enjoys teaching Social Networks to students and finds it highly rewarding to see students applying a social network perspecive in their further academic careers. So, what did students have to say about previous versions of this course? Students want me to focus more on theory and on methods and on social network data collection and on social netwtork data wrangling skills. Many (research master and PdD) students find the course content simply too difficult to master within a 7-10 weeks lecture series. At the same time, students still miss essential social network topics for social scientists in the course. Enter, this book! Together we will decide what we will focus on. You may decide for yourself if and how many background literature you will study. Do you miss a topic? Just make a pull request! ;-) Bas Hofstra (www.bashofstra.com) is assistant professor at Radboud Universitys Department of Sociology. His work orbits the study of diversity, stratification, and innovation. It captures longitudinal systems of social and cultural exchange: from the gestation and birth of networks, careers, ideas, or innovations, to their use, up until their eventual cessation. As such, his work strives for three interrelated goals: (i) answering substantive questions on causes and effects of social networks, while (ii) contributing to social theory, and (iii) using computational methods and big data. You can find his research in (among others) in PNAS, American Sociological Review, Social Forces, Social Networks, and Nature Human Behaviour. He teaches courses on contemporary sociological theories, analytical sociology, and, of course, social networks. The many advantages and the analytical strength of the social network perspective render it very useful to apply in a range of social science research themes and topics: from studying social exchange of workers between organizations, to the role of mentorship in career outcomes, to a teams social structure and creative success. Therefore, Bas finds it both necessary and inspiring to teach social science students about the social network perspective. The book is hands-on and feet-in-the-mud on the key aspects of social networks that social scientists would need to know about: from questions, to theory, to methods. Therefore, anyone should be able to start their own thesis or research project on social networks after theyve read selected parts or chapters in the book. The online version of this book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["intro.html", "Chapter 1 Introduction 1.1 Overview 1.2 Social Network Perspective 1.3 Aim 1.4 Structure 1.5 What makes this course stand out? 1.6 Assignment", " Chapter 1 Introduction 1.1 Overview Sociologists study how societies affect the lives of their members and, vice versa, how individuals shape the societies in which they live. Within societies people form and break relations with specific others and thereby form social networks. Individuals are embedded in many different social networks (e.g. based on friendships, bullying, family relations, coworking, and so forth). Within these social networks individuals influence each others attitudes, behaviors and relations via complex and dynamic processes. The attitudes, behaviors and relations of individuals shape, in turn, the societies they live in. They give rise to social phenomena, the collective outcomes caused by human behavior such as inequality, segregation, polarization and cohesion. It is difficult to explain macro-level social phenomena, let alone to solve many of the urgent social problems, without taking into account social networks. For example, researchers may be able to deduce hypotheses from an established theory on political opinions of individuals but the same theory is not able to explain when and where political polarization in a society occurs.1 To explain when and where political polarization occurs, we need the analytical toolbox of the social network perspective. It is thus unsurprising that the study of the causes and consequences of social networks lies at the core of sociology. Nor is it a surprise that the social network perspective now populates many other disciplines too  from computer science, to physics, to organization science. A social network is an finite set of actors and the relation(s) defined on this set. The actors are social entities (people, organizations, countries, etc.) whose specific ties (friendship, competition, collaboration, etc.), constitute the network. (Wasserman and Faust 1994 : 20). Networks are also called graphs or sociograms. Actors are also called points, nodes, agents or vertices. Relations are also called ties, edges or connections. Social Networks is/are no theory nor a method. Social networks are social phenomena with causes and consequences. The size, composition, structure, and evolution of social networks can be explained. Social networks have an impact on individuals (e.g. attitudes and behavior), dyads (e.g. relations), institutions (e.g. operational efficiency) and societies (e.g. inequality, segregation, or polarization). In other words, acknowledging the dynamics of social networks is crucial to correctly explain social phenomena and understand (and solve) social problems. The social network perspective is a systematic way of thinking to do so. 1.2 Social Network Perspective Social Network Perspective It is the acknowledgment that agents are embedded within social networks  no man is an island  and that this has theoretical, data, and methodological consequences. It systematically integrates theory, data, and methods according to this acknowledgment  i.e. wielding the full analytical toolbox pertaining to social networks  to rigorously explain social phenomona and social problems. Theoretical consequences: A social network perspective forces us to rethink existing hypotheses and may lead to new research questions and predictions on the causes and consequences of social networks. And by extension, it also forces us to rethink questions and hypotheses about social prolems such as segregation or polarization. Data consequences: To apply a Social Network Perspective, we need social network data. Social network data are special because we need not only information on the social agents (e.g. people, organizations) but also on the ties (or relations) between them. As a consequence, there are many unique practical and ethical aspects related to the collection of network data. The structure of social network data is also different (e.g. adjacency matrices, edge lists) and require distinct data wrangling skills compared to, say, traditional survey data. Methodological consequences: A social network perspective will make explicit that empirical observations of individuals are not always independent. The (sometimes complex) interdependencies between observations that result from social networks have consequences for many of our traditional research methods which often assume independence between observations. This has led to the development and adoption of new social network analyses techniques. In sum, taking a social network perspective leads to key theoretical, data collection, and methodological contributions. The combination and integration of these three allows for a better understanding of social phenomena and society. 1.3 Aim The intended learning outcomes of the course are: Theoretical knowledge and insight: you will be able to define core concepts related to a social network perspective. You will be able to summarize what a social network perspective in social science research entails. Academic attitude: you develop a positive attitude towards applying a social network perspective in social science research. Research skills: you will be able to apply a social network perspective in social science research. This encompasses that: you will be able to deduce relevant and new social network hypotheses from existing theories. you will be able able to collect and wrangle social network data. you will be able to test these hypotheses with different social network analysis techniques. 1.4 Structure This course is structured along three different dimensions: Type of Social Network: dyad, egonet, socionet (complete network) Causes or Consequences of social networks Implications of applying a social network perspective: Theoretical, Data and Methodological Table 1.1: Topics discussed within the course Type of Social Network Causes or Consequences Implications Dyads Causes Theory Dyads Causes Method Dyads Consequences Theory Dyads Consequences Method Dyads NA Data Egonets Causes Theory Egonets Causes Method Egonets Consequences Theory Egonets Consequences Method Egonets NA Data Socionet Causes Theory Socionet Causes Method Socionet Consequences Theory Socionet Consequences Method Socionet NA Data Feel free to use the menu on the left and to jump to the section you are most interested in. But there is a clear order in the sections. The best way to accumulate theoretical and methodological knowledge, and to gain the necessary R-skills to successfully apply a social network perspective to your own research is by going through the sections one by. Below I will discuss three dimensions in a little bit more detail. 1.4.1 Type of Social Network Dyads The smallest possible social network is a network between two persons (or, more precisely, between two social agents). A network between two persons is also called a dyad. In the clip below I will introduce you to the the main concepts involved in a dyad. Naturally, the same concepts also play a role in larger social networks. For slides, see here. After having watched the video you should be able to: give a definition of a dyad. explain what is meant by time-varying and time-constant actor attributes and dyad attributes. explain that relations between ego and alter can be classified based on whether relations are directed or undirected and on the level of measurement of the relation (i.e. nominal, ordinal, interval, ratio). be familiar with al the synonyms for networks, agents and relations. provide examples of dyads, and the relations between ego and alter. Egonets We could define an egocentric social network as a set of actors that all have relationships with ego. This definition is quite similar to Marsdens (Marsden 1990) definition: Sets of ties surrounding sampled individual units. To illustrate what is meant by these definitions, let us us consider the following world. And visualize the best-friend-forever relationships in this world. And now sample a random person. The person we sampled, ego, is made red and square in our network. Let us zoom in a little bit. 1.0 degree Lets suppose we had asked this person to name all its best-friend-forevers (BFFs). If we would assume that BFF relations are undirected, then this persons egocentric or 1.0 degree network would look like this: The alters with whom ego is not connected are not part of the egocentric network. And generally, if we collect data we do not have any information on these unconnected alters. 1.5 degree network We may have asked ego - with the question below - whether its BFFs are also BFFs of one another. Please think about the relations between the people you just mentioned. Some of them may be total strangers in the sense that they wouldnt recognize each other if they bumped into each other on the street. Others may be especially close, as close or closer to each other as they are to you. Are they especially close? PROBE: As close or closer to each other as they are to you? It turns out that two alters in egos BFF-network are also BFFs of each other. If we know the relations between the alters in a 1.0 degree network it becomes a 1.5 degree network. See below: In the lower-left corner we see a closed Triad. For more information on Triads jump to this section 5.1. 2.0 degree network Perhaps, instead of asking whether there are BFF relations between the BFFs of ego, we could also have used a snowball sampling method and interviewed the alters (or BFFs) of ego. Note that the focal actor (initial sampled unit) remains ego. Thus, if we would have asked egos alters to name their BFFs, we would have discovered the following 2.0 degree network: The newly discovered alters are in light blue. Naturally, we also observe the BFF relation between egos alters appearing. Please note that in a 2.0 degree network the number of alters within the 1.0 degree network of ego will remain the same (assuming that ego did not forget to mention a BFF). 2.X degree network Any ideas about what a 2.5 degree network would look like. To be honest, I dont. Do these networks include all relations between all nodes, or only the relations between the alters in the separate 1.0 degree networks? Perhaps we could call the latter a 2.5 degree network and the former a 2.75 degree network. 1.4.1.1 Socionets {-} A complete, full, or sociocentric network is a network within a sampled context or foci of which we know all nodes and all connections between nodes. The boundaries of the network are thus a priori defined and the contexts in which nodes are present are the sampled units. We may for example sample a classroom, neighborhood, university or country and collect all relations between all nodes within this context. You now have come across a general definition for social networks and specific definitions for dyadic, egocentric and sociocentric social networks. You also know that the social agents within the networks may not necessarily have to be persons but can also be companies, or political parties for example. A network in which the relations between two different type of nodes are present are called multiple-mode networks. Similarly, between one type of node (e.g. persons) we may have information on more than one type of relation. These networks are called multiplex networks. The networks we have considered so far refer to networks of binary relations (yes/no). If the relations can vary in strength, we call the networks a weighted network. Thus, we can have a two-mode, multiplex, weighted, directed network but also a single-mode, uniplex, binary, undirected network. It turns out that many (complete) social networks share certain network characteristics. This is called the small-world phenomena and is discussed in more detail here. But let us start with a teaser. Suppose we live in a world, called Smallworld, of 105 persons (a small world indeed) and have information on all friendship (or trust) relations between its citizens. How could such a network look like? It turns out that (large) social networks of positive relations often have a specific structure. And this structure is called A Small World. Lets have a look at the small world structure of SmallWorld. Play with the small world network of Smallworld. Zoom in and out, turn it around and click on some nodes. How would you describe the structure of the network in Smallworld. Well, I would describe it as a network with a: (1) relatively low density; (2) relatively high degree of clustering and (3) a relatively low average degree of separation (or path length). These three characteristics are defining features of small world networks. But what does density, clustering and path length mean?, and what do we mean with relatively, that does not sound very scientific does it?! Dont you worry, you will learn this during the course. 1.4.2 Causes and Consequences or Selection and Influence Social networks consist of social relations between people. For example friendships, bullying relations or working-together-during-the-course-social-networks relations. The continuous process of making and breaking social relations is also called selection. And the reasons why we make and brake relations with specific persons are important causes for the structures of the social networks we observe. Thus the causes of social networks are strongly related to selection processes. The people with whom we form social relations also influence our attitudes, behaviors and future relations. How we think, behave and with whom we make, break or maintain social relations is for an important part the consequence of the social networks in which we are embedded. Thus the consequences of social networks are strongly related to influence processes (but also to selection processes). Selection and influence processes are firmly entangled. See below for an example. The shapes represent social agents (e.g. individuals). The shape (circle or square) of the social agent is a time-stable characteristic (e.g. sex). The fill of the shape (no fill, pattern fill, solid fill) is a time-varying characteristic (e.g. music taste). The line between the shapes (no line, dashed, solid) signifies the strength of the relationship (e.g. romantic relationship) We could call the selection process Opposite Attracts and we could call the influence process Circle beats square. Let us focus on the selection process first. If there is no tie between the agents, you will notice that a tie will be formed (becomes stronger) between dissimilar agents. When a tie is present between agents, you will notice that a tie will be broken (becomes weaker) when agents are similar with respect to their time-varying characteristic. Let us now focus on the influence process. When a strong tie is present, you will notice that the square agents will assimilate to the time-varying characteristic of the circle agents. We do not know of course the mechanism behind this assimilation (or influence) process. Is it the square who (voluntary) adopts the behavior of the circle or does the circle forces the square to follow suit? 1.4.3 Implications Individuals - or social agents more generally (e.g. institutions, societies) - are part of many different social networks. 1.4.3.1 Theoretical implications A social network perspective may force to a rethinking of existing hypotheses and may lead to new research questions on the causes and consequences of social networks. That individuals are interconnected and hence that observations are (cor)related and characteristics of these observations co-vary is theoretically interesting.  It gives rise to a complete new type of research questions. Where normally our research questions refer to describing or explaining the variance between individuals (e.g. why individuals differ) a new set of research questions describe, compare and explain the covariance between individuals. For example, why connected people are more (dis)similar to one another than non-connected others. The social network functions as explanans in these questions. A different set of new research questions takes the social network itself as explanandum. These research questions aim to describe, compare and explain characteristics of social networks (e.g. with respect to size, composition, structure and evolution). 1.4.3.2 Methodological implications That individuals are interconnected and hence that observations are not independent can be seen as a nuisance. That our observations are not independent violates many assumptions of analysis techniques that social scientists commonly apply (e.g. OLS). In order to correctly estimate the effects of interest we need to take these interdependencies into account with social network analysis techniques. 1.4.3.3 Data implications If we want to collect data on social networks, we need not only information on specific focal social agents (egos) but also on its relations with other egos (egos alters) and information on characteristics of these alters. Is it ethical that we collect data on an alter via ego? What do we do if in a school class a pupil does not wish to participate in a study on bullying? Do we delete this child from our bullying networks altogether, thus both as sender and receiver of bullying relations? Collecting data on social networks gives rise to specific ethical issues.  If we want to collect data on complete social networks, the sampling unit is no longer an individual but the context in which these ties occur. How do we determine the boundaries of this context and how do we sample these. Collecting network data comes with specific sampling issues.  Network data also has a specific format (e.g. an adjacency matrix) and can become very big. How to wrangle these network data objects? Network data oftentimes lead to specific practical data wrangling issues. 1.5 What makes this course stand out? This course is not an introduction to Sociology, Social Networks or Social Network Analysis. It is also not a course in R. I assume you have some intuitive understanding of what social networks are and have opened R or RStudio at one point in your life but both are not necessary prerequisites to follow this course. There are very good (online) introductions to the study of Social Networks, see for example here and here. A good short introduction to R for this course can be found in Appendix A or have a look here for a more thorough introduction to Data Science with R. This course is tailored for research master and PhD students. I will assume that you are interested in and have studied social problems in your academic career and that your research interests fits one of the following broad themes: inequality, cohesion and diversity. For an excellent introduction into sociology see (Van Tubergen 2020). However, up to this point, when you deduced hypotheses from existing theories you did not explicitly acknowledge that individuals are interconnected in social networks. You were neither aware that this may have theoretical consequences for existing hypotheses nor that this gives rise to new research questions. Up to this point, when you tested your hypotheses you assumed that your observations were independent (e.g. OLS) or, at most, that the nonindependence was relatively easy to take into account (e.g. multi-level analysis). In this course you will learn to apply a social network perspective in the study of inequality, cohesion and diversity. You will become able to deduce more precise and new hypotheses. You will become able to test these hypotheses with social network techniques that take into account and explain complex interdependencies. Enjoy!! 1.6 Assignment Time to practice. You will make this assignment twice. The first time at the very beginning of the course and the second time at the very end. Go back to your latest paper, assignment or theses in which you investigated a research topic fitting the broad research themes of inequality, cohesion and diversity. You could also take one of your current projects you are working on if you like. Identify the social problem. Please describe what a naive approach would be to take on this social problem. Thus, being oblivious of a social network perspective. Please try to identify how your project could be improved by taking a social network perspective. Make sure to discuss the theoretical, data, and methodological consequences. Can you discover a trend in the existing literature with respect to taking a social network perspective. Prepare a short presentation (5-10 minutes) for the next class. But dear sir, I have not worked on any social problems in my academic career so far. Well, are you sure you want to follow this course? Yes, really? Okay. Start from scratch then. Formulate a research question related to a social problem and start from there. References "],["theory.html", "Chapter 2 Theory 2.1 Network structures (DYAD) 2.2 Causes of dyads 2.3 Consequences of dyads 2.4 Assignment", " Chapter 2 Theory The smallest possible social network is a network between two persons (or, more precisely, between two social agents). A network between two persons is also called a dyad. In the clip below I will introduce you to the the main concepts involved in a dyad. Naturally, the same concepts also play a role in larger social networks. For slides, see here. After having watched the video you should be able to: give a definition of a dyad. explain what is meant by time-varying and time-constant actor attributes and dyad attributes. explain that relations between ego and alter can be classified based on whether relations are directed or undirected and on the level of measurement of the relation (i.e. nominal, ordinal, interval, ratio). be familiar with al the synonyms for networks, agents and relations. provide examples of dyads, and the relations between ego and alter. 2.1 Network structures (DYAD) Dyads are the smallest possible networks. For undirected relations there are two possible dyad configurations. See Figure 2.1. Figure 2.1: undirected dyad configurations There are four options for directed dayds. See Figure 2.2. However, two configurations (the two in the middle row) are isomorphic. We cannot distinguish them in the network. From now on, I wont draw all isomorphs. Figure 2.2: undirected dyad configurations When there is a tie from i to j and vice versa, we call this a reciprocated dyad. I hope you see that the reasons for me to start or break a relation with you (e.g. a helping relation) if you dont have a relation with me, may be different from the reasons for me to start or break a relation with you if you already have a relation with me. If we consider two types of undirected relations simultaneously, we have three distinct configurations. See Figure 2.3. Figure 2.3: multiplex, undirected dyad configurations Explanations for multiplex configuration are multicomplex. A first step to consider is whether there is a necessary, or likely, order in the relations. To illustrate. I first became friends with my wife and afterwards she became my wife. Although this situations hopefully holds true for most married couples, it is not logically necessary of course. A second step is to consider the valence of the relations. Are they positive or negative. It is not likely that I would name my friend a foe and vice versa but there is a thin line between love and hate. Feelings of hate and love for the same person may even coexist. Ever heard someone saying: I cant live with him/her but I cant live without him/her either? Lets have a look at the possible configuration of directed dyads with two types of relations (Figure 2.4) Figure 2.4: multiplex, undirected dyad configurations Can you come up with an explanation for each configuration? Well you should if you study multiple type of relations between (two) social agents! But let us start with a single undirected tie: marriage. 2.2 Causes of dyads An important research topic within sociology is assortative mating (or intermarriage) (see: Kalmijn 1998; Schwartz 2013; Blossfeld 2009). Scholars in this field try to explain two things: At the meso-level (i.e. the couple): why two people in an exclusive relationship like marriage (or cohabition or best friends) are more similar to one another with respect to defining characteristics (e.g. social class, ethnicity) than two random persons. At the macro-level (i.e. society): why societies differ in the degree of assortative mating.   Assortative mating is a special case of homophily. Assortative mating is an important topic within sociology because it is, next to social mobility, an important indicator of the openness of society.  Selection and influence processes are important reasons why partners (ego and alter) within a dyad are more similar than two random persons. We may prefer to marry someone who is similar to us on key social dimensions, share our attitudes and opinions and show similar behavior. Once married we may influence each other and assimilate to one another. A third reason why we observe homophily within couples is that partners are likely to have shared and will share the same social context. With shared social context we mean the shared social and physical environment and shared life experiences. The environment pre-marriage may in part determine characteristics of the pool of potential marriage partners (i.e. the choice set). For example when neighborhoods and schools are segregated along ethnic and educational division lines, the potential marriage partners we meet are likely to be more similar to us than a random person in society at large. The shared social environment post-marriage may exert a similar influence on both partners, consider for example economic recessions effects on different geographic regions. This may impact the job opportunities for both partners similarly (assuming they live in the same house). An example of shared life experiences, would be having children. Please note that causes and consequences of homophily are closely related. A shared social context and partner preferences may predict (or cause) homophily within dyads. But once a dyad is formed, a consequence of this relationship may be that partnes become more similar over time, as a result of a shared environment, influence and (de)selection processes. If you want to disentangle these processes, it is necessarry to have information on the degree of homophily between potential marriage partners would they be randomly assigned to one another, the degree of homophily at the beginning of the union, the degree of homophily within couples after a specific time period. homophily Homophily is the principle that a contact between similar people occurs at a higher rate than among dissimilar people (McPherson, Smith-Lovin, and Cook 2001). You will also come across the terms: - baseline or structural homophily: this is the degree of homophily we observe simply as a result of the composition of the total choice set, the people with whom we can, in principle, form a relationship. - inbreeding homophily: this is the degree of homophily we observe over and above the level of baseline homophily. This may be caused by taste homophily and differences in resources and restrictions (other than set by the total choice set). - taste or choice homophily: the extent of homophily induced by personal preferences. openness of society The openness of society refers to the level of inequality of opportunities within society. The social problem of inequality consists of two sub-problems. The first refers to inequality in outcomes: the unequal distribution of resources (e.g. economic, cultural, social, knowledge, power). The second refers to inquality in opportunities, the association between specific individual or group charactersitics and the likelihood to obtain these resources. Here the opennes of society is clearly linked to the second sub-problem. Where in questions of (inter-generational) social mobility the association between social position of parents and the social position of children is assessed, within the literature on intermarriage the association between the social positions of the two spouses is assessed. Both questions or associations will tell you something about the strenght of class/social position boundaries. 2.2.1 General Theoretical Framework In this section, I would like to introduce a General Theoretical Framework (or micro-macro model) which can be used to explain more or less any social phenomena you are interested in. The GTF can thus also be used to explain the emergence of social networks, and thus also to explain the emergence of dyads, and thus also to explain educational intermarriage. For slides, see here. After having watched the video and after heaving read this page, you should be able to: Understand and summarize the building blocks of the multi-level framework which can be used to explain the emergence of social networks. macro-level (independent) variable(s) social conditions restrictions bridge assumptions (also called social context effects) Theory of Action preferences resources choice-set choice-input choice-output Transformation rules (also called aggregation mechanism) social interdependencies unintended/unforeseen consequences of micro-level behavior macro-level (dependent) variable Provide examples of all building blocks in the context of explaining the emergence of dyads For more background reading on the multi-level framework (aka Coleman-boat, Coleman-bathtub, micro-macro models) (see: Coleman 1994; Raub, Buskens, and Van Assen 2011, especially paragraph 4.4). The GTF is a framework, not a theory from which you can deduce hypotheses. Before we can do that, we need to fill in the blanks. That is, we need to make the social contexts (bridge assumptions) explicit. We need a Theory of Action. We need to think of the interdependencies and how they impact the aggregation mechanism. So, lets get started 2.2.2 Social context effects Characteristics of the social context in which people are embedded (the marco- and meso-level) may impact peoples preferences and resources. Example 1: The level of economic inequality impacts how financial resources are unequally distributed across educational groups within society. Hypo1: In countries with more wealth inequality, the difference between educational groups in economic resources is larger. Example 2: Societal norms may impact your own views and opinions and thus preferences. Hypo2: In countries with more equal gender norms, mens (womens) preferences for a partner with a higher education are stronger (weaker). 2.2.3 The special role of restrictions Restrictions or constraints also refer to macro-level characteristics but restrictions do not directly impact preferences and resources (i.e. choice input) but instead influence, or constrain how these preferences and resources lead to choice-output; we have a constrained choice model. Restrictions - in studies on the emergence of social networks - impact the choice-set, the relevant choice-options that a person has. If I would like to marry a grizzly bear but if there are no grizzly bears around which I can marry, I cannot act upon my preferences (commonly the example is about Eskimos but that may be considered more politically incorrect). This is called a structural restriction. A more realistic example would be the distribution of educational degrees within society, which depend on educational expansion and inequality of educational opportunities. Next, to structural restrictions we may also have normative restrictions, the formal and informal rules of institutions. A formal normative restriction would be a law that forbids me to marry a grizzly bear. An informal normative restriction would be a social norm, e.g. my parents who disapprove of my preference to marry a grizzly bear. Please note that social norms may thus impact my preferences directly (a social context effect) and indirectly (act as a restriction). Example1: Preferences for a partner with a similar educational level are more likely to lead to educational homogamy, if educational degrees are more evenly distributed across men and women. In the literature on resources you will see that restrictions are also commonly understood as the absence of resources. I am a stubborn scientists and DO NOT FOLLOW THIS TERMINOLOGY and neither should you. 2.2.4 Theory of Action Persons have preferences for a partner with a specific educational-level. Commonly, people prefer higher-educated partners (because of instrumental motives) and people have homomphilic preferences. Preferences may differ between persons with different educational levels and between men and women. Persons also have resources (i.e. economic, cultural, cognitive, social resources) that may affect the search behavior of persons. Example1: persons with more economic resources have more options to meet different people and may thus select a partner from a larger choice-set. Hypo1: persons with more economic resources are more likely to marry a partner that meet their preferences, i.e. a more similar partner. We would like to apply the GTF to explain the emergence of social networks. The networks we observe are the result of people making and breaking social relations. Consequently, a theory of action to explain decision about social relations should explain not only decisions about making new relations (i.e. selection) but also about decision whether or not to maintain or break existing relations (i.e. deselection). When we talk about selection processess, we implicitly mean both selection and deselection.  Concretely, if we want to explain the degree of intermarriage within society, we need to take into account both who is marrying whom and who is divorcing whom! Consider the following example. For some people the saying opposite attracts may hold true and they may be unaware of or ignore the social norm not to intermarry. But once married the couple may face unanticipated sanctions of violating the social norm, they may be ostracized. Being faced with this unanticipated consequence of their marriage decision, the couple may subsequently decide to divorce. In this example, the social norm thus not influences the selection process (more precisely, does not moderate the impact of preferences on marriage decisions) but it does influence the deselection process.  Our Theory of Action assumes a cost benefit evaluation of some sort, in line with Rational Action Theory. However, social scientists view on humans rationality is different than the view of classical economists. Social scientists speak of restricted or bounded rationality (i.e. a weak rationality assumption); people are not always able to have or process all relevant information to make accurate and correct cost-benefit evaluations. We make questimates about the costs involved in our decision and about the likelihood that our behavior will yield the desired goal. Within sociology, actors goals are not only economic, monatory goals. Actors goals can be physical and social goals (i.e. health, happiness, avoidance of downwards mobility). For a nice paper on Rational Action Theory for Sociologists, see Goldthorpe (1998). 2.2.5 Transformation rules We now almost have all ingredients to explain (or predict) the degree of intermarriage in society. We only need the aggregation mechanisms: the micro-to-macro link. We thereby need to know the macro-level (intended and unintended) consequences of individual actions. That is, we need to know how the marriage market functions.  Let us assume the following: Someone takes the initiative. This is determined by chance. The initial choice-set is formed by 5 random partners of the opposite sex (no assumptions about search behavior). Possible partners who are already married are removed from the initial choice-set. The possible partners that remain constitute the (final) choice set. Persons may prefer a partner with a higher education. These preferences may differ between educational levels and between the sexes. Persons choose a partner from their choice set (not marrying is not an option). Possible partners with a higher education have a larger chance to be chosen. How important a partners education is, depends on the preference of the one taking the initiative. The persons who is being proposed to always accepts. We observe no divorces. Resources do not play any role (e.g the higher educated do not have a larger choice set) Educational degrees are either high or low. With the above marriage-market model we have a limited number of ingredients that impact the observed degree of educational intermarriage within society: gender composition within society the distribution of educational degrees in society preferences the number of marriage proposals I hope you see that marriage choices are interdependent. If I marry person A, you no longer can marry person A. These interdependencies make it difficult to predict the macro-level dependent variable, degree of educational homogamy.  Given the market model above, can you predict who will marry whom? 2.2.6 Agent Based Modelling Well, I can not. You may be a mathematical wizard and able to find a closed solution by some algebra. Another option could be to to make a simplified model and try to simulate the macro-level outcome based purely on our micro-level theory of action and the rules of the marriage market. We call this Agent-Based-Modelling (see e.g. Macal 2016). I programmed a simply ABM based on the above. There are some parameters in the model which you can change. Suppose %_men=50 : We have an equal gender distribution in society (50% men, 50% women; range: 1-99). %_men_EducHigh=50: 50% of our male population is higher educated and 50% is lower educated (range: 1-99). %_women_EducHigh=50: 50% of our female population is higher educated and 50% is lower educated (range: 1-99). pref_men_EH=0: Higher educated men do not have any preference with respect to the educational level of their partner. (range: 0-10) pref_men_EL=0: Lower educated men do not have any preference with respect to the educational level of their partner. (range: 0-10) pref_women_EH=0: Higher educated women do not have any preference with respect to the educational level of their partner. (range: 0-10) pref_women_EL=0: Lower educated women do not have any preference with respect to the educational level of their partner. (range: 0-10) Can you make a guess about the resulting degree of eductional homogamy?? Press Update! to see if you were correct. Play around with the (agent-based simulation) model below. Go to app here 2.3 Consequences of dyads Assortative mating, or more generally mating, has consequences at different levels. At the individual- or spouse-level, it may impact, for example, political opinions, happiness and working hours, and not necessarily in a similar way for both spouses. At the meso- or dyad-level we may think of time spend together on culture consumption, divorce rates, number of children, household income, total working hours. I hope you see that these dyad-characteristics may be grouped by how they are measured, namely at the dyad-level itself (yes/no divorce, number of children) or at the ego/alter-level and aggregated to the dyad-level (e.g. total working hours of the couple is the sum of the working hours of both individual partners, culture consumption is the consumed culture of both partners (alone and together)). But with both type of dyad-level outcomes, it should be clear that they are the consequence of interdependent actions of both partners (e.g. commonly both partners decide on whether to have and make children). At the macro- or society-level, we can think of the degree of assortative mating in society. But assortative mating may also have other macro-level consequences, such as inequality, segregation and polarization. 2.3.1 Homophily Let us focus on how assortative mating may impact homophily, or more precisely similarity between spouses. Thus how time-varying characteristics of the spouses (e.g. political opinions or behaviors) become more or less similar over time as a result of the marriage. Just a quick recap, homophily within couples may be the result of: selection shared context (dyadic) influence Hopefully it is clear that it is now time to focus on the role of influence. But what do we mean with influence? Lets read the following quote. People influence one another, and as the importance and immediacy of a group or individual increases, this influence becomes stronger (Latané, 1981). Forces of influence are especially strong within romantic relationships because these relationships are important, are predicated on mutual acceptance, and involve frequent exposure to the habits of ones partner.\" (Bartel et al. 2017) I would say this a quite naive conceptualisation of influence. It is implicitly assumed that partners will match their opinions/behaviors. Thus influence here is convergence. But why should partners characteristics converge over time, why would homophily (i.e. similarity) increase? And, it still does not become clear what the forces of influence are. Let us make a distinction between: positive influence: alters become more similar to each other over time negative influence: alters become more distinct to each other over time positive feedback influence: characteristics develop in same direction over time Now suppose these influence processes are the only reasons why alters change (i.e. the ceteris paribus condition). How could dyad similarity develop over time? Figure 2.5: Positive influence With positive influence (Figure 2.5) actors will become more similar to each other over time. Figure 2.6: Negative influence With negative influence (Figure 2.6) actors will become more distinct to each other over time. Figure 2.7: Positive feedback influence With positive feedback (Figure 2.7) actors will develop in the same direction. Naturally, we need to be aware that other mechanisms may also explain these trends. For example, with respect the figure above 2.7, a shared environment may also explain a shared trend. More concretely, when a couple gets children, both spouses may become happier over time. Finally, you could think of a situation in which one of the spouses changes, for some (exogenous) reason or another, his/her opinion. The other spouse is influenced by this opinion-change and moves in the same direction. See Figure 2.8. Figure 2.8: Opinion development over time: parallel trend with shocks. How should we label this type of influence? The limited positive feedback mechanism? 2.3.2 Influence mechanisms There are several forces of positive influence mechanisms: Information: we may exchange new effective information and arguments with our alters. Persuasion (and dissuasion): we may convince, force or pressure our alters to become similar to us. Contagion: this can be taken quite literally, like how the flu spreads but also more metaphorically like how (health) behaviors like drinking, smoking, sporting spread (because our alters increase the opportunities for these behaviors). Assimilation: we may mimic our alters because of a psychological need for similarity, because we think this will be good for our identity / social status, etc. The literature is not very clear and consistent about different type of incluence processes and which influence mechanisms are at play. You will thus also see that authors use socialisation when talking about (positive) influence processes. There are several forces of negative influence mechanisms: Information: We may exchange new counter-effective information and arguments with our alters. This would especially become relevant whey we dont like or belief the source of information and arguments. Persuasion (and dissuasion): we may convince, force or pressure our alters to become dissimilar to us. Polarisation: we may distance ourselves from our alters because of a psychological need for distinctiveness. This would especially become relevant when we are already distinct on key social dimensions. There are several forces of positive feedback influence mechanisms: Confirmation: information and arguments are repeated and existing opinions and behaviors of both alters reinforced. Competition: we may have a psychological need to be better/higher/more than our alter. The crucial difference between the positive feedback mechanisms and the positive influence mechanisms are that as a result of the former homophily between the alters does not necessarily change. Note that positive feedback could entail increasing and decreasing the opinion or behavior. You could also think of influence as being either active or passive. Thus, is ego trying to actively convince alter or passively leading by example? The alter could then either accept or reject the influence push of ego. We do not always listen to our partner. Embed from Getty Images 2.3.3 Polarization Influence processes that take place within couples may not only explain (in part) homophily or similarity within couples but as a consequence of this also polarization within society. Let us define polarization. Polarization Polarization is a social problem and a society is said to be polarized when (sizeable) (and clearly distinguishable) opinion groups in society hold (ever more) opposing, extreme opinions (or attitudes and behavior). We thus observe polarization in society when (sizeable) (and clearly distinguishable) opinion groups hold (ever more) opposing, extreme (political) opinions. A nice paper in which the many senses of polarization are discussed in more detailed is Bramson et al. (2017). Polarization in society will be larger when: similarity within each group increases (higher levels of within group homogeneity); dissimilarity between groups increases (higher levels of between group heterogeneity); when the relative sizes of opposing groups in society increases; when group sizes of the (most) opposing groups are more a like; when the opinion group identity becomes a more important part of ones social identity; when ingroup love and/or outgroup hate increases. Opinion group membership may overlap with memberships of groups based on other key social dimensions (e.g. sex, ethnicity, educational level, etc.). When this is the case, we could speak of alignment. A result of alignment is that opinion groups become more clearly distinguishable in society. Subsequently, the stronger the alignment the more likely it is that opinion group identity becomes salient and that ingroup love and outgroup hate increases. Let us take as example political polarization and start with a situation in which some political opinion (ranging from -10 to 10) is distributed as follows within society. Figure 2.9: Distribution of opinions within population Is this society polarized? There are many operationalizations of polarization (for pointers see: Esteban and Ray 1994; Ruedin 2021). Since we are focusing on the consequences of homophily (or similarity) within couples in this chapter and because you are already familiar with variance (discussed in Appendix B), we will define polarization as the variance in the distribution of the pairwise opinion differences (\\(d_{ij}\\)) across all possible dyads (\\(N(N-1)\\)) (Flache and Macy 2011): \\[P=\\frac{1}{N(N-1)} \\Sigma_{ij \\in N,i \\neq j} (d_{ij} - \\bar{d_{ij}} )^2 \\] or, \\[ P=VAR(d_{ij})\\] Polarization scores will lie within the range [0, 1), where \\(P=0\\) corresponds with full consensus in the population and the maximum when \\(P\\) approaches 1 will be observed in a population that is divided equally between the extreme opinions. Let us calculate the degree of polarization. First we need to define a function which does that for us.2 fpol &lt;- function(opinions, maxdif = NULL) { N &lt;- length(opinions) egos &lt;- matrix(opinions, nrow = N, ncol = N, byrow = TRUE) alters &lt;- t(egos) difference &lt;- if (is.null(maxdif)) { abs(egos - alters)/(range(opinions)[2] - range(opinions)[1]) } else { abs(egos - alters)/maxdif } diag(difference) &lt;- NA pol &lt;- var(as.numeric(difference), na.rm = T) return(pol) } It turns out the polarization score of the above society is fpol(opinions, 20) 0.0080761. But these individuals want to get married. Who do they pick? Well, we discussed the marriage market here 2.2. Let us assume that the opinion did not yet change but that bachelors prefer, to some extent, a partner with a similar political opinion. What does the political opinion distribution look like for the mean political opinion of the couples. Figure 2.10: Distribution of opinions within couples after selective marriage Well, the preference for a partner with similar political opinions was definitely not strong enough to substantially change the opinion distribution.3 The level of polarization (between couples) is: 0.0066944 What happens with these couples? They share a common context. What is a common context? Some examples: Shared life experiences: having kids poverty Shared environment: pollution housing poverty Suppose these life events are more likely for some couples than others. More specifically, assume that the occurrence of life events is associated with the mean political opinion of the couple (at time T). The life event consequently influences the mean political opinion of the couple (at time T+1). Figure 2.11: Distribution of opinions within couples after selective marriage and shared life events We now clearly observe emerging polarization between couples. But this is not yet reflected in our measure of polarization: 0.0268526. Probably because the opposing opinions are not close to the extremes (set at -10 and 10). Take home message: dont always trust your numbers but really look at your data as well. Note that if we go back to the individual-level again, emerging polarization is also apparent but less clearly so. Here the level of polarization is: 0.0267069. Figure 2.12: Distribution of opinions within population after selective marriage and shared life events Polarization between individuals will further increase when partners (positively) influence each other. Why would partners influence each other? Well, we discussed influence here 2.3.2. So what happens after positive influence? Figure 2.13: Distribution of opinions within population after selective marriage, shared life events and positive influence We already observe clear opinion groups. The level of polarization is now: 0.0268493. These groups may start to further move apart due to two mechanisms: (1) shared context at the dyad-level; (2) by positive feedback influence. Figure 2.14: Distribution of opinions within population after selective marriage, shared life events, positive influence and feedback influence Now, the society is clearly starting to polarize. Yes! Suprisingly, although the score on our polarization measure is starting to increase (to: 0.0944597), the degree of polarization is still far off from 1. Suppose instead that the theoretical range of the opinion would have been \\([-5,10]\\). Then our polarization score would have been a massive: 0.1655218. And on that bombshell, thanks for reading!! 2.4 Assignment Formulate a hypothesis on how cultural modernisation (e.g. more equal gender norms) may have affected assortative mating. Please focus on the impact of cultural modernisation on preferences. Motivate your answer. Formulate a hypothesis on how continuing educational expansion (which has been more pronounced for women than for men) may have affected patterns of homogamy. Please focus on the impact of educational expansion on the structure of search. Motivate your answer. This question is about intermarriage. A. Why would people prefer to marry someone who is similar? B. Why would people prefer to marry someone who is dissimilar? C. To what extent do you expect these reasons  mentioned in 3A and 3B to hold with respect to online relations (e.g. friend relations on Facebook or follower relations on Twitter)? Thus, with online relations I do not mean romantic relations but simply forming a tie (e.g. befriending someone on Facebook; @-mention someone on twitter; etc.). Please motivate your answer. What does Kalmijn (1998) mean with the by-product hypothesis? What does Schwartz (2013) mean with the economic inequality hypothesis and what role do returns to schooling play in this idea/hypothesis? Please give at least two reasons why increased assortative mating could increase economic inequality. See also Gonalons-Pons and Schwartz (2017). Read Verbakel and De Graaf (2009) and answer the following questions: A. What social problem(s) do the authors address? B. How do the authors deal with interdependencies? C. What macro-micro and micro-macro links are discussed? Have a look at all the papers citing the paper of Verbakel and De Graaf (2009). How did the social-network perspective in the field addressing the same (or similar) social problems evolve? References "],["methods.html", "Chapter 3 Methods 3.1 Causes 3.2 Consequences 3.3 Research questions 3.4 Data 3.5 Descriptives 3.6 Analysis 3.7 Assignment", " Chapter 3 Methods .button1 { background-color: #f44336; /* Red */ border: none; color: white; padding: 15px 32px; text-align: center; text-decoration: none; display: inline-block; font-size: 16px; margin: 4px 2px; cursor: pointer; } .button1:hover { box-shadow: 0 12px 16px 0 rgba(0,0,0,0.24), 0 17px 50px 0 rgba(0,0,0,0.19); } .button1 {border-radius: 12px;} .button1 {width: 100%;} 3.1 Causes When testing hypotheses on assortative mating many methodological approaches can be used. We may predict the frequency of specific dyads in our population with loglinear models and the data we use is commonly structured in a square table like the one below. Loglinear models are, in essence, nothing more than a nice, parsimonious and fancy way to calculate odds ratios. If we have a small, well filled table of just a few attributes, loglinear models are considered to be the golden standard. Table 3.1: Assortative Mating (dyad frequency) Wife educ-high Wife educ-low Husband educ-high 350 150 Husband educ-low 200 400 Another approach is to take the characteristics of the dyad (e.g. 1 = intermarriage and 0 = no intermarriage) as the dependent variable. This dependent variable can than be explained by applying (conditional) (multinomial) logistic regression techniques. In this case, the data is commonly structured in long format and looks something like the table below. Table 3.2: Assortative Mating (dyad characteristic) dyad_id wife educ wife age husband educ husband age intermarriage 1 LOW 30.63 LOW 26.91 0 2 LOW 24.79 LOW 34.61 0 3 HIGH 19.00 HIGH 25.92 0 4 HIGH 29.31 LOW 39.08 0 5 HIGH 25.32 LOW 30.11 0 6 LOW 23.86 LOW 19.01 0 7 HIGH 36.36 LOW 35.34 0 8 LOW 35.58 HIGH 22.64 0 9 LOW 24.24 HIGH 36.36 0 10 LOW 20.27 HIGH 20.79 0 11 HIGH 24.08 LOW 34.20 0 12 HIGH 44.44 LOW 23.97 0 13 LOW 33.40 LOW 25.77 0 14 HIGH 27.86 HIGH 35.55 0 15 LOW 21.97 HIGH 19.44 0 16 HIGH 38.16 LOW 18.42 0 17 HIGH 44.85 HIGH 37.86 0 18 LOW 30.91 LOW 23.21 0 19 HIGH 24.30 HIGH 31.53 0 20 LOW 36.19 LOW 21.50 0 Which methodology is preferred should depend on your hypotheses and on the data you have to your availability. Please be aware that in both approaches we normally do not have information on (the frequency or characteristics of) dyads in which there is no relation between ego and alter. Thus, you may have information on characteristics of me and my wife but you do not have information on all other women (or men) I could have married but didnt. I fished my wife out of the sea but we dont know what the other fish looked like. (Luckily my wife is no scientist and wont read this clarification.) 3.1.1 Odds Ratio 3.1.2 Loglinear Model 3.1.3 Conditional multinomial logit model 3.2 Consequences The method used to explain consequences of dyads depends on our Unit of Analysis. If it is the dyad itself (e.g. mean relationship quality) methods are relatively straightforward, because we may assume that the observations at the dyad-level are independent. We may explain this dyad-characteristic by other characteristics of the dyad (e.g. length of marriage) and by characteristics of each spouse (e.g. working hours men, working hours woman).4 That characteristics of the spouses, the covariates, are correlated does not necessarily pose a big problem.5 If, on the other hand, the unit of analysis are the partners themselves who make up the dyad, we need to acknowledge that the observations between the partners of the same couple are not independent. In part exactly because partners select and influence each other and share a social context. One solution could be to simply randomly select one partner of each couple or, if partners can be clearly distinguished - for example men and women in heterosexual couples - the different partners could be analyzed separately. A disadvantage of the latter two approaches is, however, that the covariance between the partners cannot be explained anymore, although this may exactly be the focus of our research questions. A more elegant solution, is to take the interdependencies into account and model these explicitly. This can be done within a multi-level framework and within a structural-equation modelling framework. 3.3 Research questions When we discussed the methods to analyze causes of homogamy, we were interested in homophily within couples at one point in time and focused on selection as explanans. Now, when we are discussing the consequences of dyads, we are interested in explaining trends in homophily within couples - thus homophily as a consequence of mating - and focus on shared context and influence processes as explanans. Naturally, because we are interested in trends in homophily within couples, we focus on dynamic characteristics of the spouses. Thus not, ethnicity, or educational attainment, but, for example, on their political opinions. Let us suppose our aim is to write a scientific paper on political opinion homophily within couples. We will formulate the following research questions: To what extent can we observe trends in political opinion homophily (or opinion similarity) within couples? To what extent can we explain trends in political opinion homophily by influence processes taking place between spouses? To what extent can we explain trends in political opinion homophily by the shared (social) context of the couple? Perhaps, you also want to focus in this paper on the influence process itself and formulate a second set of research questions: To what extent are influence process within couples influenced by (or conditional on): the shared context? characteristics of the couple? characteristics of the spouses? 3.4 Data We will use the data from the LISS panel. More concretely, we will use: 11 waves (2008-2014, 2016-2029) Filter on heterosexual couples (cohabiting and married) Filter on couples of which both spouses are older than 25. We have already constructed a dataset for you guys and gals to work with which contains information on more than 3000 couples. Dont forget it is a panel data set. This means we have more observations for the same couple (and spouses) over time. Also be aware that our spouses can be clearly distinguished from one another (i.e. husband versus wife). Please download this data file to your working directory. partner_dataprepped.Rdata 3.4.1 Variables Variables of interest and value labels: education: = highest completed education in years (4-16.5) sex: = 0 = male / 1 = female eu_integration: 0 = eu integration has gone too far / 4 = eu integration should go further immigrants: 0 = immigrants should adjust / 4 immigrants can retain their own culture. euthanasia: 1 = euthanasia should be forbidden / 5 euthanasia should be permitted income_diff: 1 differences in income should increase / 5 differences in income should decrease For the original variables in Dutch see below: opleiding Hoogste opleiding met diploma 1 basisonderwijs 2 vmbo 3 havo/vwo 4 mbo 5 hbo 6 wo 7 anders 8 (Nog) geen onderwijs afgerond 9 Volgt nog geen onderwijs Hierbij hebben wij opleiding gecategoriseerd in drie groepen: 1. Laag: basisonderwijs en vmbo 2. Midden: havo/vwo en mbo 3. Hoog: hbo en wo We nemen enkel mensen van 25 jaar en ouder mee. Van hen verwachten we dat ze klaar zijn met hun onderwijscarriere. EU integratie De Europese integratie is te ver gegaan. 1 Helemaal oneens 2 Oneens 3 Niet eens, niet oneens 4 Eens 5 Helemaal eens Migratie/integratie In Nederland vinden sommigen dat mensen met een migratie achtergrond hier moeten kunnen leven met behoud van de eigen cultuur. Anderen vinden dat zij zich geheel moeten aanpassen aan de Nederlandse cultuur. Waar zou u uzelf plaatsen op een schaal van 1 t/m 5, waarbij 1 behoud van eigen cultuur voor mensen met een migratie achtergrond betekent en 5 dat zij zich geheel moeten aanpassen? 1 behoud van eigen cultuur voor mensen met een migratie achtergrond 2 3 4 5 mensen met een migratie achtergrond moeten zich geheel aanpassen 3.5 Descriptives 3.5.1 Preperation #### clean the environment ####. rm(list = ls()) #### packages ####. require(tidyverse) ##### Data input ###. load(&quot;addfiles/partner_dataprepped.Rdata&quot;) # clean a little bit. rm(&quot;partner_df_wide&quot;) #we will make a wide file later again. partner_df_long &lt;- partner_df_long %&gt;% rename(dyad_id = &quot;nohouse_encr&quot;) %&gt;% select(-c(&quot;hetero&quot;, &quot;n&quot;, &quot;nomem_encr.m&quot;, &quot;positie.m&quot;, &quot;nomem_encr.f&quot;, &quot;positie.f&quot;)) partner_df_wide &lt;- reshape(partner_df_long, direction = &quot;wide&quot;, idvar = &quot;dyad_id&quot;, timevar = &quot;time&quot;) Please note that our time variable indicates survey year. We do not want to describe period trends in opinion homophily within couples but lifecourse trends. We will call the latter the within-trend. For people familiar with life course research, hopefully you will see the resemblance to the Age-Period-Cohort conundrum. We therefore need to construct a time_within variable and add it to our data set. Before copying the hidden code below, please try to do this yourself. Only click button after 5 minutes! partner_df_long &lt;- partner_df_long %&gt;% arrange(dyad_id, time) %&gt;% #let us order the data set group_by(dyad_id) %&gt;% #we focus on within dyad-level thus group by dyad mutate(start_time = min(time), #determine first wave of participation within_time = (time - start_time) + 1, #count passing of time within couples n_time = n()) %&gt;% #keep track of number of times couples participated in LISS ungroup() Now we need to think of how we want to operationalize opinion homophily. The stronger the political opinion homophily within couples, the larger the spousal correspondence, the larger the association between the opinions of the partners. We discussed covariance and correlations here. Would that be an option? A different approach would be to operationalize increasing homophily as decreasing absolute opinion dissimilarity between the spouses (Iyengar, Konitzer, and Tedin 2018). 3.5.2 Opinion homophily over within-time Lets have a go. cors &lt;- partner_df_long %&gt;% group_by(within_time) %&gt;% #we want to see within couple trends. group_map(~ cor.test(x=.x$immigrants.m, y=.x$immigrants.f)$estimate ) %&gt;% unlist() # How to do this in Base R? # # we subset the data. # cor.test(partner_df_long$immigrants.m[partner_df_long$within_time==1], partner_df_long$immigrants.f[partner_df_long$within_time==1]) # # #And we could use a loop. # cors &lt;- list() # for (i in 1:11) { # cors[[i]] &lt;- cor.test(partner_df_long$immigrants.m[partner_df_long$within_time==i], partner_df_long$immigrants.f[partner_df_long$within_time==i]) # } covs &lt;- partner_df_long %&gt;% group_by(within_time) %&gt;% #we want to see within couple trends. group_map(~ cov(x=.x$immigrants.m, y=.x$immigrants.f, use=&quot;complete.obs&quot;) ) %&gt;% unlist() partner_df_long %&gt;% mutate(distance = abs(immigrants.m - immigrants.f)) %&gt;% group_by(within_time) %&gt;% #we want to see within couple trends. summarise(mean_dist = mean(distance, na.rm=T)) %&gt;% add_column(cors) %&gt;% add_column(covs) #&gt; # A tibble: 11 x 4 #&gt; within_time mean_dist cors covs #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 0.716 0.440 0.422 #&gt; 2 2 0.711 0.450 0.431 #&gt; 3 3 0.706 0.438 0.402 #&gt; 4 4 0.662 0.485 0.446 #&gt; 5 5 0.657 0.488 0.450 #&gt; 6 6 0.670 0.483 0.452 #&gt; 7 7 0.675 0.458 0.416 #&gt; 8 8 0.681 0.432 0.374 #&gt; 9 9 0.686 0.442 0.399 #&gt; 10 10 0.662 0.419 0.328 #&gt; 11 11 0.650 0.474 0.383 Whow! Please look at the above results for at least 5 minutes before you move one. What is your interpretation? Do you observe a (significant) trend? Suppose that the mean distance indeed decreased but that, as a consequence, also the variance in this opinion decreased. What impact would this have for the covariance? 3.5.3 Distinguishing period trends from lifecourse trends The above results are hard to interpret. In part because we mix up period trends and and within-trends. And although we are interested in influence and shared context effects, we must not forget about (de)selection effects. The couples who survive to reach within-time 11 are probably a selective subgroup of all couples. A bit more sophisticated: calculate the (euclidean) distance between all four attitudes. Try to distinguishing period trends from lifecourse trends. Try to take into account of selective sample attrition.6 trends &lt;- partner_df_long %&gt;% filter(n_time&gt;4) %&gt;% #trying to take selective sample attrition into account. mutate(distance = sqrt((immigrants.m - immigrants.f)^2 + (euthanasia.m - euthanasia.f)^2 + (income_diff.m - income_diff.f)^2 + (eu_integration.m - eu_integration.f)^2 )) %&gt;% #euclidean distance of four political opinions group_by(start_time, within_time) %&gt;% #we want to see within couple trends but need to &#39;control for&#39; period trends summarise(mean_dist = mean(distance, na.rm=T)) #Put results in a matrix #?? how to do this the tidy way? trends_matrix &lt;- matrix(NA, nrow=11, ncol=11) for (i in 1:length(trends$start_time)) { trends_matrix[trends$start_time[i],trends$within_time[i]] &lt;- trends$mean_dist[i] } rownames(trends_matrix) &lt;- c(2008:2014, 2016:2019) colnames(trends_matrix) &lt;- paste(&quot;within_time&quot;, 1:11, sep=&quot;_&quot;) trends_matrix Table 3.3: Trends in spousal correspondance in political attitudes within_time_1 within_time_2 within_time_3 within_time_4 within_time_5 within_time_6 within_time_7 within_time_8 within_time_9 within_time_10 within_time_11 2008 1.929 1.922 1.793 1.809 1.793 1.72 1.720 1.777 1.815 1.843 1.816 2009 2.009 1.743 1.619 1.842 1.719 1.681 1.636 1.743 1.640 1.874 NA 2010 1.823 1.798 1.827 1.691 1.811 1.697 1.699 1.861 1.776 NA NA 2011 2.072 1.803 1.931 1.549 1.829 2.035 1.466 1.973 NA NA NA 2012 1.826 1.721 1.588 1.799 1.848 1.581 1.724 NA NA NA NA 2013 1.908 1.932 1.669 1.98 1.883 1.744 NA NA NA NA NA 2014 1.945 1.959 1.981 1.693 1.999 NA NA NA NA NA NA 2016 NA NA NA NA NA NA NA NA NA NA NA 2017 NA NA NA NA NA NA NA NA NA NA NA 2018 NA NA NA NA NA NA NA NA NA NA NA 2019 NA NA NA NA NA NA NA NA NA NA NA Follow the same couples over time by following the diagonals (e.g. the blue and red). What would you conclude? 3.6 Analysis Even tough the above descriptives do not show convincing time trends, we know that partners may still influence each other. Let us try to test for influence processes. Feel free to re-estimate all models yourself. But it will be way quicker to download all results. results.Rdata 3.6.1 Preperation We need to do some additional dataprep. I know it is a bit cumbersome, but we will make a datafile for each dependent variable separately and will use some shorter names. The data files are stored in a list called datalist_ori. datalist_ori &lt;- list() # dep1: eu_integration dat &lt;- data.frame(x1 = partner_df_wide$eu_integration.m.1) dat$x2 &lt;- partner_df_wide$eu_integration.m.2 dat$x3 &lt;- partner_df_wide$eu_integration.m.3 dat$x4 &lt;- partner_df_wide$eu_integration.m.4 dat$x5 &lt;- partner_df_wide$eu_integration.m.5 dat$x6 &lt;- partner_df_wide$eu_integration.m.6 dat$x7 &lt;- partner_df_wide$eu_integration.m.7 dat$x8 &lt;- partner_df_wide$eu_integration.m.8 dat$x9 &lt;- partner_df_wide$eu_integration.m.9 dat$x10 &lt;- partner_df_wide$eu_integration.m.10 dat$x11 &lt;- partner_df_wide$eu_integration.m.11 dat$y1 &lt;- partner_df_wide$eu_integration.f.1 dat$y2 &lt;- partner_df_wide$eu_integration.f.2 dat$y3 &lt;- partner_df_wide$eu_integration.f.3 dat$y4 &lt;- partner_df_wide$eu_integration.f.4 dat$y5 &lt;- partner_df_wide$eu_integration.f.5 dat$y6 &lt;- partner_df_wide$eu_integration.f.6 dat$y7 &lt;- partner_df_wide$eu_integration.f.7 dat$y8 &lt;- partner_df_wide$eu_integration.f.8 dat$y9 &lt;- partner_df_wide$eu_integration.f.9 dat$y10 &lt;- partner_df_wide$eu_integration.f.10 dat$y11 &lt;- partner_df_wide$eu_integration.f.11 # treat education as a time stable. And use all available data. dat$oplx &lt;- rowMeans(partner_df_wide[, c(&quot;oplmet.m.1&quot;, &quot;oplmet.m.2&quot;, &quot;oplmet.m.3&quot;, &quot;oplmet.m.4&quot;, &quot;oplmet.m.5&quot;, &quot;oplmet.m.6&quot;, &quot;oplmet.m.7&quot;, &quot;oplmet.m.8&quot;, &quot;oplmet.m.9&quot;, &quot;oplmet.m.10&quot;, &quot;oplmet.m.11&quot;)], na.rm = T) dat$oply &lt;- rowMeans(partner_df_wide[, c(&quot;oplmet.f.1&quot;, &quot;oplmet.f.2&quot;, &quot;oplmet.f.3&quot;, &quot;oplmet.f.4&quot;, &quot;oplmet.f.5&quot;, &quot;oplmet.f.6&quot;, &quot;oplmet.f.7&quot;, &quot;oplmet.f.8&quot;, &quot;oplmet.f.9&quot;, &quot;oplmet.f.10&quot;, &quot;oplmet.f.11&quot;)], na.rm = T) # calculate diff in education between men and women dat$oplxy &lt;- dat$oplx - dat$oply # table(dat$oplx, dat$oply, useNA = &#39;always&#39;) hist(dat$oplxy) # define three groups for multigroup analyses dat$oplgroup &lt;- ifelse(dat$oplxy &gt; 1, &quot;menhigher&quot;, NA) dat$oplgroup &lt;- ifelse(dat$oplxy &lt; -1, &quot;womenhigher&quot;, dat$oplgroup) dat$oplgroup &lt;- ifelse(dat$oplxy &lt;= 1 &amp; dat$oplxy &gt;= -1, &quot;equal&quot;, dat$oplgroup) # table(dat$oplgroup, useNA = &#39;always&#39;) dat_ori &lt;- dat datalist_ori[[1]] &lt;- dat_ori # dep2: immigrants dat &lt;- data.frame(x1 = partner_df_wide$immigrants.m.1) dat$x2 &lt;- partner_df_wide$immigrants.m.2 dat$x3 &lt;- partner_df_wide$immigrants.m.3 dat$x4 &lt;- partner_df_wide$immigrants.m.4 dat$x5 &lt;- partner_df_wide$immigrants.m.5 dat$x6 &lt;- partner_df_wide$immigrants.m.6 dat$x7 &lt;- partner_df_wide$immigrants.m.7 dat$x8 &lt;- partner_df_wide$immigrants.m.8 dat$x9 &lt;- partner_df_wide$immigrants.m.9 dat$x10 &lt;- partner_df_wide$immigrants.m.10 dat$x11 &lt;- partner_df_wide$immigrants.m.11 dat$y1 &lt;- partner_df_wide$immigrants.f.1 dat$y2 &lt;- partner_df_wide$immigrants.f.2 dat$y3 &lt;- partner_df_wide$immigrants.f.3 dat$y4 &lt;- partner_df_wide$immigrants.f.4 dat$y5 &lt;- partner_df_wide$immigrants.f.5 dat$y6 &lt;- partner_df_wide$immigrants.f.6 dat$y7 &lt;- partner_df_wide$immigrants.f.7 dat$y8 &lt;- partner_df_wide$immigrants.f.8 dat$y9 &lt;- partner_df_wide$immigrants.f.9 dat$y10 &lt;- partner_df_wide$immigrants.f.10 dat$y11 &lt;- partner_df_wide$immigrants.f.11 # treat education as a time stable. And use all available data. dat$oplx &lt;- rowMeans(partner_df_wide[, c(&quot;oplmet.m.1&quot;, &quot;oplmet.m.2&quot;, &quot;oplmet.m.3&quot;, &quot;oplmet.m.4&quot;, &quot;oplmet.m.5&quot;, &quot;oplmet.m.6&quot;, &quot;oplmet.m.7&quot;, &quot;oplmet.m.8&quot;, &quot;oplmet.m.9&quot;, &quot;oplmet.m.10&quot;, &quot;oplmet.m.11&quot;)], na.rm = T) dat$oply &lt;- rowMeans(partner_df_wide[, c(&quot;oplmet.f.1&quot;, &quot;oplmet.f.2&quot;, &quot;oplmet.f.3&quot;, &quot;oplmet.f.4&quot;, &quot;oplmet.f.5&quot;, &quot;oplmet.f.6&quot;, &quot;oplmet.f.7&quot;, &quot;oplmet.f.8&quot;, &quot;oplmet.f.9&quot;, &quot;oplmet.f.10&quot;, &quot;oplmet.f.11&quot;)], na.rm = T) # calculate diff in education between men and women dat$oplxy &lt;- dat$oplx - dat$oply # table(dat$oplx, dat$oply, useNA = &#39;always&#39;) hist(dat$oplxy) # define three groups for multigroup analyses dat$oplgroup &lt;- ifelse(dat$oplxy &gt; 1, &quot;menhigher&quot;, NA) dat$oplgroup &lt;- ifelse(dat$oplxy &lt; -1, &quot;womenhigher&quot;, dat$oplgroup) dat$oplgroup &lt;- ifelse(dat$oplxy &lt;= 1 &amp; dat$oplxy &gt;= -1, &quot;equal&quot;, dat$oplgroup) # table(dat$oplgroup, useNA = &#39;always&#39;) dat_ori &lt;- dat datalist_ori[[2]] &lt;- dat_ori # dep3: euthanasia dat &lt;- data.frame(x1 = partner_df_wide$euthanasia.m.1) dat$x2 &lt;- partner_df_wide$euthanasia.m.2 dat$x3 &lt;- partner_df_wide$euthanasia.m.3 dat$x4 &lt;- partner_df_wide$euthanasia.m.4 dat$x5 &lt;- partner_df_wide$euthanasia.m.5 dat$x6 &lt;- partner_df_wide$euthanasia.m.6 dat$x7 &lt;- partner_df_wide$euthanasia.m.7 dat$x8 &lt;- partner_df_wide$euthanasia.m.8 dat$x9 &lt;- partner_df_wide$euthanasia.m.9 dat$x10 &lt;- partner_df_wide$euthanasia.m.10 dat$x11 &lt;- partner_df_wide$euthanasia.m.11 dat$y1 &lt;- partner_df_wide$euthanasia.f.1 dat$y2 &lt;- partner_df_wide$euthanasia.f.2 dat$y3 &lt;- partner_df_wide$euthanasia.f.3 dat$y4 &lt;- partner_df_wide$euthanasia.f.4 dat$y5 &lt;- partner_df_wide$euthanasia.f.5 dat$y6 &lt;- partner_df_wide$euthanasia.f.6 dat$y7 &lt;- partner_df_wide$euthanasia.f.7 dat$y8 &lt;- partner_df_wide$euthanasia.f.8 dat$y9 &lt;- partner_df_wide$euthanasia.f.9 dat$y10 &lt;- partner_df_wide$euthanasia.f.10 dat$y11 &lt;- partner_df_wide$euthanasia.f.11 # treat education as a time stable. And use all available data. dat$oplx &lt;- rowMeans(partner_df_wide[, c(&quot;oplmet.m.1&quot;, &quot;oplmet.m.2&quot;, &quot;oplmet.m.3&quot;, &quot;oplmet.m.4&quot;, &quot;oplmet.m.5&quot;, &quot;oplmet.m.6&quot;, &quot;oplmet.m.7&quot;, &quot;oplmet.m.8&quot;, &quot;oplmet.m.9&quot;, &quot;oplmet.m.10&quot;, &quot;oplmet.m.11&quot;)], na.rm = T) dat$oply &lt;- rowMeans(partner_df_wide[, c(&quot;oplmet.f.1&quot;, &quot;oplmet.f.2&quot;, &quot;oplmet.f.3&quot;, &quot;oplmet.f.4&quot;, &quot;oplmet.f.5&quot;, &quot;oplmet.f.6&quot;, &quot;oplmet.f.7&quot;, &quot;oplmet.f.8&quot;, &quot;oplmet.f.9&quot;, &quot;oplmet.f.10&quot;, &quot;oplmet.f.11&quot;)], na.rm = T) # calculate diff in education between men and women dat$oplxy &lt;- dat$oplx - dat$oply # table(dat$oplx, dat$oply, useNA = &#39;always&#39;) hist(dat$oplxy) # define three groups for multigroup analyses dat$oplgroup &lt;- ifelse(dat$oplxy &gt; 1, &quot;menhigher&quot;, NA) dat$oplgroup &lt;- ifelse(dat$oplxy &lt; -1, &quot;womenhigher&quot;, dat$oplgroup) dat$oplgroup &lt;- ifelse(dat$oplxy &lt;= 1 &amp; dat$oplxy &gt;= -1, &quot;equal&quot;, dat$oplgroup) # table(dat$oplgroup, useNA = &#39;always&#39;) dat_ori &lt;- dat datalist_ori[[3]] &lt;- dat_ori # dep4: income_diff dat &lt;- data.frame(x1 = partner_df_wide$income_diff.m.1) dat$x2 &lt;- partner_df_wide$income_diff.m.2 dat$x3 &lt;- partner_df_wide$income_diff.m.3 dat$x4 &lt;- partner_df_wide$income_diff.m.4 dat$x5 &lt;- partner_df_wide$income_diff.m.5 dat$x6 &lt;- partner_df_wide$income_diff.m.6 dat$x7 &lt;- partner_df_wide$income_diff.m.7 dat$x8 &lt;- partner_df_wide$income_diff.m.8 dat$x9 &lt;- partner_df_wide$income_diff.m.9 dat$x10 &lt;- partner_df_wide$income_diff.m.10 dat$x11 &lt;- partner_df_wide$income_diff.m.11 dat$y1 &lt;- partner_df_wide$income_diff.f.1 dat$y2 &lt;- partner_df_wide$income_diff.f.2 dat$y3 &lt;- partner_df_wide$income_diff.f.3 dat$y4 &lt;- partner_df_wide$income_diff.f.4 dat$y5 &lt;- partner_df_wide$income_diff.f.5 dat$y6 &lt;- partner_df_wide$income_diff.f.6 dat$y7 &lt;- partner_df_wide$income_diff.f.7 dat$y8 &lt;- partner_df_wide$income_diff.f.8 dat$y9 &lt;- partner_df_wide$income_diff.f.9 dat$y10 &lt;- partner_df_wide$income_diff.f.10 dat$y11 &lt;- partner_df_wide$income_diff.f.11 # treat education as a time stable. And use all available data. dat$oplx &lt;- rowMeans(partner_df_wide[, c(&quot;oplmet.m.1&quot;, &quot;oplmet.m.2&quot;, &quot;oplmet.m.3&quot;, &quot;oplmet.m.4&quot;, &quot;oplmet.m.5&quot;, &quot;oplmet.m.6&quot;, &quot;oplmet.m.7&quot;, &quot;oplmet.m.8&quot;, &quot;oplmet.m.9&quot;, &quot;oplmet.m.10&quot;, &quot;oplmet.m.11&quot;)], na.rm = T) dat$oply &lt;- rowMeans(partner_df_wide[, c(&quot;oplmet.f.1&quot;, &quot;oplmet.f.2&quot;, &quot;oplmet.f.3&quot;, &quot;oplmet.f.4&quot;, &quot;oplmet.f.5&quot;, &quot;oplmet.f.6&quot;, &quot;oplmet.f.7&quot;, &quot;oplmet.f.8&quot;, &quot;oplmet.f.9&quot;, &quot;oplmet.f.10&quot;, &quot;oplmet.f.11&quot;)], na.rm = T) # calculate diff in education between men and women dat$oplxy &lt;- dat$oplx - dat$oply # table(dat$oplx, dat$oply, useNA = &#39;always&#39;) hist(dat$oplxy) # define three groups for multigroup analyses dat$oplgroup &lt;- ifelse(dat$oplxy &gt; 1, &quot;menhigher&quot;, NA) dat$oplgroup &lt;- ifelse(dat$oplxy &lt; -1, &quot;womenhigher&quot;, dat$oplgroup) dat$oplgroup &lt;- ifelse(dat$oplxy &lt;= 1 &amp; dat$oplxy &gt;= -1, &quot;equal&quot;, dat$oplgroup) # table(dat$oplgroup, useNA = &#39;always&#39;) dat_ori &lt;- dat datalist_ori[[4]] &lt;- dat_ori 3.6.2 Modelling strategy takes into account that observations within couples are interdependent; that can explain the interdependence at the couple level; focus on changes taking place within couples, not on changes between couples; clearly distinguishes between: Actor effects: stability effects Partner effects: influence effects Is flexible, so we can control, for example, for educational effects. A model that tickes all the boxes is the Random Intercep Cross-Lagged Panel Model. Figure 3.1: RI-CLPM Source: (Mulder and Hamaker 2020) 3.6.3 Robustness We will compare the results across four different modeling strategies: Cross-lagged Panel Model (CLPM) (11 waves). This model lumps together between couple effects and within couples effects but it may help us to compare results. RI-CLPM (11 waves). We will focus on this model! RI-CLPM + structural time trends (11 waves). This model takes into account that there may also be general period (or structural) trends in the opinions. RI/RS-CLPM (11 waves). Finally, in this model we take into account that spouses may show different (linear) trends in their opinions for reasons we do not know. It is a random-intercept, random-slope growth curve model for the two spouses combined. We will test our hypotheses for four different dependent variables: eu-integration immigration euthanasia income differences 3.6.4 Results hypo1 Hypo1 RI-CLPM: When your partners opinion is relatively high (compared to your partners average opinion over time) at time T, your own opinion will be relatively high (compared to your own average opinion over time) at time T+1. 3.6.4.1 CLPM results &lt;- list() myModel &lt;- &quot; ### control for education x1 + x2 + x3 +x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 ~ e*oplx y1 + y2 + y3 +y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 ~ e*oply ### Estimate the lagged effects x2 ~ a*x1 + b*y1 x3 ~ a*x2 + b*y2 x4 ~ a*x3 + b*y3 x5 ~ a*x4 + b*y4 x6 ~ a*x5 + b*y5 x7 ~ a*x6 + b*y6 x8 ~ a*x7 + b*y7 x9 ~ a*x8 + b*y8 x10 ~ a*x9 + b*y9 x11 ~ a*x10 + b*y10 y2 ~ b*x1 + a*y1 y3 ~ b*x2 + a*y2 y4 ~ b*x3 + a*y3 y5 ~ b*x4 + a*y4 y6 ~ b*x5 + a*y5 y7 ~ b*x6 + a*y6 y8 ~ b*x7 + a*y7 y9 ~ b*x8 + a*y8 y10 ~ b*x9 + a*y9 y11 ~ b*x10 + a*y10 # Estimate the (residual) covariance between the variables x1 ~~ y1 # Covariance x2 ~~ y2 x3 ~~ y3 x4 ~~ y4 x5 ~~ y5 x6 ~~ y6 x7 ~~ y7 x8 ~~ y8 x9 ~~ y9 x10 ~~ y10 x11 ~~ y11 # Estimate the (residual) variance of the variables. x1 ~~ x1 # Variances y1 ~~ y1 x2 ~~ x2 # Residual variances y2 ~~ y2 x3 ~~ x3 y3 ~~ y3 x4 ~~ x4 y4 ~~ y4 x5 ~~ x5 y5 ~~ y5 x6 ~~ x6 y6 ~~ y6 x7 ~~ x7 y7 ~~ y7 x8 ~~ x8 y8 ~~ y8 x9 ~~ x9 y9 ~~ y9 x10 ~~ x10 y10 ~~ y10 x11 ~~ x11 y11 ~~ y11 #intercepts x1 ~ 1 y1 ~ 1 x2 ~ 1 y2 ~ 1 x3 ~ 1 y3 ~ 1 x4 ~ 1 y4 ~ 1 x5 ~ 1 y5 ~ 1 x6 ~ 1 y6 ~ 1 x7 ~ 1 y7 ~ 1 x8 ~ 1 y8 ~ 1 x9 ~ 1 y9 ~ 1 x10 ~ 1 y10 ~ 1 x11 ~ 1 y11 ~ 1 &quot; # Estimate models a bit faster: estimate &lt;- function(x) lavaan(myModel, data = x, missing = &quot;fiml.x&quot;, meanstructure = T) library(future.apply) plan(multisession) results_temp &lt;- future_lapply(datalist_ori, estimate) results[[1]] &lt;- results_temp[[1]] results[[2]] &lt;- results_temp[[2]] results[[3]] &lt;- results_temp[[3]] results[[4]] &lt;- results_temp[[4]] names(results)[1:4] &lt;- c(&quot;fitm1h1y1&quot;, &quot;fitm1h1y2&quot;, &quot;fitm1h1y3&quot;, &quot;fitm1h1y4&quot;) save(results, file = &quot;results.RData&quot;) load(&quot;addfiles/results.Rdata&quot;) summary(results[[1]]) summary(results[[2]]) summary(results[[3]]) summary(results[[4]]) #&gt; lavaan 0.6-9 ended normally after 31 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 117 #&gt; Number of equality constraints 59 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1415 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 3353.035 #&gt; Degrees of freedom 261 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; x1 ~ #&gt; oplx (e) 0.039 0.002 22.760 0.000 #&gt; x2 ~ #&gt; oplx (e) 0.039 0.002 22.760 0.000 #&gt; x3 ~ #&gt; oplx (e) 0.039 0.002 22.760 0.000 #&gt; x4 ~ #&gt; oplx (e) 0.039 0.002 22.760 0.000 #&gt; x5 ~ #&gt; oplx (e) 0.039 0.002 22.760 0.000 #&gt; x6 ~ #&gt; oplx (e) 0.039 0.002 22.760 0.000 #&gt; x7 ~ #&gt; oplx (e) 0.039 0.002 22.760 0.000 #&gt; x8 ~ #&gt; oplx (e) 0.039 0.002 22.760 0.000 #&gt; x9 ~ #&gt; oplx (e) 0.039 0.002 22.760 0.000 #&gt; x10 ~ #&gt; oplx (e) 0.039 0.002 22.760 0.000 #&gt; x11 ~ #&gt; oplx (e) 0.039 0.002 22.760 0.000 #&gt; y1 ~ #&gt; oply (e) 0.039 0.002 22.760 0.000 #&gt; y2 ~ #&gt; oply (e) 0.039 0.002 22.760 0.000 #&gt; y3 ~ #&gt; oply (e) 0.039 0.002 22.760 0.000 #&gt; y4 ~ #&gt; oply (e) 0.039 0.002 22.760 0.000 #&gt; y5 ~ #&gt; oply (e) 0.039 0.002 22.760 0.000 #&gt; y6 ~ #&gt; oply (e) 0.039 0.002 22.760 0.000 #&gt; y7 ~ #&gt; oply (e) 0.039 0.002 22.760 0.000 #&gt; y8 ~ #&gt; oply (e) 0.039 0.002 22.760 0.000 #&gt; y9 ~ #&gt; oply (e) 0.039 0.002 22.760 0.000 #&gt; y10 ~ #&gt; oply (e) 0.039 0.002 22.760 0.000 #&gt; y11 ~ #&gt; oply (e) 0.039 0.002 22.760 0.000 #&gt; x2 ~ #&gt; x1 (a) 0.572 0.005 114.096 0.000 #&gt; y1 (b) 0.150 0.005 29.318 0.000 #&gt; x3 ~ #&gt; x2 (a) 0.572 0.005 114.096 0.000 #&gt; y2 (b) 0.150 0.005 29.318 0.000 #&gt; x4 ~ #&gt; x3 (a) 0.572 0.005 114.096 0.000 #&gt; y3 (b) 0.150 0.005 29.318 0.000 #&gt; x5 ~ #&gt; x4 (a) 0.572 0.005 114.096 0.000 #&gt; y4 (b) 0.150 0.005 29.318 0.000 #&gt; x6 ~ #&gt; x5 (a) 0.572 0.005 114.096 0.000 #&gt; y5 (b) 0.150 0.005 29.318 0.000 #&gt; x7 ~ #&gt; x6 (a) 0.572 0.005 114.096 0.000 #&gt; y6 (b) 0.150 0.005 29.318 0.000 #&gt; x8 ~ #&gt; x7 (a) 0.572 0.005 114.096 0.000 #&gt; y7 (b) 0.150 0.005 29.318 0.000 #&gt; x9 ~ #&gt; x8 (a) 0.572 0.005 114.096 0.000 #&gt; y8 (b) 0.150 0.005 29.318 0.000 #&gt; x10 ~ #&gt; x9 (a) 0.572 0.005 114.096 0.000 #&gt; y9 (b) 0.150 0.005 29.318 0.000 #&gt; x11 ~ #&gt; x10 (a) 0.572 0.005 114.096 0.000 #&gt; y10 (b) 0.150 0.005 29.318 0.000 #&gt; y2 ~ #&gt; x1 (b) 0.150 0.005 29.318 0.000 #&gt; y1 (a) 0.572 0.005 114.096 0.000 #&gt; y3 ~ #&gt; x2 (b) 0.150 0.005 29.318 0.000 #&gt; y2 (a) 0.572 0.005 114.096 0.000 #&gt; y4 ~ #&gt; x3 (b) 0.150 0.005 29.318 0.000 #&gt; y3 (a) 0.572 0.005 114.096 0.000 #&gt; y5 ~ #&gt; x4 (b) 0.150 0.005 29.318 0.000 #&gt; y4 (a) 0.572 0.005 114.096 0.000 #&gt; y6 ~ #&gt; x5 (b) 0.150 0.005 29.318 0.000 #&gt; y5 (a) 0.572 0.005 114.096 0.000 #&gt; y7 ~ #&gt; x6 (b) 0.150 0.005 29.318 0.000 #&gt; y6 (a) 0.572 0.005 114.096 0.000 #&gt; y8 ~ #&gt; x7 (b) 0.150 0.005 29.318 0.000 #&gt; y7 (a) 0.572 0.005 114.096 0.000 #&gt; y9 ~ #&gt; x8 (b) 0.150 0.005 29.318 0.000 #&gt; y8 (a) 0.572 0.005 114.096 0.000 #&gt; y10 ~ #&gt; x9 (b) 0.150 0.005 29.318 0.000 #&gt; y9 (a) 0.572 0.005 114.096 0.000 #&gt; y11 ~ #&gt; x10 (b) 0.150 0.005 29.318 0.000 #&gt; y10 (a) 0.572 0.005 114.096 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 ~~ #&gt; .y1 0.410 0.030 13.695 0.000 #&gt; .x2 ~~ #&gt; .y2 0.121 0.021 5.706 0.000 #&gt; .x3 ~~ #&gt; .y3 0.118 0.020 5.806 0.000 #&gt; .x4 ~~ #&gt; .y4 0.152 0.022 7.063 0.000 #&gt; .x5 ~~ #&gt; .y5 0.147 0.025 5.937 0.000 #&gt; .x6 ~~ #&gt; .y6 0.110 0.020 5.561 0.000 #&gt; .x7 ~~ #&gt; .y7 0.105 0.018 5.991 0.000 #&gt; .x8 ~~ #&gt; .y8 0.112 0.021 5.242 0.000 #&gt; .x9 ~~ #&gt; .y9 0.119 0.022 5.444 0.000 #&gt; .x10 ~~ #&gt; .y10 0.078 0.023 3.335 0.001 #&gt; .x11 ~~ #&gt; .y11 0.130 0.024 5.493 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 1.133 0.034 33.382 0.000 #&gt; .y1 1.087 0.031 35.251 0.000 #&gt; .x2 0.089 0.031 2.898 0.004 #&gt; .y2 0.072 0.029 2.469 0.014 #&gt; .x3 0.016 0.031 0.509 0.611 #&gt; .y3 0.007 0.028 0.245 0.806 #&gt; .x4 -0.285 0.031 -9.066 0.000 #&gt; .y4 -0.326 0.029 -11.062 0.000 #&gt; .x5 0.107 0.032 3.285 0.001 #&gt; .y5 -0.082 0.030 -2.748 0.006 #&gt; .x6 -0.190 0.030 -6.311 0.000 #&gt; .y6 -0.150 0.029 -5.257 0.000 #&gt; .x7 -0.187 0.029 -6.370 0.000 #&gt; .y7 -0.185 0.028 -6.666 0.000 #&gt; .x8 -0.017 0.031 -0.564 0.573 #&gt; .y8 -0.032 0.029 -1.094 0.274 #&gt; .x9 -0.169 0.031 -5.452 0.000 #&gt; .y9 -0.112 0.030 -3.758 0.000 #&gt; .x10 0.041 0.032 1.282 0.200 #&gt; .y10 0.077 0.031 2.492 0.013 #&gt; .x11 0.033 0.033 0.992 0.321 #&gt; .y11 0.029 0.031 0.948 0.343 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 1.384 0.046 30.338 0.000 #&gt; .y1 1.007 0.034 29.508 0.000 #&gt; .x2 0.824 0.031 27.005 0.000 #&gt; .y2 0.685 0.026 26.343 0.000 #&gt; .x3 0.781 0.030 26.387 0.000 #&gt; .y3 0.595 0.023 25.454 0.000 #&gt; .x4 0.814 0.032 25.716 0.000 #&gt; .y4 0.639 0.026 25.011 0.000 #&gt; .x5 0.945 0.037 25.737 0.000 #&gt; .y5 0.694 0.028 25.003 0.000 #&gt; .x6 0.709 0.027 25.842 0.000 #&gt; .y6 0.581 0.023 24.975 0.000 #&gt; .x7 0.646 0.025 25.635 0.000 #&gt; .y7 0.523 0.021 25.036 0.000 #&gt; .x8 0.754 0.030 25.271 0.000 #&gt; .y8 0.626 0.025 24.953 0.000 #&gt; .x9 0.753 0.029 25.582 0.000 #&gt; .y9 0.646 0.026 24.747 0.000 #&gt; .x10 0.766 0.032 24.282 0.000 #&gt; .y10 0.651 0.028 23.475 0.000 #&gt; .x11 0.796 0.033 24.016 0.000 #&gt; .y11 0.620 0.027 22.840 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 26 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 117 #&gt; Number of equality constraints 59 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1230 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 4312.762 #&gt; Degrees of freedom 261 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; x1 ~ #&gt; oplx (e) 0.026 0.001 18.920 0.000 #&gt; x2 ~ #&gt; oplx (e) 0.026 0.001 18.920 0.000 #&gt; x3 ~ #&gt; oplx (e) 0.026 0.001 18.920 0.000 #&gt; x4 ~ #&gt; oplx (e) 0.026 0.001 18.920 0.000 #&gt; x5 ~ #&gt; oplx (e) 0.026 0.001 18.920 0.000 #&gt; x6 ~ #&gt; oplx (e) 0.026 0.001 18.920 0.000 #&gt; x7 ~ #&gt; oplx (e) 0.026 0.001 18.920 0.000 #&gt; x8 ~ #&gt; oplx (e) 0.026 0.001 18.920 0.000 #&gt; x9 ~ #&gt; oplx (e) 0.026 0.001 18.920 0.000 #&gt; x10 ~ #&gt; oplx (e) 0.026 0.001 18.920 0.000 #&gt; x11 ~ #&gt; oplx (e) 0.026 0.001 18.920 0.000 #&gt; y1 ~ #&gt; oply (e) 0.026 0.001 18.920 0.000 #&gt; y2 ~ #&gt; oply (e) 0.026 0.001 18.920 0.000 #&gt; y3 ~ #&gt; oply (e) 0.026 0.001 18.920 0.000 #&gt; y4 ~ #&gt; oply (e) 0.026 0.001 18.920 0.000 #&gt; y5 ~ #&gt; oply (e) 0.026 0.001 18.920 0.000 #&gt; y6 ~ #&gt; oply (e) 0.026 0.001 18.920 0.000 #&gt; y7 ~ #&gt; oply (e) 0.026 0.001 18.920 0.000 #&gt; y8 ~ #&gt; oply (e) 0.026 0.001 18.920 0.000 #&gt; y9 ~ #&gt; oply (e) 0.026 0.001 18.920 0.000 #&gt; y10 ~ #&gt; oply (e) 0.026 0.001 18.920 0.000 #&gt; y11 ~ #&gt; oply (e) 0.026 0.001 18.920 0.000 #&gt; x2 ~ #&gt; x1 (a) 0.570 0.005 117.122 0.000 #&gt; y1 (b) 0.164 0.005 32.553 0.000 #&gt; x3 ~ #&gt; x2 (a) 0.570 0.005 117.122 0.000 #&gt; y2 (b) 0.164 0.005 32.553 0.000 #&gt; x4 ~ #&gt; x3 (a) 0.570 0.005 117.122 0.000 #&gt; y3 (b) 0.164 0.005 32.553 0.000 #&gt; x5 ~ #&gt; x4 (a) 0.570 0.005 117.122 0.000 #&gt; y4 (b) 0.164 0.005 32.553 0.000 #&gt; x6 ~ #&gt; x5 (a) 0.570 0.005 117.122 0.000 #&gt; y5 (b) 0.164 0.005 32.553 0.000 #&gt; x7 ~ #&gt; x6 (a) 0.570 0.005 117.122 0.000 #&gt; y6 (b) 0.164 0.005 32.553 0.000 #&gt; x8 ~ #&gt; x7 (a) 0.570 0.005 117.122 0.000 #&gt; y7 (b) 0.164 0.005 32.553 0.000 #&gt; x9 ~ #&gt; x8 (a) 0.570 0.005 117.122 0.000 #&gt; y8 (b) 0.164 0.005 32.553 0.000 #&gt; x10 ~ #&gt; x9 (a) 0.570 0.005 117.122 0.000 #&gt; y9 (b) 0.164 0.005 32.553 0.000 #&gt; x11 ~ #&gt; x10 (a) 0.570 0.005 117.122 0.000 #&gt; y10 (b) 0.164 0.005 32.553 0.000 #&gt; y2 ~ #&gt; x1 (b) 0.164 0.005 32.553 0.000 #&gt; y1 (a) 0.570 0.005 117.122 0.000 #&gt; y3 ~ #&gt; x2 (b) 0.164 0.005 32.553 0.000 #&gt; y2 (a) 0.570 0.005 117.122 0.000 #&gt; y4 ~ #&gt; x3 (b) 0.164 0.005 32.553 0.000 #&gt; y3 (a) 0.570 0.005 117.122 0.000 #&gt; y5 ~ #&gt; x4 (b) 0.164 0.005 32.553 0.000 #&gt; y4 (a) 0.570 0.005 117.122 0.000 #&gt; y6 ~ #&gt; x5 (b) 0.164 0.005 32.553 0.000 #&gt; y5 (a) 0.570 0.005 117.122 0.000 #&gt; y7 ~ #&gt; x6 (b) 0.164 0.005 32.553 0.000 #&gt; y6 (a) 0.570 0.005 117.122 0.000 #&gt; y8 ~ #&gt; x7 (b) 0.164 0.005 32.553 0.000 #&gt; y7 (a) 0.570 0.005 117.122 0.000 #&gt; y9 ~ #&gt; x8 (b) 0.164 0.005 32.553 0.000 #&gt; y8 (a) 0.570 0.005 117.122 0.000 #&gt; y10 ~ #&gt; x9 (b) 0.164 0.005 32.553 0.000 #&gt; y9 (a) 0.570 0.005 117.122 0.000 #&gt; y11 ~ #&gt; x10 (b) 0.164 0.005 32.553 0.000 #&gt; y10 (a) 0.570 0.005 117.122 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 ~~ #&gt; .y1 0.383 0.023 16.619 0.000 #&gt; .x2 ~~ #&gt; .y2 0.101 0.015 6.622 0.000 #&gt; .x3 ~~ #&gt; .y3 0.104 0.015 6.806 0.000 #&gt; .x4 ~~ #&gt; .y4 0.071 0.014 5.067 0.000 #&gt; .x5 ~~ #&gt; .y5 0.086 0.014 6.103 0.000 #&gt; .x6 ~~ #&gt; .y6 0.074 0.014 5.391 0.000 #&gt; .x7 ~~ #&gt; .y7 0.055 0.014 4.035 0.000 #&gt; .x8 ~~ #&gt; .y8 0.075 0.015 5.121 0.000 #&gt; .x9 ~~ #&gt; .y9 0.040 0.015 2.684 0.007 #&gt; .x10 ~~ #&gt; .y10 0.089 0.015 5.887 0.000 #&gt; .x11 ~~ #&gt; .y11 0.102 0.015 6.774 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.892 0.028 32.245 0.000 #&gt; .y1 0.953 0.027 35.963 0.000 #&gt; .x2 0.026 0.025 1.038 0.299 #&gt; .y2 0.057 0.024 2.331 0.020 #&gt; .x3 -0.002 0.025 -0.089 0.929 #&gt; .y3 0.068 0.024 2.803 0.005 #&gt; .x4 -0.001 0.024 -0.058 0.953 #&gt; .y4 0.039 0.024 1.593 0.111 #&gt; .x5 -0.003 0.025 -0.140 0.888 #&gt; .y5 0.039 0.024 1.599 0.110 #&gt; .x6 -0.005 0.025 -0.192 0.848 #&gt; .y6 0.039 0.024 1.656 0.098 #&gt; .x7 0.018 0.025 0.708 0.479 #&gt; .y7 0.065 0.024 2.701 0.007 #&gt; .x8 -0.048 0.025 -1.921 0.055 #&gt; .y8 0.002 0.025 0.097 0.923 #&gt; .x9 -0.058 0.025 -2.280 0.023 #&gt; .y9 0.018 0.025 0.725 0.468 #&gt; .x10 0.103 0.026 3.933 0.000 #&gt; .y10 0.127 0.025 5.052 0.000 #&gt; .x11 -0.011 0.026 -0.420 0.675 #&gt; .y11 0.053 0.026 2.085 0.037 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.945 0.031 30.290 0.000 #&gt; .y1 0.870 0.029 30.158 0.000 #&gt; .x2 0.567 0.021 27.155 0.000 #&gt; .y2 0.551 0.020 27.179 0.000 #&gt; .x3 0.548 0.021 26.526 0.000 #&gt; .y3 0.504 0.019 26.218 0.000 #&gt; .x4 0.462 0.018 25.877 0.000 #&gt; .y4 0.485 0.019 25.485 0.000 #&gt; .x5 0.478 0.019 25.817 0.000 #&gt; .y5 0.462 0.018 25.165 0.000 #&gt; .x6 0.506 0.019 26.234 0.000 #&gt; .y6 0.442 0.017 26.326 0.000 #&gt; .x7 0.489 0.019 25.936 0.000 #&gt; .y7 0.450 0.017 26.082 0.000 #&gt; .x8 0.479 0.019 25.456 0.000 #&gt; .y8 0.483 0.019 25.545 0.000 #&gt; .x9 0.502 0.019 25.775 0.000 #&gt; .y9 0.506 0.020 25.464 0.000 #&gt; .x10 0.512 0.021 24.658 0.000 #&gt; .y10 0.454 0.019 24.272 0.000 #&gt; .x11 0.492 0.020 24.435 0.000 #&gt; .y11 0.451 0.019 24.036 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 74 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 117 #&gt; Number of equality constraints 59 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1290 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 5151.101 #&gt; Degrees of freedom 261 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; x1 ~ #&gt; oplx (e) 0.004 0.001 3.326 0.001 #&gt; x2 ~ #&gt; oplx (e) 0.004 0.001 3.326 0.001 #&gt; x3 ~ #&gt; oplx (e) 0.004 0.001 3.326 0.001 #&gt; x4 ~ #&gt; oplx (e) 0.004 0.001 3.326 0.001 #&gt; x5 ~ #&gt; oplx (e) 0.004 0.001 3.326 0.001 #&gt; x6 ~ #&gt; oplx (e) 0.004 0.001 3.326 0.001 #&gt; x7 ~ #&gt; oplx (e) 0.004 0.001 3.326 0.001 #&gt; x8 ~ #&gt; oplx (e) 0.004 0.001 3.326 0.001 #&gt; x9 ~ #&gt; oplx (e) 0.004 0.001 3.326 0.001 #&gt; x10 ~ #&gt; oplx (e) 0.004 0.001 3.326 0.001 #&gt; x11 ~ #&gt; oplx (e) 0.004 0.001 3.326 0.001 #&gt; y1 ~ #&gt; oply (e) 0.004 0.001 3.326 0.001 #&gt; y2 ~ #&gt; oply (e) 0.004 0.001 3.326 0.001 #&gt; y3 ~ #&gt; oply (e) 0.004 0.001 3.326 0.001 #&gt; y4 ~ #&gt; oply (e) 0.004 0.001 3.326 0.001 #&gt; y5 ~ #&gt; oply (e) 0.004 0.001 3.326 0.001 #&gt; y6 ~ #&gt; oply (e) 0.004 0.001 3.326 0.001 #&gt; y7 ~ #&gt; oply (e) 0.004 0.001 3.326 0.001 #&gt; y8 ~ #&gt; oply (e) 0.004 0.001 3.326 0.001 #&gt; y9 ~ #&gt; oply (e) 0.004 0.001 3.326 0.001 #&gt; y10 ~ #&gt; oply (e) 0.004 0.001 3.326 0.001 #&gt; y11 ~ #&gt; oply (e) 0.004 0.001 3.326 0.001 #&gt; x2 ~ #&gt; x1 (a) 0.680 0.004 158.832 0.000 #&gt; y1 (b) 0.182 0.004 41.580 0.000 #&gt; x3 ~ #&gt; x2 (a) 0.680 0.004 158.832 0.000 #&gt; y2 (b) 0.182 0.004 41.580 0.000 #&gt; x4 ~ #&gt; x3 (a) 0.680 0.004 158.832 0.000 #&gt; y3 (b) 0.182 0.004 41.580 0.000 #&gt; x5 ~ #&gt; x4 (a) 0.680 0.004 158.832 0.000 #&gt; y4 (b) 0.182 0.004 41.580 0.000 #&gt; x6 ~ #&gt; x5 (a) 0.680 0.004 158.832 0.000 #&gt; y5 (b) 0.182 0.004 41.580 0.000 #&gt; x7 ~ #&gt; x6 (a) 0.680 0.004 158.832 0.000 #&gt; y6 (b) 0.182 0.004 41.580 0.000 #&gt; x8 ~ #&gt; x7 (a) 0.680 0.004 158.832 0.000 #&gt; y7 (b) 0.182 0.004 41.580 0.000 #&gt; x9 ~ #&gt; x8 (a) 0.680 0.004 158.832 0.000 #&gt; y8 (b) 0.182 0.004 41.580 0.000 #&gt; x10 ~ #&gt; x9 (a) 0.680 0.004 158.832 0.000 #&gt; y9 (b) 0.182 0.004 41.580 0.000 #&gt; x11 ~ #&gt; x10 (a) 0.680 0.004 158.832 0.000 #&gt; y10 (b) 0.182 0.004 41.580 0.000 #&gt; y2 ~ #&gt; x1 (b) 0.182 0.004 41.580 0.000 #&gt; y1 (a) 0.680 0.004 158.832 0.000 #&gt; y3 ~ #&gt; x2 (b) 0.182 0.004 41.580 0.000 #&gt; y2 (a) 0.680 0.004 158.832 0.000 #&gt; y4 ~ #&gt; x3 (b) 0.182 0.004 41.580 0.000 #&gt; y3 (a) 0.680 0.004 158.832 0.000 #&gt; y5 ~ #&gt; x4 (b) 0.182 0.004 41.580 0.000 #&gt; y4 (a) 0.680 0.004 158.832 0.000 #&gt; y6 ~ #&gt; x5 (b) 0.182 0.004 41.580 0.000 #&gt; y5 (a) 0.680 0.004 158.832 0.000 #&gt; y7 ~ #&gt; x6 (b) 0.182 0.004 41.580 0.000 #&gt; y6 (a) 0.680 0.004 158.832 0.000 #&gt; y8 ~ #&gt; x7 (b) 0.182 0.004 41.580 0.000 #&gt; y7 (a) 0.680 0.004 158.832 0.000 #&gt; y9 ~ #&gt; x8 (b) 0.182 0.004 41.580 0.000 #&gt; y8 (a) 0.680 0.004 158.832 0.000 #&gt; y10 ~ #&gt; x9 (b) 0.182 0.004 41.580 0.000 #&gt; y9 (a) 0.680 0.004 158.832 0.000 #&gt; y11 ~ #&gt; x10 (b) 0.182 0.004 41.580 0.000 #&gt; y10 (a) 0.680 0.004 158.832 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 ~~ #&gt; .y1 0.658 0.029 22.943 0.000 #&gt; .x2 ~~ #&gt; .y2 0.081 0.012 6.912 0.000 #&gt; .x3 ~~ #&gt; .y3 0.086 0.011 7.800 0.000 #&gt; .x4 ~~ #&gt; .y4 0.086 0.011 7.867 0.000 #&gt; .x5 ~~ #&gt; .y5 0.077 0.010 7.447 0.000 #&gt; .x6 ~~ #&gt; .y6 0.066 0.009 7.440 0.000 #&gt; .x7 ~~ #&gt; .y7 0.057 0.009 6.176 0.000 #&gt; .x8 ~~ #&gt; .y8 0.063 0.010 6.072 0.000 #&gt; .x9 ~~ #&gt; .y9 0.074 0.010 7.093 0.000 #&gt; .x10 ~~ #&gt; .y10 0.096 0.013 7.554 0.000 #&gt; .x11 ~~ #&gt; .y11 0.101 0.012 8.407 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 4.256 0.026 161.422 0.000 #&gt; .y1 4.274 0.026 161.379 0.000 #&gt; .x2 0.559 0.027 20.780 0.000 #&gt; .y2 0.587 0.026 22.511 0.000 #&gt; .x3 0.537 0.026 20.488 0.000 #&gt; .y3 0.529 0.027 19.968 0.000 #&gt; .x4 0.579 0.026 22.213 0.000 #&gt; .y4 0.601 0.027 22.523 0.000 #&gt; .x5 0.567 0.026 21.500 0.000 #&gt; .y5 0.581 0.026 22.174 0.000 #&gt; .x6 0.569 0.026 21.651 0.000 #&gt; .y6 0.572 0.025 22.816 0.000 #&gt; .x7 0.587 0.026 22.663 0.000 #&gt; .y7 0.587 0.026 22.866 0.000 #&gt; .x8 0.570 0.026 21.537 0.000 #&gt; .y8 0.575 0.027 21.666 0.000 #&gt; .x9 0.509 0.027 18.825 0.000 #&gt; .y9 0.535 0.026 20.449 0.000 #&gt; .x10 0.543 0.028 19.517 0.000 #&gt; .y10 0.544 0.027 20.026 0.000 #&gt; .x11 0.597 0.028 21.710 0.000 #&gt; .y11 0.600 0.027 21.817 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 1.060 0.034 31.095 0.000 #&gt; .y1 1.113 0.036 30.875 0.000 #&gt; .x2 0.440 0.016 26.862 0.000 #&gt; .y2 0.392 0.015 26.637 0.000 #&gt; .x3 0.359 0.014 26.181 0.000 #&gt; .y3 0.408 0.016 25.925 0.000 #&gt; .x4 0.338 0.013 26.091 0.000 #&gt; .y4 0.403 0.016 25.781 0.000 #&gt; .x5 0.339 0.013 25.647 0.000 #&gt; .y5 0.347 0.014 25.241 0.000 #&gt; .x6 0.344 0.013 26.462 0.000 #&gt; .y6 0.272 0.011 25.709 0.000 #&gt; .x7 0.308 0.012 25.948 0.000 #&gt; .y7 0.308 0.012 25.196 0.000 #&gt; .x8 0.325 0.013 25.093 0.000 #&gt; .y8 0.351 0.014 24.731 0.000 #&gt; .x9 0.367 0.015 25.327 0.000 #&gt; .y9 0.321 0.013 25.158 0.000 #&gt; .x10 0.394 0.017 23.697 0.000 #&gt; .y10 0.360 0.015 23.902 0.000 #&gt; .x11 0.362 0.015 24.067 0.000 #&gt; .y11 0.367 0.015 23.792 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 48 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 117 #&gt; Number of equality constraints 59 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1309 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 4211.139 #&gt; Degrees of freedom 261 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; x1 ~ #&gt; oplx (e) -0.017 0.001 -11.913 0.000 #&gt; x2 ~ #&gt; oplx (e) -0.017 0.001 -11.913 0.000 #&gt; x3 ~ #&gt; oplx (e) -0.017 0.001 -11.913 0.000 #&gt; x4 ~ #&gt; oplx (e) -0.017 0.001 -11.913 0.000 #&gt; x5 ~ #&gt; oplx (e) -0.017 0.001 -11.913 0.000 #&gt; x6 ~ #&gt; oplx (e) -0.017 0.001 -11.913 0.000 #&gt; x7 ~ #&gt; oplx (e) -0.017 0.001 -11.913 0.000 #&gt; x8 ~ #&gt; oplx (e) -0.017 0.001 -11.913 0.000 #&gt; x9 ~ #&gt; oplx (e) -0.017 0.001 -11.913 0.000 #&gt; x10 ~ #&gt; oplx (e) -0.017 0.001 -11.913 0.000 #&gt; x11 ~ #&gt; oplx (e) -0.017 0.001 -11.913 0.000 #&gt; y1 ~ #&gt; oply (e) -0.017 0.001 -11.913 0.000 #&gt; y2 ~ #&gt; oply (e) -0.017 0.001 -11.913 0.000 #&gt; y3 ~ #&gt; oply (e) -0.017 0.001 -11.913 0.000 #&gt; y4 ~ #&gt; oply (e) -0.017 0.001 -11.913 0.000 #&gt; y5 ~ #&gt; oply (e) -0.017 0.001 -11.913 0.000 #&gt; y6 ~ #&gt; oply (e) -0.017 0.001 -11.913 0.000 #&gt; y7 ~ #&gt; oply (e) -0.017 0.001 -11.913 0.000 #&gt; y8 ~ #&gt; oply (e) -0.017 0.001 -11.913 0.000 #&gt; y9 ~ #&gt; oply (e) -0.017 0.001 -11.913 0.000 #&gt; y10 ~ #&gt; oply (e) -0.017 0.001 -11.913 0.000 #&gt; y11 ~ #&gt; oply (e) -0.017 0.001 -11.913 0.000 #&gt; x2 ~ #&gt; x1 (a) 0.576 0.005 117.622 0.000 #&gt; y1 (b) 0.146 0.005 28.545 0.000 #&gt; x3 ~ #&gt; x2 (a) 0.576 0.005 117.622 0.000 #&gt; y2 (b) 0.146 0.005 28.545 0.000 #&gt; x4 ~ #&gt; x3 (a) 0.576 0.005 117.622 0.000 #&gt; y3 (b) 0.146 0.005 28.545 0.000 #&gt; x5 ~ #&gt; x4 (a) 0.576 0.005 117.622 0.000 #&gt; y4 (b) 0.146 0.005 28.545 0.000 #&gt; x6 ~ #&gt; x5 (a) 0.576 0.005 117.622 0.000 #&gt; y5 (b) 0.146 0.005 28.545 0.000 #&gt; x7 ~ #&gt; x6 (a) 0.576 0.005 117.622 0.000 #&gt; y6 (b) 0.146 0.005 28.545 0.000 #&gt; x8 ~ #&gt; x7 (a) 0.576 0.005 117.622 0.000 #&gt; y7 (b) 0.146 0.005 28.545 0.000 #&gt; x9 ~ #&gt; x8 (a) 0.576 0.005 117.622 0.000 #&gt; y8 (b) 0.146 0.005 28.545 0.000 #&gt; x10 ~ #&gt; x9 (a) 0.576 0.005 117.622 0.000 #&gt; y9 (b) 0.146 0.005 28.545 0.000 #&gt; x11 ~ #&gt; x10 (a) 0.576 0.005 117.622 0.000 #&gt; y10 (b) 0.146 0.005 28.545 0.000 #&gt; y2 ~ #&gt; x1 (b) 0.146 0.005 28.545 0.000 #&gt; y1 (a) 0.576 0.005 117.622 0.000 #&gt; y3 ~ #&gt; x2 (b) 0.146 0.005 28.545 0.000 #&gt; y2 (a) 0.576 0.005 117.622 0.000 #&gt; y4 ~ #&gt; x3 (b) 0.146 0.005 28.545 0.000 #&gt; y3 (a) 0.576 0.005 117.622 0.000 #&gt; y5 ~ #&gt; x4 (b) 0.146 0.005 28.545 0.000 #&gt; y4 (a) 0.576 0.005 117.622 0.000 #&gt; y6 ~ #&gt; x5 (b) 0.146 0.005 28.545 0.000 #&gt; y5 (a) 0.576 0.005 117.622 0.000 #&gt; y7 ~ #&gt; x6 (b) 0.146 0.005 28.545 0.000 #&gt; y6 (a) 0.576 0.005 117.622 0.000 #&gt; y8 ~ #&gt; x7 (b) 0.146 0.005 28.545 0.000 #&gt; y7 (a) 0.576 0.005 117.622 0.000 #&gt; y9 ~ #&gt; x8 (b) 0.146 0.005 28.545 0.000 #&gt; y8 (a) 0.576 0.005 117.622 0.000 #&gt; y10 ~ #&gt; x9 (b) 0.146 0.005 28.545 0.000 #&gt; y9 (a) 0.576 0.005 117.622 0.000 #&gt; y11 ~ #&gt; x10 (b) 0.146 0.005 28.545 0.000 #&gt; y10 (a) 0.576 0.005 117.622 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 ~~ #&gt; .y1 0.421 0.025 16.651 0.000 #&gt; .x2 ~~ #&gt; .y2 0.140 0.016 8.804 0.000 #&gt; .x3 ~~ #&gt; .y3 0.099 0.016 6.283 0.000 #&gt; .x4 ~~ #&gt; .y4 0.110 0.017 6.538 0.000 #&gt; .x5 ~~ #&gt; .y5 0.062 0.014 4.389 0.000 #&gt; .x6 ~~ #&gt; .y6 0.132 0.016 8.277 0.000 #&gt; .x7 ~~ #&gt; .y7 0.092 0.014 6.442 0.000 #&gt; .x8 ~~ #&gt; .y8 0.084 0.016 5.379 0.000 #&gt; .x9 ~~ #&gt; .y9 0.103 0.017 6.144 0.000 #&gt; .x10 ~~ #&gt; .y10 0.094 0.017 5.492 0.000 #&gt; .x11 ~~ #&gt; .y11 0.069 0.017 3.957 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 3.856 0.029 130.976 0.000 #&gt; .y1 4.007 0.027 146.050 0.000 #&gt; .x2 1.238 0.035 35.035 0.000 #&gt; .y2 1.350 0.034 39.374 0.000 #&gt; .x3 1.210 0.035 34.432 0.000 #&gt; .y3 1.249 0.035 35.770 0.000 #&gt; .x4 1.210 0.036 34.057 0.000 #&gt; .y4 1.294 0.035 37.093 0.000 #&gt; .x5 1.328 0.035 38.257 0.000 #&gt; .y5 1.345 0.034 39.097 0.000 #&gt; .x6 1.135 0.036 31.683 0.000 #&gt; .y6 1.215 0.035 34.565 0.000 #&gt; .x7 1.227 0.035 35.149 0.000 #&gt; .y7 1.269 0.034 37.270 0.000 #&gt; .x8 1.276 0.036 35.898 0.000 #&gt; .y8 1.323 0.035 37.776 0.000 #&gt; .x9 1.262 0.036 35.163 0.000 #&gt; .y9 1.286 0.036 36.048 0.000 #&gt; .x10 1.252 0.036 34.580 0.000 #&gt; .y10 1.253 0.036 34.746 0.000 #&gt; .x11 1.303 0.037 35.598 0.000 #&gt; .y11 1.283 0.036 35.215 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 1.095 0.036 30.386 0.000 #&gt; .y1 0.910 0.030 30.280 0.000 #&gt; .x2 0.608 0.022 27.022 0.000 #&gt; .y2 0.541 0.020 27.090 0.000 #&gt; .x3 0.541 0.021 26.339 0.000 #&gt; .y3 0.555 0.021 26.223 0.000 #&gt; .x4 0.565 0.022 25.748 0.000 #&gt; .y4 0.532 0.021 25.544 0.000 #&gt; .x5 0.468 0.018 25.316 0.000 #&gt; .y5 0.471 0.019 25.266 0.000 #&gt; .x6 0.562 0.021 26.194 0.000 #&gt; .y6 0.532 0.020 26.158 0.000 #&gt; .x7 0.502 0.019 25.972 0.000 #&gt; .y7 0.444 0.017 25.676 0.000 #&gt; .x8 0.535 0.021 25.603 0.000 #&gt; .y8 0.505 0.020 25.362 0.000 #&gt; .x9 0.552 0.021 25.758 0.000 #&gt; .y9 0.542 0.021 25.218 0.000 #&gt; .x10 0.532 0.022 24.556 0.000 #&gt; .y10 0.531 0.022 24.152 0.000 #&gt; .x11 0.543 0.022 24.310 0.000 #&gt; .y11 0.525 0.022 23.738 0.000 3.6.4.2 RI-CLPM RICLPM &lt;- &#39; # Create between components (random intercepts) RIx =~ 1*x1 + 1*x2 + 1*x3 + 1*x4 + 1*x5 + 1*x6 + 1*x7 + 1*x8 + 1*x9 + 1*x10 + 1*x11 RIy =~ 1*y1 + 1*y2 + 1*y3 + 1*y4 + 1*y5 + 1*y6 + 1*y7 + 1*y8 + 1*y9 + 1*y10 + 1*y11 RIx ~ e*oplx RIy ~ e*oply # Create within-person centered variables wx1 =~ 1*x1 wx2 =~ 1*x2 wx3 =~ 1*x3 wx4 =~ 1*x4 wx5 =~ 1*x5 wx6 =~ 1*x6 wx7 =~ 1*x7 wx8 =~ 1*x8 wx9 =~ 1*x9 wx10 =~ 1*x10 wx11 =~ 1*x11 wy1 =~ 1*y1 wy2 =~ 1*y2 wy3 =~ 1*y3 wy4 =~ 1*y4 wy5 =~ 1*y5 wy6 =~ 1*y6 wy7 =~ 1*y7 wy8 =~ 1*y8 wy9 =~ 1*y9 wy10 =~ 1*y10 wy11 =~ 1*y11 # Estimate the lagged effects between the within-person centered variables. wx2 ~ a*wx1 + b*wy1 wx3 ~ a*wx2 + b*wy2 wx4 ~ a*wx3 + b*wy3 wx5 ~ a*wx4 + b*wy4 wx6 ~ a*wx5 + b*wy5 wx7 ~ a*wx6 + b*wy6 wx8 ~ a*wx7 + b*wy7 wx9 ~ a*wx8 + b*wy8 wx10 ~ a*wx9 + b*wy9 wx11 ~ a*wx10 + b*wy10 wy2 ~ b*wx1 + a*wy1 wy3 ~ b*wx2 + a*wy2 wy4 ~ b*wx3 + a*wy3 wy5 ~ b*wx4 + a*wy4 wy6 ~ b*wx5 + a*wy5 wy7 ~ b*wx6 + a*wy6 wy8 ~ b*wx7 + a*wy7 wy9 ~ b*wx8 + a*wy8 wy10 ~ b*wx9 + a*wy9 wy11 ~ b*wx10 + a*wy10 # Estimate the (residual) covariance between the within-person centered variables wx1 ~~ wy1 # Covariance wx2 ~~ wy2 wx3 ~~ wy3 wx4 ~~ wy4 wx5 ~~ wy5 wx6 ~~ wy6 wx7 ~~ wy7 wx8 ~~ wy8 wx9 ~~ wy9 wx10 ~~ wy10 wx11 ~~ wy11 # Estimate the variance and covariance of the random intercepts. RIx ~~ RIx RIy ~~ RIy RIx ~~ RIy # Estimate the (residual) variance of the within-person centered variables. wx1 ~~ wx1 # Variances wy1 ~~ wy1 wx2 ~~ wx2 # Residual variances wy2 ~~ wy2 wx3 ~~ wx3 wy3 ~~ wy3 wx4 ~~ wx4 wy4 ~~ wy4 wx5 ~~ wx5 wy5 ~~ wy5 wx6 ~~ wx6 wy6 ~~ wy6 wx7 ~~ wx7 wy7 ~~ wy7 wx8 ~~ wx8 wy8 ~~ wy8 wx9 ~~ wx9 wy9 ~~ wy9 wx10 ~~ wx10 wy10 ~~ wy10 wx11 ~~ wx11 wy11 ~~ wy11 &#39; #Estimate models a bit faster: estimate &lt;- function(x) lavaan(RICLPM, data=x, missing = &quot;fiml.x&quot;, meanstructure = T ) library(future.apply) plan(multisession) results_temp &lt;- future_lapply(datalist_ori, estimate) results[[5]] &lt;- results_temp[[1]] results[[6]] &lt;- results_temp[[2]] results[[7]] &lt;- results_temp[[3]] results[[8]] &lt;- results_temp[[4]] names(results)[5:8] &lt;- c(&quot;fitm2h1y1&quot;, &quot;fitm2h1y2&quot;,&quot;fitm2h1y3&quot;,&quot;fitm2h1y4&quot;) load(&quot;addfiles/results.Rdata&quot;) summary(results[[5]]) summary(results[[6]]) summary(results[[7]]) summary(results[[8]]) #&gt; lavaan 0.6-9 ended normally after 42 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 78 #&gt; Number of equality constraints 39 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1415 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 1632.289 #&gt; Degrees of freedom 280 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (e) 0.122 0.001 107.040 0.000 #&gt; RIy ~ #&gt; oply (e) 0.122 0.001 107.040 0.000 #&gt; wx2 ~ #&gt; wx1 (a) 0.203 0.008 24.374 0.000 #&gt; wy1 (b) 0.075 0.009 8.629 0.000 #&gt; wx3 ~ #&gt; wx2 (a) 0.203 0.008 24.374 0.000 #&gt; wy2 (b) 0.075 0.009 8.629 0.000 #&gt; wx4 ~ #&gt; wx3 (a) 0.203 0.008 24.374 0.000 #&gt; wy3 (b) 0.075 0.009 8.629 0.000 #&gt; wx5 ~ #&gt; wx4 (a) 0.203 0.008 24.374 0.000 #&gt; wy4 (b) 0.075 0.009 8.629 0.000 #&gt; wx6 ~ #&gt; wx5 (a) 0.203 0.008 24.374 0.000 #&gt; wy5 (b) 0.075 0.009 8.629 0.000 #&gt; wx7 ~ #&gt; wx6 (a) 0.203 0.008 24.374 0.000 #&gt; wy6 (b) 0.075 0.009 8.629 0.000 #&gt; wx8 ~ #&gt; wx7 (a) 0.203 0.008 24.374 0.000 #&gt; wy7 (b) 0.075 0.009 8.629 0.000 #&gt; wx9 ~ #&gt; wx8 (a) 0.203 0.008 24.374 0.000 #&gt; wy8 (b) 0.075 0.009 8.629 0.000 #&gt; wx10 ~ #&gt; wx9 (a) 0.203 0.008 24.374 0.000 #&gt; wy9 (b) 0.075 0.009 8.629 0.000 #&gt; wx11 ~ #&gt; wx10 (a) 0.203 0.008 24.374 0.000 #&gt; wy10 (b) 0.075 0.009 8.629 0.000 #&gt; wy2 ~ #&gt; wx1 (b) 0.075 0.009 8.629 0.000 #&gt; wy1 (a) 0.203 0.008 24.374 0.000 #&gt; wy3 ~ #&gt; wx2 (b) 0.075 0.009 8.629 0.000 #&gt; wy2 (a) 0.203 0.008 24.374 0.000 #&gt; wy4 ~ #&gt; wx3 (b) 0.075 0.009 8.629 0.000 #&gt; wy3 (a) 0.203 0.008 24.374 0.000 #&gt; wy5 ~ #&gt; wx4 (b) 0.075 0.009 8.629 0.000 #&gt; wy4 (a) 0.203 0.008 24.374 0.000 #&gt; wy6 ~ #&gt; wx5 (b) 0.075 0.009 8.629 0.000 #&gt; wy5 (a) 0.203 0.008 24.374 0.000 #&gt; wy7 ~ #&gt; wx6 (b) 0.075 0.009 8.629 0.000 #&gt; wy6 (a) 0.203 0.008 24.374 0.000 #&gt; wy8 ~ #&gt; wx7 (b) 0.075 0.009 8.629 0.000 #&gt; wy7 (a) 0.203 0.008 24.374 0.000 #&gt; wy9 ~ #&gt; wx8 (b) 0.075 0.009 8.629 0.000 #&gt; wy8 (a) 0.203 0.008 24.374 0.000 #&gt; wy10 ~ #&gt; wx9 (b) 0.075 0.009 8.629 0.000 #&gt; wy9 (a) 0.203 0.008 24.374 0.000 #&gt; wy11 ~ #&gt; wx10 (b) 0.075 0.009 8.629 0.000 #&gt; wy10 (a) 0.203 0.008 24.374 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.120 0.022 5.386 0.000 #&gt; .wx2 ~~ #&gt; .wy2 0.118 0.020 5.771 0.000 #&gt; .wx3 ~~ #&gt; .wy3 0.121 0.019 6.356 0.000 #&gt; .wx4 ~~ #&gt; .wy4 0.115 0.019 6.072 0.000 #&gt; .wx5 ~~ #&gt; .wy5 0.104 0.022 4.757 0.000 #&gt; .wx6 ~~ #&gt; .wy6 0.073 0.017 4.381 0.000 #&gt; .wx7 ~~ #&gt; .wy7 0.100 0.016 6.324 0.000 #&gt; .wx8 ~~ #&gt; .wy8 0.083 0.018 4.541 0.000 #&gt; .wx9 ~~ #&gt; .wy9 0.102 0.019 5.241 0.000 #&gt; .wx10 ~~ #&gt; .wy10 0.043 0.020 2.127 0.033 #&gt; .wx11 ~~ #&gt; .wy11 0.101 0.021 4.850 0.000 #&gt; .RIx ~~ #&gt; .RIy 0.354 0.017 21.005 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; .wx2 0.000 #&gt; .wx3 0.000 #&gt; .wx4 0.000 #&gt; .wx5 0.000 #&gt; .wx6 0.000 #&gt; .wx7 0.000 #&gt; .wx8 0.000 #&gt; .wx9 0.000 #&gt; .wx10 0.000 #&gt; .wx11 0.000 #&gt; wy1 0.000 #&gt; .wy2 0.000 #&gt; .wy3 0.000 #&gt; .wy4 0.000 #&gt; .wy5 0.000 #&gt; .wy6 0.000 #&gt; .wy7 0.000 #&gt; .wy8 0.000 #&gt; .wy9 0.000 #&gt; .wy10 0.000 #&gt; .wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 0.757 0.025 30.317 0.000 #&gt; .RIy 0.488 0.018 27.518 0.000 #&gt; wx1 0.717 0.031 22.848 0.000 #&gt; wy1 0.618 0.027 22.504 0.000 #&gt; .wx2 0.712 0.029 24.248 0.000 #&gt; .wy2 0.588 0.025 23.509 0.000 #&gt; .wx3 0.614 0.026 23.815 0.000 #&gt; .wy3 0.533 0.023 23.030 0.000 #&gt; .wx4 0.604 0.027 22.679 0.000 #&gt; .wy4 0.490 0.022 21.934 0.000 #&gt; .wx5 0.758 0.032 24.016 0.000 #&gt; .wy5 0.567 0.024 23.286 0.000 #&gt; .wx6 0.510 0.022 22.955 0.000 #&gt; .wy6 0.432 0.019 22.186 0.000 #&gt; .wx7 0.488 0.021 22.844 0.000 #&gt; .wy7 0.444 0.020 22.585 0.000 #&gt; .wx8 0.546 0.024 23.023 0.000 #&gt; .wy8 0.505 0.022 22.795 0.000 #&gt; .wx9 0.604 0.026 23.063 0.000 #&gt; .wy9 0.510 0.023 22.176 0.000 #&gt; .wx10 0.562 0.026 21.674 0.000 #&gt; .wy10 0.511 0.024 21.292 0.000 #&gt; .wx11 0.612 0.028 21.517 0.000 #&gt; .wy11 0.519 0.025 20.641 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 39 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 78 #&gt; Number of equality constraints 39 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1230 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 833.630 #&gt; Degrees of freedom 280 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (e) 0.104 0.001 103.001 0.000 #&gt; RIy ~ #&gt; oply (e) 0.104 0.001 103.001 0.000 #&gt; wx2 ~ #&gt; wx1 (a) 0.102 0.008 12.802 0.000 #&gt; wy1 (b) 0.030 0.008 3.548 0.000 #&gt; wx3 ~ #&gt; wx2 (a) 0.102 0.008 12.802 0.000 #&gt; wy2 (b) 0.030 0.008 3.548 0.000 #&gt; wx4 ~ #&gt; wx3 (a) 0.102 0.008 12.802 0.000 #&gt; wy3 (b) 0.030 0.008 3.548 0.000 #&gt; wx5 ~ #&gt; wx4 (a) 0.102 0.008 12.802 0.000 #&gt; wy4 (b) 0.030 0.008 3.548 0.000 #&gt; wx6 ~ #&gt; wx5 (a) 0.102 0.008 12.802 0.000 #&gt; wy5 (b) 0.030 0.008 3.548 0.000 #&gt; wx7 ~ #&gt; wx6 (a) 0.102 0.008 12.802 0.000 #&gt; wy6 (b) 0.030 0.008 3.548 0.000 #&gt; wx8 ~ #&gt; wx7 (a) 0.102 0.008 12.802 0.000 #&gt; wy7 (b) 0.030 0.008 3.548 0.000 #&gt; wx9 ~ #&gt; wx8 (a) 0.102 0.008 12.802 0.000 #&gt; wy8 (b) 0.030 0.008 3.548 0.000 #&gt; wx10 ~ #&gt; wx9 (a) 0.102 0.008 12.802 0.000 #&gt; wy9 (b) 0.030 0.008 3.548 0.000 #&gt; wx11 ~ #&gt; wx10 (a) 0.102 0.008 12.802 0.000 #&gt; wy10 (b) 0.030 0.008 3.548 0.000 #&gt; wy2 ~ #&gt; wx1 (b) 0.030 0.008 3.548 0.000 #&gt; wy1 (a) 0.102 0.008 12.802 0.000 #&gt; wy3 ~ #&gt; wx2 (b) 0.030 0.008 3.548 0.000 #&gt; wy2 (a) 0.102 0.008 12.802 0.000 #&gt; wy4 ~ #&gt; wx3 (b) 0.030 0.008 3.548 0.000 #&gt; wy3 (a) 0.102 0.008 12.802 0.000 #&gt; wy5 ~ #&gt; wx4 (b) 0.030 0.008 3.548 0.000 #&gt; wy4 (a) 0.102 0.008 12.802 0.000 #&gt; wy6 ~ #&gt; wx5 (b) 0.030 0.008 3.548 0.000 #&gt; wy5 (a) 0.102 0.008 12.802 0.000 #&gt; wy7 ~ #&gt; wx6 (b) 0.030 0.008 3.548 0.000 #&gt; wy6 (a) 0.102 0.008 12.802 0.000 #&gt; wy8 ~ #&gt; wx7 (b) 0.030 0.008 3.548 0.000 #&gt; wy7 (a) 0.102 0.008 12.802 0.000 #&gt; wy9 ~ #&gt; wx8 (b) 0.030 0.008 3.548 0.000 #&gt; wy8 (a) 0.102 0.008 12.802 0.000 #&gt; wy10 ~ #&gt; wx9 (b) 0.030 0.008 3.548 0.000 #&gt; wy9 (a) 0.102 0.008 12.802 0.000 #&gt; wy11 ~ #&gt; wx10 (b) 0.030 0.008 3.548 0.000 #&gt; wy10 (a) 0.102 0.008 12.802 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.052 0.013 4.087 0.000 #&gt; .wx2 ~~ #&gt; .wy2 0.047 0.013 3.720 0.000 #&gt; .wx3 ~~ #&gt; .wy3 0.030 0.011 2.687 0.007 #&gt; .wx4 ~~ #&gt; .wy4 0.045 0.011 4.062 0.000 #&gt; .wx5 ~~ #&gt; .wy5 0.044 0.011 4.057 0.000 #&gt; .wx6 ~~ #&gt; .wy6 0.026 0.011 2.404 0.016 #&gt; .wx7 ~~ #&gt; .wy7 0.034 0.011 2.992 0.003 #&gt; .wx8 ~~ #&gt; .wy8 0.032 0.011 2.834 0.005 #&gt; .wx9 ~~ #&gt; .wy9 0.012 0.013 0.959 0.337 #&gt; .wx10 ~~ #&gt; .wy10 0.048 0.013 3.688 0.000 #&gt; .wx11 ~~ #&gt; .wy11 0.062 0.013 4.865 0.000 #&gt; .RIx ~~ #&gt; .RIy 0.308 0.013 23.781 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; .wx2 0.000 #&gt; .wx3 0.000 #&gt; .wx4 0.000 #&gt; .wx5 0.000 #&gt; .wx6 0.000 #&gt; .wx7 0.000 #&gt; .wx8 0.000 #&gt; .wx9 0.000 #&gt; .wx10 0.000 #&gt; .wx11 0.000 #&gt; wy1 0.000 #&gt; .wy2 0.000 #&gt; .wy3 0.000 #&gt; .wy4 0.000 #&gt; .wy5 0.000 #&gt; .wy6 0.000 #&gt; .wy7 0.000 #&gt; .wy8 0.000 #&gt; .wy9 0.000 #&gt; .wy10 0.000 #&gt; .wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 0.535 0.017 32.040 0.000 #&gt; .RIy 0.500 0.016 31.781 0.000 #&gt; wx1 0.449 0.019 23.866 0.000 #&gt; wy1 0.374 0.016 23.272 0.000 #&gt; .wx2 0.406 0.017 24.105 0.000 #&gt; .wy2 0.428 0.017 24.616 0.000 #&gt; .wx3 0.353 0.015 23.825 0.000 #&gt; .wy3 0.319 0.014 23.389 0.000 #&gt; .wx4 0.327 0.014 22.780 0.000 #&gt; .wy4 0.328 0.014 22.674 0.000 #&gt; .wx5 0.329 0.014 23.605 0.000 #&gt; .wy5 0.337 0.014 23.534 0.000 #&gt; .wx6 0.355 0.015 23.698 0.000 #&gt; .wy6 0.308 0.013 23.315 0.000 #&gt; .wx7 0.352 0.015 23.524 0.000 #&gt; .wy7 0.332 0.014 23.601 0.000 #&gt; .wx8 0.327 0.014 22.999 0.000 #&gt; .wy8 0.331 0.014 23.197 0.000 #&gt; .wx9 0.374 0.016 23.077 0.000 #&gt; .wy9 0.376 0.016 22.787 0.000 #&gt; .wx10 0.375 0.017 22.119 0.000 #&gt; .wy10 0.343 0.016 21.618 0.000 #&gt; .wx11 0.363 0.017 21.477 0.000 #&gt; .wy11 0.329 0.016 20.681 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 57 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 78 #&gt; Number of equality constraints 39 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1290 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 5881.742 #&gt; Degrees of freedom 280 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (e) 0.334 0.002 181.923 0.000 #&gt; RIy ~ #&gt; oply (e) 0.334 0.002 181.923 0.000 #&gt; wx2 ~ #&gt; wx1 (a) 0.116 0.008 14.473 0.000 #&gt; wy1 (b) 0.042 0.008 4.918 0.000 #&gt; wx3 ~ #&gt; wx2 (a) 0.116 0.008 14.473 0.000 #&gt; wy2 (b) 0.042 0.008 4.918 0.000 #&gt; wx4 ~ #&gt; wx3 (a) 0.116 0.008 14.473 0.000 #&gt; wy3 (b) 0.042 0.008 4.918 0.000 #&gt; wx5 ~ #&gt; wx4 (a) 0.116 0.008 14.473 0.000 #&gt; wy4 (b) 0.042 0.008 4.918 0.000 #&gt; wx6 ~ #&gt; wx5 (a) 0.116 0.008 14.473 0.000 #&gt; wy5 (b) 0.042 0.008 4.918 0.000 #&gt; wx7 ~ #&gt; wx6 (a) 0.116 0.008 14.473 0.000 #&gt; wy6 (b) 0.042 0.008 4.918 0.000 #&gt; wx8 ~ #&gt; wx7 (a) 0.116 0.008 14.473 0.000 #&gt; wy7 (b) 0.042 0.008 4.918 0.000 #&gt; wx9 ~ #&gt; wx8 (a) 0.116 0.008 14.473 0.000 #&gt; wy8 (b) 0.042 0.008 4.918 0.000 #&gt; wx10 ~ #&gt; wx9 (a) 0.116 0.008 14.473 0.000 #&gt; wy9 (b) 0.042 0.008 4.918 0.000 #&gt; wx11 ~ #&gt; wx10 (a) 0.116 0.008 14.473 0.000 #&gt; wy10 (b) 0.042 0.008 4.918 0.000 #&gt; wy2 ~ #&gt; wx1 (b) 0.042 0.008 4.918 0.000 #&gt; wy1 (a) 0.116 0.008 14.473 0.000 #&gt; wy3 ~ #&gt; wx2 (b) 0.042 0.008 4.918 0.000 #&gt; wy2 (a) 0.116 0.008 14.473 0.000 #&gt; wy4 ~ #&gt; wx3 (b) 0.042 0.008 4.918 0.000 #&gt; wy3 (a) 0.116 0.008 14.473 0.000 #&gt; wy5 ~ #&gt; wx4 (b) 0.042 0.008 4.918 0.000 #&gt; wy4 (a) 0.116 0.008 14.473 0.000 #&gt; wy6 ~ #&gt; wx5 (b) 0.042 0.008 4.918 0.000 #&gt; wy5 (a) 0.116 0.008 14.473 0.000 #&gt; wy7 ~ #&gt; wx6 (b) 0.042 0.008 4.918 0.000 #&gt; wy6 (a) 0.116 0.008 14.473 0.000 #&gt; wy8 ~ #&gt; wx7 (b) 0.042 0.008 4.918 0.000 #&gt; wy7 (a) 0.116 0.008 14.473 0.000 #&gt; wy9 ~ #&gt; wx8 (b) 0.042 0.008 4.918 0.000 #&gt; wy8 (a) 0.116 0.008 14.473 0.000 #&gt; wy10 ~ #&gt; wx9 (b) 0.042 0.008 4.918 0.000 #&gt; wy9 (a) 0.116 0.008 14.473 0.000 #&gt; wy11 ~ #&gt; wx10 (b) 0.042 0.008 4.918 0.000 #&gt; wy10 (a) 0.116 0.008 14.473 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.067 0.011 6.180 0.000 #&gt; .wx2 ~~ #&gt; .wy2 0.036 0.009 4.073 0.000 #&gt; .wx3 ~~ #&gt; .wy3 0.068 0.009 7.443 0.000 #&gt; .wx4 ~~ #&gt; .wy4 0.029 0.008 3.608 0.000 #&gt; .wx5 ~~ #&gt; .wy5 0.040 0.008 5.196 0.000 #&gt; .wx6 ~~ #&gt; .wy6 0.036 0.007 5.156 0.000 #&gt; .wx7 ~~ #&gt; .wy7 0.034 0.007 4.786 0.000 #&gt; .wx8 ~~ #&gt; .wy8 0.027 0.008 3.461 0.001 #&gt; .wx9 ~~ #&gt; .wy9 0.034 0.008 4.181 0.000 #&gt; .wx10 ~~ #&gt; .wy10 0.063 0.010 6.040 0.000 #&gt; .wx11 ~~ #&gt; .wy11 0.068 0.009 7.155 0.000 #&gt; .RIx ~~ #&gt; .RIy 1.081 0.039 27.517 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; .wx2 0.000 #&gt; .wx3 0.000 #&gt; .wx4 0.000 #&gt; .wx5 0.000 #&gt; .wx6 0.000 #&gt; .wx7 0.000 #&gt; .wx8 0.000 #&gt; .wx9 0.000 #&gt; .wx10 0.000 #&gt; .wx11 0.000 #&gt; wy1 0.000 #&gt; .wy2 0.000 #&gt; .wy3 0.000 #&gt; .wy4 0.000 #&gt; .wy5 0.000 #&gt; .wy6 0.000 #&gt; .wy7 0.000 #&gt; .wy8 0.000 #&gt; .wy9 0.000 #&gt; .wy10 0.000 #&gt; .wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 1.608 0.043 37.197 0.000 #&gt; .RIy 1.738 0.048 36.494 0.000 #&gt; wx1 0.318 0.015 21.885 0.000 #&gt; wy1 0.333 0.015 22.559 0.000 #&gt; .wx2 0.295 0.013 22.916 0.000 #&gt; .wy2 0.245 0.011 22.343 0.000 #&gt; .wx3 0.264 0.011 23.389 0.000 #&gt; .wy3 0.306 0.013 23.763 0.000 #&gt; .wx4 0.219 0.010 22.181 0.000 #&gt; .wy4 0.246 0.011 22.427 0.000 #&gt; .wx5 0.249 0.011 22.940 0.000 #&gt; .wy5 0.206 0.009 22.655 0.000 #&gt; .wx6 0.219 0.010 22.441 0.000 #&gt; .wy6 0.182 0.008 21.994 0.000 #&gt; .wx7 0.210 0.009 22.603 0.000 #&gt; .wy7 0.204 0.009 22.442 0.000 #&gt; .wx8 0.218 0.010 22.064 0.000 #&gt; .wy8 0.229 0.010 22.744 0.000 #&gt; .wx9 0.248 0.011 22.330 0.000 #&gt; .wy9 0.210 0.010 22.063 0.000 #&gt; .wx10 0.280 0.013 21.647 0.000 #&gt; .wy10 0.266 0.012 21.498 0.000 #&gt; .wx11 0.257 0.012 20.889 0.000 #&gt; .wy11 0.243 0.012 20.488 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 51 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 78 #&gt; Number of equality constraints 39 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1309 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 6298.286 #&gt; Degrees of freedom 280 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (e) 0.286 0.002 160.481 0.000 #&gt; RIy ~ #&gt; oply (e) 0.286 0.002 160.481 0.000 #&gt; wx2 ~ #&gt; wx1 (a) 0.121 0.008 14.959 0.000 #&gt; wy1 (b) 0.007 0.009 0.845 0.398 #&gt; wx3 ~ #&gt; wx2 (a) 0.121 0.008 14.959 0.000 #&gt; wy2 (b) 0.007 0.009 0.845 0.398 #&gt; wx4 ~ #&gt; wx3 (a) 0.121 0.008 14.959 0.000 #&gt; wy3 (b) 0.007 0.009 0.845 0.398 #&gt; wx5 ~ #&gt; wx4 (a) 0.121 0.008 14.959 0.000 #&gt; wy4 (b) 0.007 0.009 0.845 0.398 #&gt; wx6 ~ #&gt; wx5 (a) 0.121 0.008 14.959 0.000 #&gt; wy5 (b) 0.007 0.009 0.845 0.398 #&gt; wx7 ~ #&gt; wx6 (a) 0.121 0.008 14.959 0.000 #&gt; wy6 (b) 0.007 0.009 0.845 0.398 #&gt; wx8 ~ #&gt; wx7 (a) 0.121 0.008 14.959 0.000 #&gt; wy7 (b) 0.007 0.009 0.845 0.398 #&gt; wx9 ~ #&gt; wx8 (a) 0.121 0.008 14.959 0.000 #&gt; wy8 (b) 0.007 0.009 0.845 0.398 #&gt; wx10 ~ #&gt; wx9 (a) 0.121 0.008 14.959 0.000 #&gt; wy9 (b) 0.007 0.009 0.845 0.398 #&gt; wx11 ~ #&gt; wx10 (a) 0.121 0.008 14.959 0.000 #&gt; wy10 (b) 0.007 0.009 0.845 0.398 #&gt; wy2 ~ #&gt; wx1 (b) 0.007 0.009 0.845 0.398 #&gt; wy1 (a) 0.121 0.008 14.959 0.000 #&gt; wy3 ~ #&gt; wx2 (b) 0.007 0.009 0.845 0.398 #&gt; wy2 (a) 0.121 0.008 14.959 0.000 #&gt; wy4 ~ #&gt; wx3 (b) 0.007 0.009 0.845 0.398 #&gt; wy3 (a) 0.121 0.008 14.959 0.000 #&gt; wy5 ~ #&gt; wx4 (b) 0.007 0.009 0.845 0.398 #&gt; wy4 (a) 0.121 0.008 14.959 0.000 #&gt; wy6 ~ #&gt; wx5 (b) 0.007 0.009 0.845 0.398 #&gt; wy5 (a) 0.121 0.008 14.959 0.000 #&gt; wy7 ~ #&gt; wx6 (b) 0.007 0.009 0.845 0.398 #&gt; wy6 (a) 0.121 0.008 14.959 0.000 #&gt; wy8 ~ #&gt; wx7 (b) 0.007 0.009 0.845 0.398 #&gt; wy7 (a) 0.121 0.008 14.959 0.000 #&gt; wy9 ~ #&gt; wx8 (b) 0.007 0.009 0.845 0.398 #&gt; wy8 (a) 0.121 0.008 14.959 0.000 #&gt; wy10 ~ #&gt; wx9 (b) 0.007 0.009 0.845 0.398 #&gt; wy9 (a) 0.121 0.008 14.959 0.000 #&gt; wy11 ~ #&gt; wx10 (b) 0.007 0.009 0.845 0.398 #&gt; wy10 (a) 0.121 0.008 14.959 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.111 0.017 6.646 0.000 #&gt; .wx2 ~~ #&gt; .wy2 0.045 0.013 3.333 0.001 #&gt; .wx3 ~~ #&gt; .wy3 0.062 0.013 4.765 0.000 #&gt; .wx4 ~~ #&gt; .wy4 0.034 0.013 2.656 0.008 #&gt; .wx5 ~~ #&gt; .wy5 0.019 0.012 1.558 0.119 #&gt; .wx6 ~~ #&gt; .wy6 0.056 0.013 4.396 0.000 #&gt; .wx7 ~~ #&gt; .wy7 0.052 0.012 4.483 0.000 #&gt; .wx8 ~~ #&gt; .wy8 0.042 0.013 3.257 0.001 #&gt; .wx9 ~~ #&gt; .wy9 0.037 0.014 2.730 0.006 #&gt; .wx10 ~~ #&gt; .wy10 0.049 0.014 3.365 0.001 #&gt; .wx11 ~~ #&gt; .wy11 0.051 0.016 3.186 0.001 #&gt; .RIx ~~ #&gt; .RIy 0.999 0.036 27.380 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; .wx2 0.000 #&gt; .wx3 0.000 #&gt; .wx4 0.000 #&gt; .wx5 0.000 #&gt; .wx6 0.000 #&gt; .wx7 0.000 #&gt; .wx8 0.000 #&gt; .wx9 0.000 #&gt; .wx10 0.000 #&gt; .wx11 0.000 #&gt; wy1 0.000 #&gt; .wy2 0.000 #&gt; .wy3 0.000 #&gt; .wy4 0.000 #&gt; .wy5 0.000 #&gt; .wy6 0.000 #&gt; .wy7 0.000 #&gt; .wy8 0.000 #&gt; .wy9 0.000 #&gt; .wy10 0.000 #&gt; .wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 1.570 0.043 36.219 0.000 #&gt; .RIy 1.488 0.043 34.746 0.000 #&gt; wx1 0.513 0.023 22.698 0.000 #&gt; wy1 0.493 0.022 22.602 0.000 #&gt; .wx2 0.409 0.018 22.994 0.000 #&gt; .wy2 0.406 0.018 22.862 0.000 #&gt; .wx3 0.398 0.017 23.436 0.000 #&gt; .wy3 0.416 0.018 23.277 0.000 #&gt; .wx4 0.379 0.017 22.322 0.000 #&gt; .wy4 0.337 0.015 21.761 0.000 #&gt; .wx5 0.348 0.015 22.775 0.000 #&gt; .wy5 0.359 0.016 22.819 0.000 #&gt; .wx6 0.383 0.017 23.183 0.000 #&gt; .wy6 0.369 0.016 22.899 0.000 #&gt; .wx7 0.365 0.016 22.978 0.000 #&gt; .wy7 0.312 0.014 22.275 0.000 #&gt; .wx8 0.362 0.016 22.679 0.000 #&gt; .wy8 0.395 0.017 22.813 0.000 #&gt; .wx9 0.390 0.017 22.479 0.000 #&gt; .wy9 0.390 0.017 22.308 0.000 #&gt; .wx10 0.377 0.018 21.329 0.000 #&gt; .wy10 0.400 0.019 21.140 0.000 #&gt; .wx11 0.454 0.021 21.540 0.000 #&gt; .wy11 0.385 0.019 20.250 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 3.6.4.3 SC-RI-CLPM SCCLPM &lt;- &#39; # Create between components RIx =~ 1*x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 RIy =~ 1*y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 RIx ~ e*oplx RIy ~ e*oply # Create within-person centered variables wx1 =~ 1*x1 wx2 =~ 1*x2 wx3 =~ 1*x3 wx4 =~ 1*x4 wx5 =~ 1*x5 wx6 =~ 1*x6 wx7 =~ 1*x7 wx8 =~ 1*x8 wx9 =~ 1*x9 wx10 =~ 1*x10 wx11 =~ 1*x11 wy1 =~ 1*y1 wy2 =~ 1*y2 wy3 =~ 1*y3 wy4 =~ 1*y4 wy5 =~ 1*y5 wy6 =~ 1*y6 wy7 =~ 1*y7 wy8 =~ 1*y8 wy9 =~ 1*y9 wy10 =~ 1*y10 wy11 =~ 1*y11 # Estimate the lagged effects between the within-person centered variables. wx2 ~ a*wx1 + b*wy1 wx3 ~ a*wx2 + b*wy2 wx4 ~ a*wx3 + b*wy3 wx5 ~ a*wx4 + b*wy4 wx6 ~ a*wx5 + b*wy5 wx7 ~ a*wx6 + b*wy6 wx8 ~ a*wx7 + b*wy7 wx9 ~ a*wx8 + b*wy8 wx10 ~ a*wx9 + b*wy9 wx11 ~ a*wx10 + b*wy10 wy2 ~ b*wx1 + a*wy1 wy3 ~ b*wx2 + a*wy2 wy4 ~ b*wx3 + a*wy3 wy5 ~ b*wx4 + a*wy4 wy6 ~ b*wx5 + a*wy5 wy7 ~ b*wx6 + a*wy6 wy8 ~ b*wx7 + a*wy7 wy9 ~ b*wx8 + a*wy8 wy10 ~ b*wx9 + a*wy9 wy11 ~ b*wx10 + a*wy10 # Estimate the (residual) covariance between the within-person centered variables wx1 ~~ wy1 # Covariance wx2 ~~ wy2 wx3 ~~ wy3 wx4 ~~ wy4 wx5 ~~ wy5 wx6 ~~ wy6 wx7 ~~ wy7 wx8 ~~ wy8 wx9 ~~ wy9 wx10 ~~ wy10 wx11 ~~ wy11 # Estimate the variance and covariance of the random intercepts. RIx ~~ RIx RIy ~~ RIy RIx ~~ RIy # Estimate the (residual) variance of the within-person centered variables. wx1 ~~ wx1 # Variances wy1 ~~ wy1 wx2 ~~ wx2 # Residual variances wy2 ~~ wy2 wx3 ~~ wx3 wy3 ~~ wy3 wx4 ~~ wx4 wy4 ~~ wy4 wx5 ~~ wx5 wy5 ~~ wy5 wx6 ~~ wx6 wy6 ~~ wy6 wx7 ~~ wx7 wy7 ~~ wy7 wx8 ~~ wx8 wy8 ~~ wy8 wx9 ~~ wx9 wy9 ~~ wy9 wx10 ~~ wx10 wy10 ~~ wy10 wx11 ~~ wx11 wy11 ~~ wy11 &#39; #Estimate models a bit faster: estimate &lt;- function(x) lavaan(SCCLPM, data=x, missing = &quot;fiml.x&quot;, meanstructure = T ) library(future.apply) plan(multisession) results_temp &lt;- future_lapply(datalist_ori, estimate) results[[9]] &lt;- results_temp[[1]] results[[10]] &lt;- results_temp[[2]] results[[11]] &lt;- results_temp[[3]] results[[12]] &lt;- results_temp[[4]] names(results)[9:12] &lt;- c(&quot;fitm3h1y1&quot;, &quot;fitm3h1y2&quot;,&quot;fitm3h1y3&quot;,&quot;fitm3h1y4&quot;) load(&quot;addfiles/results.Rdata&quot;) summary(results[[9]]) summary(results[[10]]) summary(results[[11]]) summary(results[[12]]) #&gt; lavaan 0.6-9 ended normally after 51 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 98 #&gt; Number of equality constraints 39 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1415 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 1184.580 #&gt; Degrees of freedom 260 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.032 0.015 70.760 0.000 #&gt; x3 1.046 0.015 68.405 0.000 #&gt; x4 0.917 0.015 59.630 0.000 #&gt; x5 1.028 0.017 60.851 0.000 #&gt; x6 0.936 0.015 61.616 0.000 #&gt; x7 0.876 0.014 60.474 0.000 #&gt; x8 0.919 0.015 60.197 0.000 #&gt; x9 0.871 0.016 56.165 0.000 #&gt; x10 0.944 0.016 58.963 0.000 #&gt; x11 0.996 0.017 58.689 0.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.045 0.015 69.564 0.000 #&gt; y3 1.059 0.016 66.061 0.000 #&gt; y4 0.908 0.016 57.240 0.000 #&gt; y5 0.919 0.016 56.258 0.000 #&gt; y6 0.908 0.015 58.915 0.000 #&gt; y7 0.863 0.015 57.729 0.000 #&gt; y8 0.895 0.016 56.765 0.000 #&gt; y9 0.899 0.016 55.775 0.000 #&gt; y10 0.975 0.017 58.061 0.000 #&gt; y11 1.022 0.018 57.752 0.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (e) 0.127 0.002 79.687 0.000 #&gt; RIy ~ #&gt; oply (e) 0.127 0.002 79.687 0.000 #&gt; wx2 ~ #&gt; wx1 (a) 0.203 0.008 24.247 0.000 #&gt; wy1 (b) 0.071 0.009 8.002 0.000 #&gt; wx3 ~ #&gt; wx2 (a) 0.203 0.008 24.247 0.000 #&gt; wy2 (b) 0.071 0.009 8.002 0.000 #&gt; wx4 ~ #&gt; wx3 (a) 0.203 0.008 24.247 0.000 #&gt; wy3 (b) 0.071 0.009 8.002 0.000 #&gt; wx5 ~ #&gt; wx4 (a) 0.203 0.008 24.247 0.000 #&gt; wy4 (b) 0.071 0.009 8.002 0.000 #&gt; wx6 ~ #&gt; wx5 (a) 0.203 0.008 24.247 0.000 #&gt; wy5 (b) 0.071 0.009 8.002 0.000 #&gt; wx7 ~ #&gt; wx6 (a) 0.203 0.008 24.247 0.000 #&gt; wy6 (b) 0.071 0.009 8.002 0.000 #&gt; wx8 ~ #&gt; wx7 (a) 0.203 0.008 24.247 0.000 #&gt; wy7 (b) 0.071 0.009 8.002 0.000 #&gt; wx9 ~ #&gt; wx8 (a) 0.203 0.008 24.247 0.000 #&gt; wy8 (b) 0.071 0.009 8.002 0.000 #&gt; wx10 ~ #&gt; wx9 (a) 0.203 0.008 24.247 0.000 #&gt; wy9 (b) 0.071 0.009 8.002 0.000 #&gt; wx11 ~ #&gt; wx10 (a) 0.203 0.008 24.247 0.000 #&gt; wy10 (b) 0.071 0.009 8.002 0.000 #&gt; wy2 ~ #&gt; wx1 (b) 0.071 0.009 8.002 0.000 #&gt; wy1 (a) 0.203 0.008 24.247 0.000 #&gt; wy3 ~ #&gt; wx2 (b) 0.071 0.009 8.002 0.000 #&gt; wy2 (a) 0.203 0.008 24.247 0.000 #&gt; wy4 ~ #&gt; wx3 (b) 0.071 0.009 8.002 0.000 #&gt; wy3 (a) 0.203 0.008 24.247 0.000 #&gt; wy5 ~ #&gt; wx4 (b) 0.071 0.009 8.002 0.000 #&gt; wy4 (a) 0.203 0.008 24.247 0.000 #&gt; wy6 ~ #&gt; wx5 (b) 0.071 0.009 8.002 0.000 #&gt; wy5 (a) 0.203 0.008 24.247 0.000 #&gt; wy7 ~ #&gt; wx6 (b) 0.071 0.009 8.002 0.000 #&gt; wy6 (a) 0.203 0.008 24.247 0.000 #&gt; wy8 ~ #&gt; wx7 (b) 0.071 0.009 8.002 0.000 #&gt; wy7 (a) 0.203 0.008 24.247 0.000 #&gt; wy9 ~ #&gt; wx8 (b) 0.071 0.009 8.002 0.000 #&gt; wy8 (a) 0.203 0.008 24.247 0.000 #&gt; wy10 ~ #&gt; wx9 (b) 0.071 0.009 8.002 0.000 #&gt; wy9 (a) 0.203 0.008 24.247 0.000 #&gt; wy11 ~ #&gt; wx10 (b) 0.071 0.009 8.002 0.000 #&gt; wy10 (a) 0.203 0.008 24.247 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.105 0.022 4.761 0.000 #&gt; .wx2 ~~ #&gt; .wy2 0.090 0.020 4.528 0.000 #&gt; .wx3 ~~ #&gt; .wy3 0.088 0.018 4.821 0.000 #&gt; .wx4 ~~ #&gt; .wy4 0.103 0.018 5.617 0.000 #&gt; .wx5 ~~ #&gt; .wy5 0.103 0.021 4.820 0.000 #&gt; .wx6 ~~ #&gt; .wy6 0.072 0.017 4.327 0.000 #&gt; .wx7 ~~ #&gt; .wy7 0.087 0.015 5.705 0.000 #&gt; .wx8 ~~ #&gt; .wy8 0.082 0.018 4.524 0.000 #&gt; .wx9 ~~ #&gt; .wy9 0.098 0.019 5.111 0.000 #&gt; .wx10 ~~ #&gt; .wy10 0.038 0.020 1.893 0.058 #&gt; .wx11 ~~ #&gt; .wy11 0.088 0.020 4.313 0.000 #&gt; .RIx ~~ #&gt; .RIy 0.376 0.019 19.861 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; .wx2 0.000 #&gt; .wx3 0.000 #&gt; .wx4 0.000 #&gt; .wx5 0.000 #&gt; .wx6 0.000 #&gt; .wx7 0.000 #&gt; .wx8 0.000 #&gt; .wx9 0.000 #&gt; .wx10 0.000 #&gt; .wx11 0.000 #&gt; wy1 0.000 #&gt; .wy2 0.000 #&gt; .wy3 0.000 #&gt; .wy4 0.000 #&gt; .wy5 0.000 #&gt; .wy6 0.000 #&gt; .wy7 0.000 #&gt; .wy8 0.000 #&gt; .wy9 0.000 #&gt; .wy10 0.000 #&gt; .wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 0.806 0.031 26.229 0.000 #&gt; .RIy 0.520 0.021 24.305 0.000 #&gt; wx1 0.696 0.031 22.335 0.000 #&gt; wy1 0.601 0.027 21.922 0.000 #&gt; .wx2 0.681 0.029 23.590 0.000 #&gt; .wy2 0.555 0.024 22.682 0.000 #&gt; .wx3 0.575 0.025 22.989 0.000 #&gt; .wy3 0.491 0.022 22.059 0.000 #&gt; .wx4 0.605 0.026 23.151 0.000 #&gt; .wy4 0.488 0.022 22.431 0.000 #&gt; .wx5 0.711 0.031 23.224 0.000 #&gt; .wy5 0.573 0.024 23.432 0.000 #&gt; .wx6 0.520 0.022 23.228 0.000 #&gt; .wy6 0.438 0.019 22.472 0.000 #&gt; .wx7 0.489 0.021 23.403 0.000 #&gt; .wy7 0.441 0.019 23.043 0.000 #&gt; .wx8 0.553 0.024 23.131 0.000 #&gt; .wy8 0.508 0.022 23.022 0.000 #&gt; .wx9 0.604 0.026 23.489 0.000 #&gt; .wy9 0.516 0.023 22.406 0.000 #&gt; .wx10 0.564 0.026 21.577 0.000 #&gt; .wy10 0.502 0.024 20.981 0.000 #&gt; .wx11 0.597 0.028 21.051 0.000 #&gt; .wy11 0.495 0.025 20.035 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 44 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 98 #&gt; Number of equality constraints 39 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1230 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 745.392 #&gt; Degrees of freedom 260 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 0.014 70.278 0.000 #&gt; x3 1.005 0.015 69.049 0.000 #&gt; x4 1.008 0.015 67.174 0.000 #&gt; x5 1.018 0.015 68.283 0.000 #&gt; x6 1.003 0.015 65.205 0.000 #&gt; x7 1.018 0.015 66.017 0.000 #&gt; x8 0.977 0.015 65.373 0.000 #&gt; x9 0.952 0.016 61.324 0.000 #&gt; x10 1.015 0.016 62.772 0.000 #&gt; x11 0.991 0.016 61.377 0.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.040 0.015 69.260 0.000 #&gt; y3 1.072 0.015 71.615 0.000 #&gt; y4 1.071 0.016 67.625 0.000 #&gt; y5 1.077 0.016 68.626 0.000 #&gt; y6 1.069 0.016 68.829 0.000 #&gt; y7 1.069 0.016 67.652 0.000 #&gt; y8 1.032 0.016 65.732 0.000 #&gt; y9 1.020 0.016 61.914 0.000 #&gt; y10 1.054 0.017 63.867 0.000 #&gt; y11 1.062 0.017 63.216 0.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (e) 0.102 0.001 80.150 0.000 #&gt; RIy ~ #&gt; oply (e) 0.102 0.001 80.150 0.000 #&gt; wx2 ~ #&gt; wx1 (a) 0.101 0.008 12.700 0.000 #&gt; wy1 (b) 0.028 0.008 3.362 0.001 #&gt; wx3 ~ #&gt; wx2 (a) 0.101 0.008 12.700 0.000 #&gt; wy2 (b) 0.028 0.008 3.362 0.001 #&gt; wx4 ~ #&gt; wx3 (a) 0.101 0.008 12.700 0.000 #&gt; wy3 (b) 0.028 0.008 3.362 0.001 #&gt; wx5 ~ #&gt; wx4 (a) 0.101 0.008 12.700 0.000 #&gt; wy4 (b) 0.028 0.008 3.362 0.001 #&gt; wx6 ~ #&gt; wx5 (a) 0.101 0.008 12.700 0.000 #&gt; wy5 (b) 0.028 0.008 3.362 0.001 #&gt; wx7 ~ #&gt; wx6 (a) 0.101 0.008 12.700 0.000 #&gt; wy6 (b) 0.028 0.008 3.362 0.001 #&gt; wx8 ~ #&gt; wx7 (a) 0.101 0.008 12.700 0.000 #&gt; wy7 (b) 0.028 0.008 3.362 0.001 #&gt; wx9 ~ #&gt; wx8 (a) 0.101 0.008 12.700 0.000 #&gt; wy8 (b) 0.028 0.008 3.362 0.001 #&gt; wx10 ~ #&gt; wx9 (a) 0.101 0.008 12.700 0.000 #&gt; wy9 (b) 0.028 0.008 3.362 0.001 #&gt; wx11 ~ #&gt; wx10 (a) 0.101 0.008 12.700 0.000 #&gt; wy10 (b) 0.028 0.008 3.362 0.001 #&gt; wy2 ~ #&gt; wx1 (b) 0.028 0.008 3.362 0.001 #&gt; wy1 (a) 0.101 0.008 12.700 0.000 #&gt; wy3 ~ #&gt; wx2 (b) 0.028 0.008 3.362 0.001 #&gt; wy2 (a) 0.101 0.008 12.700 0.000 #&gt; wy4 ~ #&gt; wx3 (b) 0.028 0.008 3.362 0.001 #&gt; wy3 (a) 0.101 0.008 12.700 0.000 #&gt; wy5 ~ #&gt; wx4 (b) 0.028 0.008 3.362 0.001 #&gt; wy4 (a) 0.101 0.008 12.700 0.000 #&gt; wy6 ~ #&gt; wx5 (b) 0.028 0.008 3.362 0.001 #&gt; wy5 (a) 0.101 0.008 12.700 0.000 #&gt; wy7 ~ #&gt; wx6 (b) 0.028 0.008 3.362 0.001 #&gt; wy6 (a) 0.101 0.008 12.700 0.000 #&gt; wy8 ~ #&gt; wx7 (b) 0.028 0.008 3.362 0.001 #&gt; wy7 (a) 0.101 0.008 12.700 0.000 #&gt; wy9 ~ #&gt; wx8 (b) 0.028 0.008 3.362 0.001 #&gt; wy8 (a) 0.101 0.008 12.700 0.000 #&gt; wy10 ~ #&gt; wx9 (b) 0.028 0.008 3.362 0.001 #&gt; wy9 (a) 0.101 0.008 12.700 0.000 #&gt; wy11 ~ #&gt; wx10 (b) 0.028 0.008 3.362 0.001 #&gt; wy10 (a) 0.101 0.008 12.700 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.050 0.013 3.930 0.000 #&gt; .wx2 ~~ #&gt; .wy2 0.047 0.013 3.703 0.000 #&gt; .wx3 ~~ #&gt; .wy3 0.030 0.011 2.673 0.008 #&gt; .wx4 ~~ #&gt; .wy4 0.043 0.011 3.898 0.000 #&gt; .wx5 ~~ #&gt; .wy5 0.043 0.011 3.948 0.000 #&gt; .wx6 ~~ #&gt; .wy6 0.026 0.011 2.376 0.017 #&gt; .wx7 ~~ #&gt; .wy7 0.032 0.011 2.881 0.004 #&gt; .wx8 ~~ #&gt; .wy8 0.030 0.011 2.717 0.007 #&gt; .wx9 ~~ #&gt; .wy9 0.010 0.013 0.761 0.447 #&gt; .wx10 ~~ #&gt; .wy10 0.045 0.013 3.478 0.001 #&gt; .wx11 ~~ #&gt; .wy11 0.062 0.013 4.871 0.000 #&gt; .RIx ~~ #&gt; .RIy 0.297 0.013 22.520 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; .wx2 0.000 #&gt; .wx3 0.000 #&gt; .wx4 0.000 #&gt; .wx5 0.000 #&gt; .wx6 0.000 #&gt; .wx7 0.000 #&gt; .wx8 0.000 #&gt; .wx9 0.000 #&gt; .wx10 0.000 #&gt; .wx11 0.000 #&gt; wy1 0.000 #&gt; .wy2 0.000 #&gt; .wy3 0.000 #&gt; .wy4 0.000 #&gt; .wy5 0.000 #&gt; .wy6 0.000 #&gt; .wy7 0.000 #&gt; .wy8 0.000 #&gt; .wy9 0.000 #&gt; .wy10 0.000 #&gt; .wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 0.535 0.019 27.861 0.000 #&gt; .RIy 0.451 0.017 27.154 0.000 #&gt; wx1 0.448 0.019 23.890 0.000 #&gt; wy1 0.387 0.017 23.322 0.000 #&gt; .wx2 0.407 0.017 23.992 0.000 #&gt; .wy2 0.428 0.017 24.549 0.000 #&gt; .wx3 0.352 0.015 23.687 0.000 #&gt; .wy3 0.315 0.014 23.088 0.000 #&gt; .wx4 0.326 0.014 22.627 0.000 #&gt; .wy4 0.325 0.014 22.443 0.000 #&gt; .wx5 0.327 0.014 23.360 0.000 #&gt; .wy5 0.333 0.014 23.306 0.000 #&gt; .wx6 0.355 0.015 23.631 0.000 #&gt; .wy6 0.306 0.013 23.113 0.000 #&gt; .wx7 0.349 0.015 23.310 0.000 #&gt; .wy7 0.330 0.014 23.423 0.000 #&gt; .wx8 0.328 0.014 23.096 0.000 #&gt; .wy8 0.333 0.014 23.284 0.000 #&gt; .wx9 0.374 0.016 23.256 0.000 #&gt; .wy9 0.377 0.016 22.894 0.000 #&gt; .wx10 0.370 0.017 21.778 0.000 #&gt; .wy10 0.343 0.016 21.456 0.000 #&gt; .wx11 0.365 0.017 21.452 0.000 #&gt; .wy11 0.326 0.016 20.501 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 94 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 98 #&gt; Number of equality constraints 39 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1290 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 5754.776 #&gt; Degrees of freedom 260 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 0.995 0.004 241.929 0.000 #&gt; x3 0.993 0.004 232.141 0.000 #&gt; x4 1.001 0.004 231.154 0.000 #&gt; x5 1.003 0.004 227.019 0.000 #&gt; x6 1.008 0.004 231.190 0.000 #&gt; x7 1.013 0.004 233.021 0.000 #&gt; x8 1.016 0.004 228.624 0.000 #&gt; x9 1.008 0.005 218.153 0.000 #&gt; x10 1.004 0.005 208.323 0.000 #&gt; x11 1.014 0.005 208.580 0.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.015 0.004 251.843 0.000 #&gt; y3 1.011 0.005 221.550 0.000 #&gt; y4 1.023 0.005 224.154 0.000 #&gt; y5 1.028 0.004 234.838 0.000 #&gt; y6 1.030 0.004 237.318 0.000 #&gt; y7 1.033 0.004 232.516 0.000 #&gt; y8 1.035 0.005 224.870 0.000 #&gt; y9 1.028 0.005 224.226 0.000 #&gt; y10 1.024 0.005 208.550 0.000 #&gt; y11 1.034 0.005 208.052 0.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (e) 0.330 0.002 168.741 0.000 #&gt; RIy ~ #&gt; oply (e) 0.330 0.002 168.741 0.000 #&gt; wx2 ~ #&gt; wx1 (a) 0.112 0.008 14.014 0.000 #&gt; wy1 (b) 0.037 0.008 4.430 0.000 #&gt; wx3 ~ #&gt; wx2 (a) 0.112 0.008 14.014 0.000 #&gt; wy2 (b) 0.037 0.008 4.430 0.000 #&gt; wx4 ~ #&gt; wx3 (a) 0.112 0.008 14.014 0.000 #&gt; wy3 (b) 0.037 0.008 4.430 0.000 #&gt; wx5 ~ #&gt; wx4 (a) 0.112 0.008 14.014 0.000 #&gt; wy4 (b) 0.037 0.008 4.430 0.000 #&gt; wx6 ~ #&gt; wx5 (a) 0.112 0.008 14.014 0.000 #&gt; wy5 (b) 0.037 0.008 4.430 0.000 #&gt; wx7 ~ #&gt; wx6 (a) 0.112 0.008 14.014 0.000 #&gt; wy6 (b) 0.037 0.008 4.430 0.000 #&gt; wx8 ~ #&gt; wx7 (a) 0.112 0.008 14.014 0.000 #&gt; wy7 (b) 0.037 0.008 4.430 0.000 #&gt; wx9 ~ #&gt; wx8 (a) 0.112 0.008 14.014 0.000 #&gt; wy8 (b) 0.037 0.008 4.430 0.000 #&gt; wx10 ~ #&gt; wx9 (a) 0.112 0.008 14.014 0.000 #&gt; wy9 (b) 0.037 0.008 4.430 0.000 #&gt; wx11 ~ #&gt; wx10 (a) 0.112 0.008 14.014 0.000 #&gt; wy10 (b) 0.037 0.008 4.430 0.000 #&gt; wy2 ~ #&gt; wx1 (b) 0.037 0.008 4.430 0.000 #&gt; wy1 (a) 0.112 0.008 14.014 0.000 #&gt; wy3 ~ #&gt; wx2 (b) 0.037 0.008 4.430 0.000 #&gt; wy2 (a) 0.112 0.008 14.014 0.000 #&gt; wy4 ~ #&gt; wx3 (b) 0.037 0.008 4.430 0.000 #&gt; wy3 (a) 0.112 0.008 14.014 0.000 #&gt; wy5 ~ #&gt; wx4 (b) 0.037 0.008 4.430 0.000 #&gt; wy4 (a) 0.112 0.008 14.014 0.000 #&gt; wy6 ~ #&gt; wx5 (b) 0.037 0.008 4.430 0.000 #&gt; wy5 (a) 0.112 0.008 14.014 0.000 #&gt; wy7 ~ #&gt; wx6 (b) 0.037 0.008 4.430 0.000 #&gt; wy6 (a) 0.112 0.008 14.014 0.000 #&gt; wy8 ~ #&gt; wx7 (b) 0.037 0.008 4.430 0.000 #&gt; wy7 (a) 0.112 0.008 14.014 0.000 #&gt; wy9 ~ #&gt; wx8 (b) 0.037 0.008 4.430 0.000 #&gt; wy8 (a) 0.112 0.008 14.014 0.000 #&gt; wy10 ~ #&gt; wx9 (b) 0.037 0.008 4.430 0.000 #&gt; wy9 (a) 0.112 0.008 14.014 0.000 #&gt; wy11 ~ #&gt; wx10 (b) 0.037 0.008 4.430 0.000 #&gt; wy10 (a) 0.112 0.008 14.014 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.063 0.011 5.802 0.000 #&gt; .wx2 ~~ #&gt; .wy2 0.035 0.009 3.985 0.000 #&gt; .wx3 ~~ #&gt; .wy3 0.066 0.009 7.321 0.000 #&gt; .wx4 ~~ #&gt; .wy4 0.029 0.008 3.610 0.000 #&gt; .wx5 ~~ #&gt; .wy5 0.040 0.008 5.149 0.000 #&gt; .wx6 ~~ #&gt; .wy6 0.035 0.007 5.062 0.000 #&gt; .wx7 ~~ #&gt; .wy7 0.032 0.007 4.544 0.000 #&gt; .wx8 ~~ #&gt; .wy8 0.024 0.008 3.085 0.002 #&gt; .wx9 ~~ #&gt; .wy9 0.034 0.008 4.193 0.000 #&gt; .wx10 ~~ #&gt; .wy10 0.062 0.010 6.033 0.000 #&gt; .wx11 ~~ #&gt; .wy11 0.066 0.009 6.958 0.000 #&gt; .RIx ~~ #&gt; .RIy 1.057 0.038 27.498 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; .wx2 0.000 #&gt; .wx3 0.000 #&gt; .wx4 0.000 #&gt; .wx5 0.000 #&gt; .wx6 0.000 #&gt; .wx7 0.000 #&gt; .wx8 0.000 #&gt; .wx9 0.000 #&gt; .wx10 0.000 #&gt; .wx11 0.000 #&gt; wy1 0.000 #&gt; .wy2 0.000 #&gt; .wy3 0.000 #&gt; .wy4 0.000 #&gt; .wy5 0.000 #&gt; .wy6 0.000 #&gt; .wy7 0.000 #&gt; .wy8 0.000 #&gt; .wy9 0.000 #&gt; .wy10 0.000 #&gt; .wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 1.603 0.044 36.255 0.000 #&gt; .RIy 1.651 0.046 35.668 0.000 #&gt; wx1 0.316 0.014 21.886 0.000 #&gt; wy1 0.332 0.015 22.517 0.000 #&gt; .wx2 0.294 0.013 22.946 0.000 #&gt; .wy2 0.244 0.011 22.349 0.000 #&gt; .wx3 0.262 0.011 23.429 0.000 #&gt; .wy3 0.304 0.013 23.807 0.000 #&gt; .wx4 0.219 0.010 22.197 0.000 #&gt; .wy4 0.245 0.011 22.431 0.000 #&gt; .wx5 0.249 0.011 22.961 0.000 #&gt; .wy5 0.204 0.009 22.632 0.000 #&gt; .wx6 0.219 0.010 22.422 0.000 #&gt; .wy6 0.180 0.008 21.944 0.000 #&gt; .wx7 0.209 0.009 22.518 0.000 #&gt; .wy7 0.202 0.009 22.378 0.000 #&gt; .wx8 0.215 0.010 21.933 0.000 #&gt; .wy8 0.226 0.010 22.647 0.000 #&gt; .wx9 0.248 0.011 22.337 0.000 #&gt; .wy9 0.210 0.010 22.065 0.000 #&gt; .wx10 0.279 0.013 21.674 0.000 #&gt; .wy10 0.266 0.012 21.520 0.000 #&gt; .wx11 0.255 0.012 20.788 0.000 #&gt; .wy11 0.240 0.012 20.376 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 67 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 98 #&gt; Number of equality constraints 39 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1309 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 6116.744 #&gt; Degrees of freedom 260 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 0.991 0.006 172.616 0.000 #&gt; x3 0.989 0.006 162.821 0.000 #&gt; x4 0.989 0.006 157.453 0.000 #&gt; x5 1.016 0.006 164.159 0.000 #&gt; x6 0.988 0.006 157.004 0.000 #&gt; x7 0.992 0.006 158.043 0.000 #&gt; x8 1.003 0.006 157.073 0.000 #&gt; x9 1.012 0.007 152.846 0.000 #&gt; x10 1.013 0.007 151.056 0.000 #&gt; x11 1.024 0.007 142.513 0.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.041 0.006 175.131 0.000 #&gt; y3 1.031 0.006 160.421 0.000 #&gt; y4 1.041 0.006 162.313 0.000 #&gt; y5 1.056 0.006 163.112 0.000 #&gt; y6 1.035 0.007 158.293 0.000 #&gt; y7 1.030 0.006 162.578 0.000 #&gt; y8 1.040 0.007 154.358 0.000 #&gt; y9 1.039 0.007 151.676 0.000 #&gt; y10 1.033 0.007 147.337 0.000 #&gt; y11 1.036 0.007 144.519 0.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (e) 0.281 0.002 142.213 0.000 #&gt; RIy ~ #&gt; oply (e) 0.281 0.002 142.213 0.000 #&gt; wx2 ~ #&gt; wx1 (a) 0.119 0.008 14.812 0.000 #&gt; wy1 (b) 0.009 0.009 1.067 0.286 #&gt; wx3 ~ #&gt; wx2 (a) 0.119 0.008 14.812 0.000 #&gt; wy2 (b) 0.009 0.009 1.067 0.286 #&gt; wx4 ~ #&gt; wx3 (a) 0.119 0.008 14.812 0.000 #&gt; wy3 (b) 0.009 0.009 1.067 0.286 #&gt; wx5 ~ #&gt; wx4 (a) 0.119 0.008 14.812 0.000 #&gt; wy4 (b) 0.009 0.009 1.067 0.286 #&gt; wx6 ~ #&gt; wx5 (a) 0.119 0.008 14.812 0.000 #&gt; wy5 (b) 0.009 0.009 1.067 0.286 #&gt; wx7 ~ #&gt; wx6 (a) 0.119 0.008 14.812 0.000 #&gt; wy6 (b) 0.009 0.009 1.067 0.286 #&gt; wx8 ~ #&gt; wx7 (a) 0.119 0.008 14.812 0.000 #&gt; wy7 (b) 0.009 0.009 1.067 0.286 #&gt; wx9 ~ #&gt; wx8 (a) 0.119 0.008 14.812 0.000 #&gt; wy8 (b) 0.009 0.009 1.067 0.286 #&gt; wx10 ~ #&gt; wx9 (a) 0.119 0.008 14.812 0.000 #&gt; wy9 (b) 0.009 0.009 1.067 0.286 #&gt; wx11 ~ #&gt; wx10 (a) 0.119 0.008 14.812 0.000 #&gt; wy10 (b) 0.009 0.009 1.067 0.286 #&gt; wy2 ~ #&gt; wx1 (b) 0.009 0.009 1.067 0.286 #&gt; wy1 (a) 0.119 0.008 14.812 0.000 #&gt; wy3 ~ #&gt; wx2 (b) 0.009 0.009 1.067 0.286 #&gt; wy2 (a) 0.119 0.008 14.812 0.000 #&gt; wy4 ~ #&gt; wx3 (b) 0.009 0.009 1.067 0.286 #&gt; wy3 (a) 0.119 0.008 14.812 0.000 #&gt; wy5 ~ #&gt; wx4 (b) 0.009 0.009 1.067 0.286 #&gt; wy4 (a) 0.119 0.008 14.812 0.000 #&gt; wy6 ~ #&gt; wx5 (b) 0.009 0.009 1.067 0.286 #&gt; wy5 (a) 0.119 0.008 14.812 0.000 #&gt; wy7 ~ #&gt; wx6 (b) 0.009 0.009 1.067 0.286 #&gt; wy6 (a) 0.119 0.008 14.812 0.000 #&gt; wy8 ~ #&gt; wx7 (b) 0.009 0.009 1.067 0.286 #&gt; wy7 (a) 0.119 0.008 14.812 0.000 #&gt; wy9 ~ #&gt; wx8 (b) 0.009 0.009 1.067 0.286 #&gt; wy8 (a) 0.119 0.008 14.812 0.000 #&gt; wy10 ~ #&gt; wx9 (b) 0.009 0.009 1.067 0.286 #&gt; wy9 (a) 0.119 0.008 14.812 0.000 #&gt; wy11 ~ #&gt; wx10 (b) 0.009 0.009 1.067 0.286 #&gt; wy10 (a) 0.119 0.008 14.812 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.099 0.017 5.902 0.000 #&gt; .wx2 ~~ #&gt; .wy2 0.047 0.013 3.526 0.000 #&gt; .wx3 ~~ #&gt; .wy3 0.063 0.013 4.792 0.000 #&gt; .wx4 ~~ #&gt; .wy4 0.037 0.013 2.841 0.005 #&gt; .wx5 ~~ #&gt; .wy5 0.009 0.012 0.756 0.450 #&gt; .wx6 ~~ #&gt; .wy6 0.057 0.013 4.538 0.000 #&gt; .wx7 ~~ #&gt; .wy7 0.053 0.012 4.517 0.000 #&gt; .wx8 ~~ #&gt; .wy8 0.042 0.013 3.259 0.001 #&gt; .wx9 ~~ #&gt; .wy9 0.037 0.014 2.714 0.007 #&gt; .wx10 ~~ #&gt; .wy10 0.050 0.014 3.474 0.001 #&gt; .wx11 ~~ #&gt; .wy11 0.048 0.016 3.034 0.002 #&gt; .RIx ~~ #&gt; .RIy 0.979 0.036 27.320 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; .wx2 0.000 #&gt; .wx3 0.000 #&gt; .wx4 0.000 #&gt; .wx5 0.000 #&gt; .wx6 0.000 #&gt; .wx7 0.000 #&gt; .wx8 0.000 #&gt; .wx9 0.000 #&gt; .wx10 0.000 #&gt; .wx11 0.000 #&gt; wy1 0.000 #&gt; .wy2 0.000 #&gt; .wy3 0.000 #&gt; .wy4 0.000 #&gt; .wy5 0.000 #&gt; .wy6 0.000 #&gt; .wy7 0.000 #&gt; .wy8 0.000 #&gt; .wy9 0.000 #&gt; .wy10 0.000 #&gt; .wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 1.575 0.045 34.743 0.000 #&gt; .RIy 1.372 0.041 33.292 0.000 #&gt; wx1 0.511 0.023 22.541 0.000 #&gt; wy1 0.511 0.023 22.456 0.000 #&gt; .wx2 0.410 0.018 23.033 0.000 #&gt; .wy2 0.398 0.017 22.725 0.000 #&gt; .wx3 0.398 0.017 23.489 0.000 #&gt; .wy3 0.417 0.018 23.305 0.000 #&gt; .wx4 0.379 0.017 22.390 0.000 #&gt; .wy4 0.335 0.015 21.696 0.000 #&gt; .wx5 0.341 0.015 22.554 0.000 #&gt; .wy5 0.348 0.015 22.574 0.000 #&gt; .wx6 0.381 0.016 23.268 0.000 #&gt; .wy6 0.371 0.016 22.940 0.000 #&gt; .wx7 0.364 0.016 23.035 0.000 #&gt; .wy7 0.313 0.014 22.330 0.000 #&gt; .wx8 0.361 0.016 22.670 0.000 #&gt; .wy8 0.394 0.017 22.772 0.000 #&gt; .wx9 0.387 0.017 22.395 0.000 #&gt; .wy9 0.389 0.017 22.283 0.000 #&gt; .wx10 0.375 0.018 21.249 0.000 #&gt; .wy10 0.400 0.019 21.157 0.000 #&gt; .wx11 0.443 0.021 21.352 0.000 #&gt; .wy11 0.384 0.019 20.226 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 3.6.4.4 LT-RI-CLPM LTRICLPM &lt;- &#39; # Create between components (random intercepts) RIx =~ 1*x1 + 1*x2 + 1*x3 + 1*x4 + 1*x5 + 1*x6 + 1*x7 + 1*x8 + 1*x9 + 1*x10 + 1*x11 RIy =~ 1*y1 + 1*y2 + 1*y3 + 1*y4 + 1*y5 + 1*y6 + 1*y7 + 1*y8 + 1*y9 + 1*y10 + 1*y11 RIx ~ e*oplx RIy ~ e*oply #Random slopes RIsx =~ 1*x1 + 2*x2 + 3*x3 + 4*x4 + 5*x5 + 6*x6 + 7*x7 + 8*x8 + 9*x9 + 10*x10 + 11*x11 RIsy =~ 1*y1 + 2*y2 + 3*y3 + 4*y4 + 5*y5 + 6*y6 + 7*y7 + 8*y8 + 9*y9 + 10*y10 + 11*y11 RIsx ~ f*oplx RIsy ~ f*oply # Create within-person centered variables wx1 =~ 1*x1 wx2 =~ 1*x2 wx3 =~ 1*x3 wx4 =~ 1*x4 wx5 =~ 1*x5 wx6 =~ 1*x6 wx7 =~ 1*x7 wx8 =~ 1*x8 wx9 =~ 1*x9 wx10 =~ 1*x10 wx11 =~ 1*x11 wy1 =~ 1*y1 wy2 =~ 1*y2 wy3 =~ 1*y3 wy4 =~ 1*y4 wy5 =~ 1*y5 wy6 =~ 1*y6 wy7 =~ 1*y7 wy8 =~ 1*y8 wy9 =~ 1*y9 wy10 =~ 1*y10 wy11 =~ 1*y11 # Estimate the lagged effects between the within-person centered variables. wx2 ~ a*wx1 + b*wy1 wx3 ~ a*wx2 + b*wy2 wx4 ~ a*wx3 + b*wy3 wx5 ~ a*wx4 + b*wy4 wx6 ~ a*wx5 + b*wy5 wx7 ~ a*wx6 + b*wy6 wx8 ~ a*wx7 + b*wy7 wx9 ~ a*wx8 + b*wy8 wx10 ~ a*wx9 + b*wy9 wx11 ~ a*wx10 + b*wy10 wy2 ~ b*wx1 + a*wy1 wy3 ~ b*wx2 + a*wy2 wy4 ~ b*wx3 + a*wy3 wy5 ~ b*wx4 + a*wy4 wy6 ~ b*wx5 + a*wy5 wy7 ~ b*wx6 + a*wy6 wy8 ~ b*wx7 + a*wy7 wy9 ~ b*wx8 + a*wy8 wy10 ~ b*wx9 + a*wy9 wy11 ~ b*wx10 + a*wy10 # Estimate the (residual) covariance between the within-person centered variables wx1 ~~ wy1 # Covariance wx2 ~~ wy2 wx3 ~~ wy3 wx4 ~~ wy4 wx5 ~~ wy5 wx6 ~~ wy6 wx7 ~~ wy7 wx8 ~~ wy8 wx9 ~~ wy9 wx10 ~~ wy10 wx11 ~~ wy11 # Estimate the variance and covariance of the random intercepts and random slopes. RIx ~~ RIx RIy ~~ RIy RIx ~~ RIy #covariance intercepts: interpretation SELECTION RIsx ~~ RIsx RIsy ~~ RIsy RIsx ~~ RIsy #covariance slopes: interpretation COMMON CONTEXT RIx ~~ RIsx #covariance intercept/slope: interpretation regression to the mean RIy ~~ RIsy RIx ~~ RIsy #cross-covariance: interpretation INFLUENCE? RIy ~~ RIsx # Estimate the (residual) variance of the within-person centered variables. wx1 ~~ wx1 # Variances wy1 ~~ wy1 wx2 ~~ wx2 # Residual variances wy2 ~~ wy2 wx3 ~~ wx3 wy3 ~~ wy3 wx4 ~~ wx4 wy4 ~~ wy4 wx5 ~~ wx5 wy5 ~~ wy5 wx6 ~~ wx6 wy6 ~~ wy6 wx7 ~~ wx7 wy7 ~~ wy7 wx8 ~~ wx8 wy8 ~~ wy8 wx9 ~~ wx9 wy9 ~~ wy9 wx10 ~~ wx10 wy10 ~~ wy10 wx11 ~~ wx11 wy11 ~~ wy11 &#39; #Estimate models a bit faster: estimate &lt;- function(x) lavaan(LTRICLPM, data=x, missing = &quot;fiml.x&quot;, meanstructure = T ) library(future.apply) plan(multisession) results_temp &lt;- future_lapply(datalist_ori, estimate) results[[13]] &lt;- results_temp[[1]] results[[14]] &lt;- results_temp[[2]] results[[15]] &lt;- results_temp[[3]] results[[16]] &lt;- results_temp[[4]] names(results)[9:12] &lt;- c(&quot;fitm4h1y1&quot;, &quot;fitm4h1y2&quot;,&quot;fitm4h1y3&quot;,&quot;fitm4h1y4&quot;) save(results, file=&quot;results.RData&quot;) load(&quot;addfiles/results.Rdata&quot;) summary(results[[13]]) summary(results[[14]]) summary(results[[15]]) summary(results[[16]]) #&gt; lavaan 0.6-9 ended normally after 86 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 87 #&gt; Number of equality constraints 40 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1415 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 1387.507 #&gt; Degrees of freedom 272 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; RIsx =~ #&gt; x1 1.000 #&gt; x2 2.000 #&gt; x3 3.000 #&gt; x4 4.000 #&gt; x5 5.000 #&gt; x6 6.000 #&gt; x7 7.000 #&gt; x8 8.000 #&gt; x9 9.000 #&gt; x10 10.000 #&gt; x11 11.000 #&gt; RIsy =~ #&gt; y1 1.000 #&gt; y2 2.000 #&gt; y3 3.000 #&gt; y4 4.000 #&gt; y5 5.000 #&gt; y6 6.000 #&gt; y7 7.000 #&gt; y8 8.000 #&gt; y9 9.000 #&gt; y10 10.000 #&gt; y11 11.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (e) 0.132 0.001 91.424 0.000 #&gt; RIy ~ #&gt; oply (e) 0.132 0.001 91.424 0.000 #&gt; RIsx ~ #&gt; oplx (f) -0.002 0.000 -9.935 0.000 #&gt; RIsy ~ #&gt; oply (f) -0.002 0.000 -9.935 0.000 #&gt; wx2 ~ #&gt; wx1 (a) 0.145 0.010 14.885 0.000 #&gt; wy1 (b) 0.058 0.010 5.674 0.000 #&gt; wx3 ~ #&gt; wx2 (a) 0.145 0.010 14.885 0.000 #&gt; wy2 (b) 0.058 0.010 5.674 0.000 #&gt; wx4 ~ #&gt; wx3 (a) 0.145 0.010 14.885 0.000 #&gt; wy3 (b) 0.058 0.010 5.674 0.000 #&gt; wx5 ~ #&gt; wx4 (a) 0.145 0.010 14.885 0.000 #&gt; wy4 (b) 0.058 0.010 5.674 0.000 #&gt; wx6 ~ #&gt; wx5 (a) 0.145 0.010 14.885 0.000 #&gt; wy5 (b) 0.058 0.010 5.674 0.000 #&gt; wx7 ~ #&gt; wx6 (a) 0.145 0.010 14.885 0.000 #&gt; wy6 (b) 0.058 0.010 5.674 0.000 #&gt; wx8 ~ #&gt; wx7 (a) 0.145 0.010 14.885 0.000 #&gt; wy7 (b) 0.058 0.010 5.674 0.000 #&gt; wx9 ~ #&gt; wx8 (a) 0.145 0.010 14.885 0.000 #&gt; wy8 (b) 0.058 0.010 5.674 0.000 #&gt; wx10 ~ #&gt; wx9 (a) 0.145 0.010 14.885 0.000 #&gt; wy9 (b) 0.058 0.010 5.674 0.000 #&gt; wx11 ~ #&gt; wx10 (a) 0.145 0.010 14.885 0.000 #&gt; wy10 (b) 0.058 0.010 5.674 0.000 #&gt; wy2 ~ #&gt; wx1 (b) 0.058 0.010 5.674 0.000 #&gt; wy1 (a) 0.145 0.010 14.885 0.000 #&gt; wy3 ~ #&gt; wx2 (b) 0.058 0.010 5.674 0.000 #&gt; wy2 (a) 0.145 0.010 14.885 0.000 #&gt; wy4 ~ #&gt; wx3 (b) 0.058 0.010 5.674 0.000 #&gt; wy3 (a) 0.145 0.010 14.885 0.000 #&gt; wy5 ~ #&gt; wx4 (b) 0.058 0.010 5.674 0.000 #&gt; wy4 (a) 0.145 0.010 14.885 0.000 #&gt; wy6 ~ #&gt; wx5 (b) 0.058 0.010 5.674 0.000 #&gt; wy5 (a) 0.145 0.010 14.885 0.000 #&gt; wy7 ~ #&gt; wx6 (b) 0.058 0.010 5.674 0.000 #&gt; wy6 (a) 0.145 0.010 14.885 0.000 #&gt; wy8 ~ #&gt; wx7 (b) 0.058 0.010 5.674 0.000 #&gt; wy7 (a) 0.145 0.010 14.885 0.000 #&gt; wy9 ~ #&gt; wx8 (b) 0.058 0.010 5.674 0.000 #&gt; wy8 (a) 0.145 0.010 14.885 0.000 #&gt; wy10 ~ #&gt; wx9 (b) 0.058 0.010 5.674 0.000 #&gt; wy9 (a) 0.145 0.010 14.885 0.000 #&gt; wy11 ~ #&gt; wx10 (b) 0.058 0.010 5.674 0.000 #&gt; wy10 (a) 0.145 0.010 14.885 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.093 0.023 4.098 0.000 #&gt; .wx2 ~~ #&gt; .wy2 0.090 0.020 4.521 0.000 #&gt; .wx3 ~~ #&gt; .wy3 0.110 0.019 5.912 0.000 #&gt; .wx4 ~~ #&gt; .wy4 0.111 0.019 5.894 0.000 #&gt; .wx5 ~~ #&gt; .wy5 0.095 0.021 4.460 0.000 #&gt; .wx6 ~~ #&gt; .wy6 0.065 0.016 4.046 0.000 #&gt; .wx7 ~~ #&gt; .wy7 0.083 0.015 5.458 0.000 #&gt; .wx8 ~~ #&gt; .wy8 0.072 0.018 4.054 0.000 #&gt; .wx9 ~~ #&gt; .wy9 0.080 0.019 4.260 0.000 #&gt; .wx10 ~~ #&gt; .wy10 0.047 0.021 2.275 0.023 #&gt; .wx11 ~~ #&gt; .wy11 0.110 0.022 4.958 0.000 #&gt; .RIx ~~ #&gt; .RIy 0.294 0.027 10.860 0.000 #&gt; .RIsx ~~ #&gt; .RIsy 0.001 0.000 2.509 0.012 #&gt; .RIx ~~ #&gt; .RIsx -0.014 0.004 -3.372 0.001 #&gt; .RIy ~~ #&gt; .RIsy -0.005 0.003 -1.453 0.146 #&gt; .RIx ~~ #&gt; .RIsy 0.003 0.003 0.762 0.446 #&gt; .RIy ~~ #&gt; .RIsx 0.003 0.003 0.931 0.352 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; .RIsx 0.000 #&gt; .RIsy 0.000 #&gt; wx1 0.000 #&gt; .wx2 0.000 #&gt; .wx3 0.000 #&gt; .wx4 0.000 #&gt; .wx5 0.000 #&gt; .wx6 0.000 #&gt; .wx7 0.000 #&gt; .wx8 0.000 #&gt; .wx9 0.000 #&gt; .wx10 0.000 #&gt; .wx11 0.000 #&gt; wy1 0.000 #&gt; .wy2 0.000 #&gt; .wy3 0.000 #&gt; .wy4 0.000 #&gt; .wy5 0.000 #&gt; .wy6 0.000 #&gt; .wy7 0.000 #&gt; .wy8 0.000 #&gt; .wy9 0.000 #&gt; .wy10 0.000 #&gt; .wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 0.775 0.040 19.367 0.000 #&gt; .RIy 0.483 0.031 15.779 0.000 #&gt; .RIsx 0.005 0.001 7.510 0.000 #&gt; .RIsy 0.002 0.000 4.589 0.000 #&gt; wx1 0.613 0.031 19.513 0.000 #&gt; wy1 0.566 0.028 20.232 0.000 #&gt; .wx2 0.648 0.028 22.850 0.000 #&gt; .wy2 0.545 0.024 22.400 0.000 #&gt; .wx3 0.577 0.025 23.074 0.000 #&gt; .wy3 0.509 0.023 22.501 0.000 #&gt; .wx4 0.588 0.026 22.408 0.000 #&gt; .wy4 0.481 0.022 21.760 0.000 #&gt; .wx5 0.730 0.031 23.908 0.000 #&gt; .wy5 0.557 0.024 23.263 0.000 #&gt; .wx6 0.498 0.022 22.911 0.000 #&gt; .wy6 0.417 0.019 22.078 0.000 #&gt; .wx7 0.460 0.021 22.404 0.000 #&gt; .wy7 0.427 0.019 22.272 0.000 #&gt; .wx8 0.516 0.023 22.410 0.000 #&gt; .wy8 0.485 0.022 22.333 0.000 #&gt; .wx9 0.551 0.025 21.848 0.000 #&gt; .wy9 0.474 0.022 21.162 0.000 #&gt; .wx10 0.516 0.026 19.884 0.000 #&gt; .wy10 0.491 0.025 19.977 0.000 #&gt; .wx11 0.563 0.029 19.152 0.000 #&gt; .wy11 0.504 0.027 18.572 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 122 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 87 #&gt; Number of equality constraints 40 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1230 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 753.109 #&gt; Degrees of freedom 272 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; RIsx =~ #&gt; x1 1.000 #&gt; x2 2.000 #&gt; x3 3.000 #&gt; x4 4.000 #&gt; x5 5.000 #&gt; x6 6.000 #&gt; x7 7.000 #&gt; x8 8.000 #&gt; x9 9.000 #&gt; x10 10.000 #&gt; x11 11.000 #&gt; RIsy =~ #&gt; y1 1.000 #&gt; y2 2.000 #&gt; y3 3.000 #&gt; y4 4.000 #&gt; y5 5.000 #&gt; y6 6.000 #&gt; y7 7.000 #&gt; y8 8.000 #&gt; y9 9.000 #&gt; y10 10.000 #&gt; y11 11.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (e) 0.103 0.001 80.062 0.000 #&gt; RIy ~ #&gt; oply (e) 0.103 0.001 80.062 0.000 #&gt; RIsx ~ #&gt; oplx (f) 0.000 0.000 1.452 0.146 #&gt; RIsy ~ #&gt; oply (f) 0.000 0.000 1.452 0.146 #&gt; wx2 ~ #&gt; wx1 (a) 0.066 0.009 7.261 0.000 #&gt; wy1 (b) 0.015 0.009 1.542 0.123 #&gt; wx3 ~ #&gt; wx2 (a) 0.066 0.009 7.261 0.000 #&gt; wy2 (b) 0.015 0.009 1.542 0.123 #&gt; wx4 ~ #&gt; wx3 (a) 0.066 0.009 7.261 0.000 #&gt; wy3 (b) 0.015 0.009 1.542 0.123 #&gt; wx5 ~ #&gt; wx4 (a) 0.066 0.009 7.261 0.000 #&gt; wy4 (b) 0.015 0.009 1.542 0.123 #&gt; wx6 ~ #&gt; wx5 (a) 0.066 0.009 7.261 0.000 #&gt; wy5 (b) 0.015 0.009 1.542 0.123 #&gt; wx7 ~ #&gt; wx6 (a) 0.066 0.009 7.261 0.000 #&gt; wy6 (b) 0.015 0.009 1.542 0.123 #&gt; wx8 ~ #&gt; wx7 (a) 0.066 0.009 7.261 0.000 #&gt; wy7 (b) 0.015 0.009 1.542 0.123 #&gt; wx9 ~ #&gt; wx8 (a) 0.066 0.009 7.261 0.000 #&gt; wy8 (b) 0.015 0.009 1.542 0.123 #&gt; wx10 ~ #&gt; wx9 (a) 0.066 0.009 7.261 0.000 #&gt; wy9 (b) 0.015 0.009 1.542 0.123 #&gt; wx11 ~ #&gt; wx10 (a) 0.066 0.009 7.261 0.000 #&gt; wy10 (b) 0.015 0.009 1.542 0.123 #&gt; wy2 ~ #&gt; wx1 (b) 0.015 0.009 1.542 0.123 #&gt; wy1 (a) 0.066 0.009 7.261 0.000 #&gt; wy3 ~ #&gt; wx2 (b) 0.015 0.009 1.542 0.123 #&gt; wy2 (a) 0.066 0.009 7.261 0.000 #&gt; wy4 ~ #&gt; wx3 (b) 0.015 0.009 1.542 0.123 #&gt; wy3 (a) 0.066 0.009 7.261 0.000 #&gt; wy5 ~ #&gt; wx4 (b) 0.015 0.009 1.542 0.123 #&gt; wy4 (a) 0.066 0.009 7.261 0.000 #&gt; wy6 ~ #&gt; wx5 (b) 0.015 0.009 1.542 0.123 #&gt; wy5 (a) 0.066 0.009 7.261 0.000 #&gt; wy7 ~ #&gt; wx6 (b) 0.015 0.009 1.542 0.123 #&gt; wy6 (a) 0.066 0.009 7.261 0.000 #&gt; wy8 ~ #&gt; wx7 (b) 0.015 0.009 1.542 0.123 #&gt; wy7 (a) 0.066 0.009 7.261 0.000 #&gt; wy9 ~ #&gt; wx8 (b) 0.015 0.009 1.542 0.123 #&gt; wy8 (a) 0.066 0.009 7.261 0.000 #&gt; wy10 ~ #&gt; wx9 (b) 0.015 0.009 1.542 0.123 #&gt; wy9 (a) 0.066 0.009 7.261 0.000 #&gt; wy11 ~ #&gt; wx10 (b) 0.015 0.009 1.542 0.123 #&gt; wy10 (a) 0.066 0.009 7.261 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.030 0.013 2.313 0.021 #&gt; .wx2 ~~ #&gt; .wy2 0.041 0.013 3.195 0.001 #&gt; .wx3 ~~ #&gt; .wy3 0.020 0.011 1.832 0.067 #&gt; .wx4 ~~ #&gt; .wy4 0.039 0.011 3.589 0.000 #&gt; .wx5 ~~ #&gt; .wy5 0.041 0.011 3.808 0.000 #&gt; .wx6 ~~ #&gt; .wy6 0.024 0.011 2.233 0.026 #&gt; .wx7 ~~ #&gt; .wy7 0.032 0.011 2.814 0.005 #&gt; .wx8 ~~ #&gt; .wy8 0.027 0.011 2.402 0.016 #&gt; .wx9 ~~ #&gt; .wy9 0.009 0.013 0.744 0.457 #&gt; .wx10 ~~ #&gt; .wy10 0.038 0.013 2.986 0.003 #&gt; .wx11 ~~ #&gt; .wy11 0.052 0.013 3.997 0.000 #&gt; .RIx ~~ #&gt; .RIy 0.379 0.021 18.233 0.000 #&gt; .RIsx ~~ #&gt; .RIsy 0.001 0.000 3.498 0.000 #&gt; .RIx ~~ #&gt; .RIsx -0.011 0.002 -4.796 0.000 #&gt; .RIy ~~ #&gt; .RIsy -0.014 0.002 -6.106 0.000 #&gt; .RIx ~~ #&gt; .RIsy -0.010 0.002 -4.812 0.000 #&gt; .RIy ~~ #&gt; .RIsx -0.006 0.002 -3.087 0.002 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; .RIsx 0.000 #&gt; .RIsy 0.000 #&gt; wx1 0.000 #&gt; .wx2 0.000 #&gt; .wx3 0.000 #&gt; .wx4 0.000 #&gt; .wx5 0.000 #&gt; .wx6 0.000 #&gt; .wx7 0.000 #&gt; .wx8 0.000 #&gt; .wx9 0.000 #&gt; .wx10 0.000 #&gt; .wx11 0.000 #&gt; wy1 0.000 #&gt; .wy2 0.000 #&gt; .wy3 0.000 #&gt; .wy4 0.000 #&gt; .wy5 0.000 #&gt; .wy6 0.000 #&gt; .wy7 0.000 #&gt; .wy8 0.000 #&gt; .wy9 0.000 #&gt; .wy10 0.000 #&gt; .wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 0.619 0.027 23.131 0.000 #&gt; .RIy 0.613 0.026 23.241 0.000 #&gt; .RIsx 0.001 0.000 5.094 0.000 #&gt; .RIsy 0.002 0.000 5.690 0.000 #&gt; wx1 0.410 0.019 21.642 0.000 #&gt; wy1 0.332 0.016 20.354 0.000 #&gt; .wx2 0.389 0.017 22.962 0.000 #&gt; .wy2 0.410 0.017 23.591 0.000 #&gt; .wx3 0.338 0.015 23.049 0.000 #&gt; .wy3 0.304 0.013 22.542 0.000 #&gt; .wx4 0.317 0.014 22.420 0.000 #&gt; .wy4 0.318 0.014 22.330 0.000 #&gt; .wx5 0.325 0.014 23.514 0.000 #&gt; .wy5 0.332 0.014 23.431 0.000 #&gt; .wx6 0.350 0.015 23.649 0.000 #&gt; .wy6 0.304 0.013 23.244 0.000 #&gt; .wx7 0.348 0.015 23.436 0.000 #&gt; .wy7 0.327 0.014 23.469 0.000 #&gt; .wx8 0.318 0.014 22.681 0.000 #&gt; .wy8 0.322 0.014 22.854 0.000 #&gt; .wx9 0.365 0.016 22.530 0.000 #&gt; .wy9 0.364 0.016 22.194 0.000 #&gt; .wx10 0.352 0.017 20.833 0.000 #&gt; .wy10 0.322 0.016 20.438 0.000 #&gt; .wx11 0.342 0.017 19.837 0.000 #&gt; .wy11 0.304 0.016 18.850 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 107 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 87 #&gt; Number of equality constraints 40 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1290 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 5704.541 #&gt; Degrees of freedom 272 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; RIsx =~ #&gt; x1 1.000 #&gt; x2 2.000 #&gt; x3 3.000 #&gt; x4 4.000 #&gt; x5 5.000 #&gt; x6 6.000 #&gt; x7 7.000 #&gt; x8 8.000 #&gt; x9 9.000 #&gt; x10 10.000 #&gt; x11 11.000 #&gt; RIsy =~ #&gt; y1 1.000 #&gt; y2 2.000 #&gt; y3 3.000 #&gt; y4 4.000 #&gt; y5 5.000 #&gt; y6 6.000 #&gt; y7 7.000 #&gt; y8 8.000 #&gt; y9 9.000 #&gt; y10 10.000 #&gt; y11 11.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (e) 0.330 0.002 166.688 0.000 #&gt; RIy ~ #&gt; oply (e) 0.330 0.002 166.688 0.000 #&gt; RIsx ~ #&gt; oplx (f) 0.001 0.000 6.280 0.000 #&gt; RIsy ~ #&gt; oply (f) 0.001 0.000 6.280 0.000 #&gt; wx2 ~ #&gt; wx1 (a) 0.060 0.009 6.641 0.000 #&gt; wy1 (b) 0.033 0.009 3.485 0.000 #&gt; wx3 ~ #&gt; wx2 (a) 0.060 0.009 6.641 0.000 #&gt; wy2 (b) 0.033 0.009 3.485 0.000 #&gt; wx4 ~ #&gt; wx3 (a) 0.060 0.009 6.641 0.000 #&gt; wy3 (b) 0.033 0.009 3.485 0.000 #&gt; wx5 ~ #&gt; wx4 (a) 0.060 0.009 6.641 0.000 #&gt; wy4 (b) 0.033 0.009 3.485 0.000 #&gt; wx6 ~ #&gt; wx5 (a) 0.060 0.009 6.641 0.000 #&gt; wy5 (b) 0.033 0.009 3.485 0.000 #&gt; wx7 ~ #&gt; wx6 (a) 0.060 0.009 6.641 0.000 #&gt; wy6 (b) 0.033 0.009 3.485 0.000 #&gt; wx8 ~ #&gt; wx7 (a) 0.060 0.009 6.641 0.000 #&gt; wy7 (b) 0.033 0.009 3.485 0.000 #&gt; wx9 ~ #&gt; wx8 (a) 0.060 0.009 6.641 0.000 #&gt; wy8 (b) 0.033 0.009 3.485 0.000 #&gt; wx10 ~ #&gt; wx9 (a) 0.060 0.009 6.641 0.000 #&gt; wy9 (b) 0.033 0.009 3.485 0.000 #&gt; wx11 ~ #&gt; wx10 (a) 0.060 0.009 6.641 0.000 #&gt; wy10 (b) 0.033 0.009 3.485 0.000 #&gt; wy2 ~ #&gt; wx1 (b) 0.033 0.009 3.485 0.000 #&gt; wy1 (a) 0.060 0.009 6.641 0.000 #&gt; wy3 ~ #&gt; wx2 (b) 0.033 0.009 3.485 0.000 #&gt; wy2 (a) 0.060 0.009 6.641 0.000 #&gt; wy4 ~ #&gt; wx3 (b) 0.033 0.009 3.485 0.000 #&gt; wy3 (a) 0.060 0.009 6.641 0.000 #&gt; wy5 ~ #&gt; wx4 (b) 0.033 0.009 3.485 0.000 #&gt; wy4 (a) 0.060 0.009 6.641 0.000 #&gt; wy6 ~ #&gt; wx5 (b) 0.033 0.009 3.485 0.000 #&gt; wy5 (a) 0.060 0.009 6.641 0.000 #&gt; wy7 ~ #&gt; wx6 (b) 0.033 0.009 3.485 0.000 #&gt; wy6 (a) 0.060 0.009 6.641 0.000 #&gt; wy8 ~ #&gt; wx7 (b) 0.033 0.009 3.485 0.000 #&gt; wy7 (a) 0.060 0.009 6.641 0.000 #&gt; wy9 ~ #&gt; wx8 (b) 0.033 0.009 3.485 0.000 #&gt; wy8 (a) 0.060 0.009 6.641 0.000 #&gt; wy10 ~ #&gt; wx9 (b) 0.033 0.009 3.485 0.000 #&gt; wy9 (a) 0.060 0.009 6.641 0.000 #&gt; wy11 ~ #&gt; wx10 (b) 0.033 0.009 3.485 0.000 #&gt; wy10 (a) 0.060 0.009 6.641 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.054 0.011 5.038 0.000 #&gt; .wx2 ~~ #&gt; .wy2 0.029 0.009 3.278 0.001 #&gt; .wx3 ~~ #&gt; .wy3 0.062 0.009 7.003 0.000 #&gt; .wx4 ~~ #&gt; .wy4 0.028 0.008 3.525 0.000 #&gt; .wx5 ~~ #&gt; .wy5 0.038 0.008 5.017 0.000 #&gt; .wx6 ~~ #&gt; .wy6 0.035 0.007 5.156 0.000 #&gt; .wx7 ~~ #&gt; .wy7 0.031 0.007 4.500 0.000 #&gt; .wx8 ~~ #&gt; .wy8 0.024 0.008 3.157 0.002 #&gt; .wx9 ~~ #&gt; .wy9 0.031 0.008 3.942 0.000 #&gt; .wx10 ~~ #&gt; .wy10 0.060 0.010 5.887 0.000 #&gt; .wx11 ~~ #&gt; .wy11 0.055 0.009 5.812 0.000 #&gt; .RIx ~~ #&gt; .RIy 1.116 0.045 24.683 0.000 #&gt; .RIsx ~~ #&gt; .RIsy 0.000 0.000 2.880 0.004 #&gt; .RIx ~~ #&gt; .RIsx -0.007 0.003 -2.606 0.009 #&gt; .RIy ~~ #&gt; .RIsy -0.011 0.003 -3.935 0.000 #&gt; .RIx ~~ #&gt; .RIsy -0.006 0.003 -2.150 0.032 #&gt; .RIy ~~ #&gt; .RIsx -0.003 0.003 -0.893 0.372 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; .RIsx 0.000 #&gt; .RIsy 0.000 #&gt; wx1 0.000 #&gt; .wx2 0.000 #&gt; .wx3 0.000 #&gt; .wx4 0.000 #&gt; .wx5 0.000 #&gt; .wx6 0.000 #&gt; .wx7 0.000 #&gt; .wx8 0.000 #&gt; .wx9 0.000 #&gt; .wx10 0.000 #&gt; .wx11 0.000 #&gt; wy1 0.000 #&gt; .wy2 0.000 #&gt; .wy3 0.000 #&gt; .wy4 0.000 #&gt; .wy5 0.000 #&gt; .wy6 0.000 #&gt; .wy7 0.000 #&gt; .wy8 0.000 #&gt; .wy9 0.000 #&gt; .wy10 0.000 #&gt; .wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 1.647 0.051 32.469 0.000 #&gt; .RIy 1.823 0.056 32.345 0.000 #&gt; .RIsx 0.002 0.000 6.807 0.000 #&gt; .RIsy 0.002 0.000 7.292 0.000 #&gt; wx1 0.285 0.014 19.984 0.000 #&gt; wy1 0.294 0.014 20.516 0.000 #&gt; .wx2 0.272 0.013 21.496 0.000 #&gt; .wy2 0.224 0.011 20.538 0.000 #&gt; .wx3 0.247 0.011 22.437 0.000 #&gt; .wy3 0.290 0.013 23.006 0.000 #&gt; .wx4 0.212 0.010 21.773 0.000 #&gt; .wy4 0.237 0.011 22.066 0.000 #&gt; .wx5 0.242 0.011 22.814 0.000 #&gt; .wy5 0.196 0.009 22.359 0.000 #&gt; .wx6 0.214 0.010 22.380 0.000 #&gt; .wy6 0.179 0.008 21.978 0.000 #&gt; .wx7 0.205 0.009 22.465 0.000 #&gt; .wy7 0.198 0.009 22.261 0.000 #&gt; .wx8 0.211 0.010 21.711 0.000 #&gt; .wy8 0.217 0.010 22.304 0.000 #&gt; .wx9 0.233 0.011 21.420 0.000 #&gt; .wy9 0.197 0.009 21.021 0.000 #&gt; .wx10 0.263 0.013 20.555 0.000 #&gt; .wy10 0.250 0.012 20.312 0.000 #&gt; .wx11 0.225 0.012 18.348 0.000 #&gt; .wy11 0.209 0.012 17.768 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 83 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 87 #&gt; Number of equality constraints 40 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1309 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 6137.986 #&gt; Degrees of freedom 272 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; RIsx =~ #&gt; x1 1.000 #&gt; x2 2.000 #&gt; x3 3.000 #&gt; x4 4.000 #&gt; x5 5.000 #&gt; x6 6.000 #&gt; x7 7.000 #&gt; x8 8.000 #&gt; x9 9.000 #&gt; x10 10.000 #&gt; x11 11.000 #&gt; RIsy =~ #&gt; y1 1.000 #&gt; y2 2.000 #&gt; y3 3.000 #&gt; y4 4.000 #&gt; y5 5.000 #&gt; y6 6.000 #&gt; y7 7.000 #&gt; y8 8.000 #&gt; y9 9.000 #&gt; y10 10.000 #&gt; y11 11.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (e) 0.282 0.002 139.133 0.000 #&gt; RIy ~ #&gt; oply (e) 0.282 0.002 139.133 0.000 #&gt; RIsx ~ #&gt; oplx (f) 0.001 0.000 4.139 0.000 #&gt; RIsy ~ #&gt; oply (f) 0.001 0.000 4.139 0.000 #&gt; wx2 ~ #&gt; wx1 (a) 0.066 0.009 7.091 0.000 #&gt; wy1 (b) 0.009 0.010 0.933 0.351 #&gt; wx3 ~ #&gt; wx2 (a) 0.066 0.009 7.091 0.000 #&gt; wy2 (b) 0.009 0.010 0.933 0.351 #&gt; wx4 ~ #&gt; wx3 (a) 0.066 0.009 7.091 0.000 #&gt; wy3 (b) 0.009 0.010 0.933 0.351 #&gt; wx5 ~ #&gt; wx4 (a) 0.066 0.009 7.091 0.000 #&gt; wy4 (b) 0.009 0.010 0.933 0.351 #&gt; wx6 ~ #&gt; wx5 (a) 0.066 0.009 7.091 0.000 #&gt; wy5 (b) 0.009 0.010 0.933 0.351 #&gt; wx7 ~ #&gt; wx6 (a) 0.066 0.009 7.091 0.000 #&gt; wy6 (b) 0.009 0.010 0.933 0.351 #&gt; wx8 ~ #&gt; wx7 (a) 0.066 0.009 7.091 0.000 #&gt; wy7 (b) 0.009 0.010 0.933 0.351 #&gt; wx9 ~ #&gt; wx8 (a) 0.066 0.009 7.091 0.000 #&gt; wy8 (b) 0.009 0.010 0.933 0.351 #&gt; wx10 ~ #&gt; wx9 (a) 0.066 0.009 7.091 0.000 #&gt; wy9 (b) 0.009 0.010 0.933 0.351 #&gt; wx11 ~ #&gt; wx10 (a) 0.066 0.009 7.091 0.000 #&gt; wy10 (b) 0.009 0.010 0.933 0.351 #&gt; wy2 ~ #&gt; wx1 (b) 0.009 0.010 0.933 0.351 #&gt; wy1 (a) 0.066 0.009 7.091 0.000 #&gt; wy3 ~ #&gt; wx2 (b) 0.009 0.010 0.933 0.351 #&gt; wy2 (a) 0.066 0.009 7.091 0.000 #&gt; wy4 ~ #&gt; wx3 (b) 0.009 0.010 0.933 0.351 #&gt; wy3 (a) 0.066 0.009 7.091 0.000 #&gt; wy5 ~ #&gt; wx4 (b) 0.009 0.010 0.933 0.351 #&gt; wy4 (a) 0.066 0.009 7.091 0.000 #&gt; wy6 ~ #&gt; wx5 (b) 0.009 0.010 0.933 0.351 #&gt; wy5 (a) 0.066 0.009 7.091 0.000 #&gt; wy7 ~ #&gt; wx6 (b) 0.009 0.010 0.933 0.351 #&gt; wy6 (a) 0.066 0.009 7.091 0.000 #&gt; wy8 ~ #&gt; wx7 (b) 0.009 0.010 0.933 0.351 #&gt; wy7 (a) 0.066 0.009 7.091 0.000 #&gt; wy9 ~ #&gt; wx8 (b) 0.009 0.010 0.933 0.351 #&gt; wy8 (a) 0.066 0.009 7.091 0.000 #&gt; wy10 ~ #&gt; wx9 (b) 0.009 0.010 0.933 0.351 #&gt; wy9 (a) 0.066 0.009 7.091 0.000 #&gt; wy11 ~ #&gt; wx10 (b) 0.009 0.010 0.933 0.351 #&gt; wy10 (a) 0.066 0.009 7.091 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.105 0.016 6.436 0.000 #&gt; .wx2 ~~ #&gt; .wy2 0.046 0.013 3.430 0.001 #&gt; .wx3 ~~ #&gt; .wy3 0.058 0.013 4.500 0.000 #&gt; .wx4 ~~ #&gt; .wy4 0.032 0.013 2.539 0.011 #&gt; .wx5 ~~ #&gt; .wy5 0.020 0.012 1.665 0.096 #&gt; .wx6 ~~ #&gt; .wy6 0.054 0.012 4.325 0.000 #&gt; .wx7 ~~ #&gt; .wy7 0.052 0.012 4.510 0.000 #&gt; .wx8 ~~ #&gt; .wy8 0.040 0.013 3.189 0.001 #&gt; .wx9 ~~ #&gt; .wy9 0.036 0.013 2.678 0.007 #&gt; .wx10 ~~ #&gt; .wy10 0.041 0.014 2.892 0.004 #&gt; .wx11 ~~ #&gt; .wy11 0.047 0.016 2.978 0.003 #&gt; .RIx ~~ #&gt; .RIy 1.042 0.046 22.479 0.000 #&gt; .RIsx ~~ #&gt; .RIsy 0.000 0.000 0.823 0.410 #&gt; .RIx ~~ #&gt; .RIsx -0.019 0.004 -5.360 0.000 #&gt; .RIy ~~ #&gt; .RIsy -0.021 0.004 -5.550 0.000 #&gt; .RIx ~~ #&gt; .RIsy -0.008 0.003 -2.453 0.014 #&gt; .RIy ~~ #&gt; .RIsx 0.001 0.003 0.233 0.816 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; .RIsx 0.000 #&gt; .RIsy 0.000 #&gt; wx1 0.000 #&gt; .wx2 0.000 #&gt; .wx3 0.000 #&gt; .wx4 0.000 #&gt; .wx5 0.000 #&gt; .wx6 0.000 #&gt; .wx7 0.000 #&gt; .wx8 0.000 #&gt; .wx9 0.000 #&gt; .wx10 0.000 #&gt; .wx11 0.000 #&gt; wy1 0.000 #&gt; .wy2 0.000 #&gt; .wy3 0.000 #&gt; .wy4 0.000 #&gt; .wy5 0.000 #&gt; .wy6 0.000 #&gt; .wy7 0.000 #&gt; .wy8 0.000 #&gt; .wy9 0.000 #&gt; .wy10 0.000 #&gt; .wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 1.718 0.057 30.394 0.000 #&gt; .RIy 1.656 0.057 28.874 0.000 #&gt; .RIsx 0.002 0.000 7.151 0.000 #&gt; .RIsy 0.002 0.000 6.694 0.000 #&gt; wx1 0.448 0.022 20.478 0.000 #&gt; wy1 0.432 0.021 20.173 0.000 #&gt; .wx2 0.378 0.018 21.405 0.000 #&gt; .wy2 0.382 0.018 21.484 0.000 #&gt; .wx3 0.377 0.017 22.562 0.000 #&gt; .wy3 0.396 0.018 22.493 0.000 #&gt; .wx4 0.358 0.016 21.777 0.000 #&gt; .wy4 0.323 0.015 21.294 0.000 #&gt; .wx5 0.344 0.015 22.653 0.000 #&gt; .wy5 0.354 0.016 22.753 0.000 #&gt; .wx6 0.372 0.016 23.082 0.000 #&gt; .wy6 0.358 0.016 22.781 0.000 #&gt; .wx7 0.364 0.016 22.894 0.000 #&gt; .wy7 0.306 0.014 22.140 0.000 #&gt; .wx8 0.346 0.016 22.265 0.000 #&gt; .wy8 0.379 0.017 22.438 0.000 #&gt; .wx9 0.369 0.017 21.651 0.000 #&gt; .wy9 0.369 0.017 21.490 0.000 #&gt; .wx10 0.341 0.017 19.631 0.000 #&gt; .wy10 0.372 0.019 19.741 0.000 #&gt; .wx11 0.405 0.021 19.525 0.000 #&gt; .wy11 0.349 0.019 18.242 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 3.6.5 Summary results hypo1 Table 3.4: Results Hypo1 CLPM RI-CLPM SC-RI-CLPM LT-RI-CLPM paths est se pvalue est se pvalue est se pvalue est se pvalue eu-integration stability 0.57 0.01 0 0.20 0.01 0.0 0.20 0.01 0.00 0.14 0.01 0.00 partner-effect 0.15 0.01 0 0.08 0.01 0.0 0.07 0.01 0.00 0.06 0.01 0.00 immigrants stability 0.57 0.00 0 0.10 0.01 0.0 0.10 0.01 0.00 0.07 0.01 0.00 partner-effect 0.16 0.01 0 0.03 0.01 0.0 0.03 0.01 0.00 0.01 0.01 0.12 euthanasia stability 0.68 0.00 0 0.12 0.01 0.0 0.11 0.01 0.00 0.06 0.01 0.00 partner-effect 0.18 0.00 0 0.04 0.01 0.0 0.04 0.01 0.00 0.03 0.01 0.00 income_diff stability 0.58 0.00 0 0.12 0.01 0.0 0.12 0.01 0.00 0.07 0.01 0.00 partner-effect 0.15 0.01 0 0.01 0.01 0.4 0.01 0.01 0.29 0.01 0.01 0.35 3.6.6 Conclusion hypo1 3.6.7 Results hypo2 Hypo2 RI-CLPM: Women are more influenced by their (male) spouse than men are influenced by their (female) spouse. 3.6.7.1 CLPM myModel &lt;- &quot; ### control for education x1 + x2 + x3 +x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 ~ ex*oplx y1 + y2 + y3 +y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 ~ ey*oply ### Estimate the lagged effects x2 ~ ax*x1 + bx*y1 x3 ~ ax*x2 + bx*y2 x4 ~ ax*x3 + bx*y3 x5 ~ ax*x4 + bx*y4 x6 ~ ax*x5 + bx*y5 x7 ~ ax*x6 + bx*y6 x8 ~ ax*x7 + bx*y7 x9 ~ ax*x8 + bx*y8 x10 ~ ax*x9 + bx*y9 x11 ~ ax*x10 + bx*y10 y2 ~ by*x1 + ay*y1 y3 ~ by*x2 + ay*y2 y4 ~ by*x3 + ay*y3 y5 ~ by*x4 + ay*y4 y6 ~ by*x5 + ay*y5 y7 ~ by*x6 + ay*y6 y8 ~ by*x7 + ay*y7 y9 ~ by*x8 + ay*y8 y10 ~ by*x9 + ay*y9 y11 ~ by*x10 + ay*y10 # Estimate the (residual) covariance between the variables x1 ~~ y1 # Covariance x2 ~~ y2 x3 ~~ y3 x4 ~~ y4 x5 ~~ y5 x6 ~~ y6 x7 ~~ y7 x8 ~~ y8 x9 ~~ y9 x10 ~~ y10 x11 ~~ y11 # Estimate the (residual) variance of the variables. x1 ~~ x1 # Variances y1 ~~ y1 x2 ~~ x2 # Residual variances y2 ~~ y2 x3 ~~ x3 y3 ~~ y3 x4 ~~ x4 y4 ~~ y4 x5 ~~ x5 y5 ~~ y5 x6 ~~ x6 y6 ~~ y6 x7 ~~ x7 y7 ~~ y7 x8 ~~ x8 y8 ~~ y8 x9 ~~ x9 y9 ~~ y9 x10 ~~ x10 y10 ~~ y10 x11 ~~ x11 y11 ~~ y11 #intercepts x1 ~ 1 y1 ~ 1 x2 ~ 1 y2 ~ 1 x3 ~ 1 y3 ~ 1 x4 ~ 1 y4 ~ 1 x5 ~ 1 y5 ~ 1 x6 ~ 1 y6 ~ 1 x7 ~ 1 y7 ~ 1 x8 ~ 1 y8 ~ 1 x9 ~ 1 y9 ~ 1 x10 ~ 1 y10 ~ 1 x11 ~ 1 y11 ~ 1 dif1 := ax - ay dif2 := bx - by dif3 := ex - ey &quot; # Estimate models a bit faster: estimate &lt;- function(x) lavaan(myModel, data = x, missing = &quot;fiml.x&quot;, meanstructure = T) library(future.apply) plan(multisession) results_temp &lt;- future_lapply(datalist_ori, estimate) summary(results_temp[[1]]) results[[17]] &lt;- results_temp[[1]] results[[18]] &lt;- results_temp[[2]] results[[19]] &lt;- results_temp[[3]] results[[20]] &lt;- results_temp[[4]] names(results)[17:20] &lt;- c(&quot;fitm1h2y1&quot;, &quot;fitm1h2y2&quot;, &quot;fitm1h2y3&quot;, &quot;fitm1h2y4&quot;) load(&quot;addfiles/results.Rdata&quot;) summary(results[[17]]) summary(results[[18]]) summary(results[[19]]) summary(results[[20]]) #&gt; lavaan 0.6-9 ended normally after 39 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 117 #&gt; Number of equality constraints 56 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1415 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 3233.567 #&gt; Degrees of freedom 258 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; x1 ~ #&gt; oplx (ex) 0.043 0.002 17.477 0.000 #&gt; x2 ~ #&gt; oplx (ex) 0.043 0.002 17.477 0.000 #&gt; x3 ~ #&gt; oplx (ex) 0.043 0.002 17.477 0.000 #&gt; x4 ~ #&gt; oplx (ex) 0.043 0.002 17.477 0.000 #&gt; x5 ~ #&gt; oplx (ex) 0.043 0.002 17.477 0.000 #&gt; x6 ~ #&gt; oplx (ex) 0.043 0.002 17.477 0.000 #&gt; x7 ~ #&gt; oplx (ex) 0.043 0.002 17.477 0.000 #&gt; x8 ~ #&gt; oplx (ex) 0.043 0.002 17.477 0.000 #&gt; x9 ~ #&gt; oplx (ex) 0.043 0.002 17.477 0.000 #&gt; x10 ~ #&gt; oplx (ex) 0.043 0.002 17.477 0.000 #&gt; x11 ~ #&gt; oplx (ex) 0.043 0.002 17.477 0.000 #&gt; y1 ~ #&gt; oply (ey) 0.036 0.002 15.163 0.000 #&gt; y2 ~ #&gt; oply (ey) 0.036 0.002 15.163 0.000 #&gt; y3 ~ #&gt; oply (ey) 0.036 0.002 15.163 0.000 #&gt; y4 ~ #&gt; oply (ey) 0.036 0.002 15.163 0.000 #&gt; y5 ~ #&gt; oply (ey) 0.036 0.002 15.163 0.000 #&gt; y6 ~ #&gt; oply (ey) 0.036 0.002 15.163 0.000 #&gt; y7 ~ #&gt; oply (ey) 0.036 0.002 15.163 0.000 #&gt; y8 ~ #&gt; oply (ey) 0.036 0.002 15.163 0.000 #&gt; y9 ~ #&gt; oply (ey) 0.036 0.002 15.163 0.000 #&gt; y10 ~ #&gt; oply (ey) 0.036 0.002 15.163 0.000 #&gt; y11 ~ #&gt; oply (ey) 0.036 0.002 15.163 0.000 #&gt; x2 ~ #&gt; x1 (ax) 0.605 0.007 86.610 0.000 #&gt; y1 (bx) 0.163 0.009 18.854 0.000 #&gt; x3 ~ #&gt; x2 (ax) 0.605 0.007 86.610 0.000 #&gt; y2 (bx) 0.163 0.009 18.854 0.000 #&gt; x4 ~ #&gt; x3 (ax) 0.605 0.007 86.610 0.000 #&gt; y3 (bx) 0.163 0.009 18.854 0.000 #&gt; x5 ~ #&gt; x4 (ax) 0.605 0.007 86.610 0.000 #&gt; y4 (bx) 0.163 0.009 18.854 0.000 #&gt; x6 ~ #&gt; x5 (ax) 0.605 0.007 86.610 0.000 #&gt; y5 (bx) 0.163 0.009 18.854 0.000 #&gt; x7 ~ #&gt; x6 (ax) 0.605 0.007 86.610 0.000 #&gt; y6 (bx) 0.163 0.009 18.854 0.000 #&gt; x8 ~ #&gt; x7 (ax) 0.605 0.007 86.610 0.000 #&gt; y7 (bx) 0.163 0.009 18.854 0.000 #&gt; x9 ~ #&gt; x8 (ax) 0.605 0.007 86.610 0.000 #&gt; y8 (bx) 0.163 0.009 18.854 0.000 #&gt; x10 ~ #&gt; x9 (ax) 0.605 0.007 86.610 0.000 #&gt; y9 (bx) 0.163 0.009 18.854 0.000 #&gt; x11 ~ #&gt; x10 (ax) 0.605 0.007 86.610 0.000 #&gt; y10 (bx) 0.163 0.009 18.854 0.000 #&gt; y2 ~ #&gt; x1 (by) 0.151 0.007 23.192 0.000 #&gt; y1 (ay) 0.525 0.008 67.181 0.000 #&gt; y3 ~ #&gt; x2 (by) 0.151 0.007 23.192 0.000 #&gt; y2 (ay) 0.525 0.008 67.181 0.000 #&gt; y4 ~ #&gt; x3 (by) 0.151 0.007 23.192 0.000 #&gt; y3 (ay) 0.525 0.008 67.181 0.000 #&gt; y5 ~ #&gt; x4 (by) 0.151 0.007 23.192 0.000 #&gt; y4 (ay) 0.525 0.008 67.181 0.000 #&gt; y6 ~ #&gt; x5 (by) 0.151 0.007 23.192 0.000 #&gt; y5 (ay) 0.525 0.008 67.181 0.000 #&gt; y7 ~ #&gt; x6 (by) 0.151 0.007 23.192 0.000 #&gt; y6 (ay) 0.525 0.008 67.181 0.000 #&gt; y8 ~ #&gt; x7 (by) 0.151 0.007 23.192 0.000 #&gt; y7 (ay) 0.525 0.008 67.181 0.000 #&gt; y9 ~ #&gt; x8 (by) 0.151 0.007 23.192 0.000 #&gt; y8 (ay) 0.525 0.008 67.181 0.000 #&gt; y10 ~ #&gt; x9 (by) 0.151 0.007 23.192 0.000 #&gt; y9 (ay) 0.525 0.008 67.181 0.000 #&gt; y11 ~ #&gt; x10 (by) 0.151 0.007 23.192 0.000 #&gt; y10 (ay) 0.525 0.008 67.181 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 ~~ #&gt; .y1 0.410 0.030 13.668 0.000 #&gt; .x2 ~~ #&gt; .y2 0.123 0.021 5.822 0.000 #&gt; .x3 ~~ #&gt; .y3 0.119 0.020 5.841 0.000 #&gt; .x4 ~~ #&gt; .y4 0.154 0.022 7.146 0.000 #&gt; .x5 ~~ #&gt; .y5 0.146 0.025 5.956 0.000 #&gt; .x6 ~~ #&gt; .y6 0.112 0.020 5.655 0.000 #&gt; .x7 ~~ #&gt; .y7 0.104 0.018 5.963 0.000 #&gt; .x8 ~~ #&gt; .y8 0.114 0.021 5.336 0.000 #&gt; .x9 ~~ #&gt; .y9 0.119 0.022 5.476 0.000 #&gt; .x10 ~~ #&gt; .y10 0.081 0.023 3.487 0.000 #&gt; .x11 ~~ #&gt; .y11 0.131 0.024 5.568 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 1.087 0.040 27.297 0.000 #&gt; .y1 1.125 0.036 31.131 0.000 #&gt; .x2 -0.030 0.037 -0.818 0.413 #&gt; .y2 0.179 0.034 5.266 0.000 #&gt; .x3 -0.107 0.037 -2.916 0.004 #&gt; .y3 0.118 0.034 3.499 0.000 #&gt; .x4 -0.408 0.038 -10.870 0.000 #&gt; .y4 -0.216 0.034 -6.268 0.000 #&gt; .x5 -0.001 0.038 -0.039 0.969 #&gt; .y5 0.015 0.035 0.418 0.676 #&gt; .x6 -0.304 0.036 -8.390 0.000 #&gt; .y6 -0.054 0.034 -1.600 0.110 #&gt; .x7 -0.295 0.036 -8.238 0.000 #&gt; .y7 -0.089 0.033 -2.666 0.008 #&gt; .x8 -0.124 0.037 -3.329 0.001 #&gt; .y8 0.062 0.035 1.785 0.074 #&gt; .x9 -0.278 0.037 -7.464 0.000 #&gt; .y9 -0.013 0.035 -0.376 0.707 #&gt; .x10 -0.067 0.038 -1.745 0.081 #&gt; .y10 0.177 0.036 4.925 0.000 #&gt; .x11 -0.083 0.039 -2.115 0.034 #&gt; .y11 0.137 0.036 3.769 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 1.375 0.045 30.425 0.000 #&gt; .y1 1.013 0.034 29.369 0.000 #&gt; .x2 0.825 0.030 27.054 0.000 #&gt; .y2 0.679 0.026 26.286 0.000 #&gt; .x3 0.775 0.029 26.359 0.000 #&gt; .y3 0.599 0.024 25.470 0.000 #&gt; .x4 0.809 0.031 25.775 0.000 #&gt; .y4 0.642 0.026 24.959 0.000 #&gt; .x5 0.922 0.036 25.674 0.000 #&gt; .y5 0.703 0.028 25.034 0.000 #&gt; .x6 0.703 0.027 25.881 0.000 #&gt; .y6 0.584 0.023 24.937 0.000 #&gt; .x7 0.637 0.025 25.650 0.000 #&gt; .y7 0.532 0.021 24.981 0.000 #&gt; .x8 0.741 0.029 25.230 0.000 #&gt; .y8 0.628 0.025 24.978 0.000 #&gt; .x9 0.744 0.029 25.613 0.000 #&gt; .y9 0.648 0.026 24.738 0.000 #&gt; .x10 0.757 0.031 24.217 0.000 #&gt; .y10 0.651 0.028 23.538 0.000 #&gt; .x11 0.787 0.033 23.979 0.000 #&gt; .y11 0.625 0.027 22.882 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 0.080 0.011 7.385 0.000 #&gt; dif2 0.012 0.011 1.097 0.273 #&gt; dif3 0.007 0.003 2.106 0.035 #&gt; #&gt; lavaan 0.6-9 ended normally after 38 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 117 #&gt; Number of equality constraints 56 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1230 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 4304.782 #&gt; Degrees of freedom 258 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; x1 ~ #&gt; oplx (ex) 0.024 0.002 12.493 0.000 #&gt; x2 ~ #&gt; oplx (ex) 0.024 0.002 12.493 0.000 #&gt; x3 ~ #&gt; oplx (ex) 0.024 0.002 12.493 0.000 #&gt; x4 ~ #&gt; oplx (ex) 0.024 0.002 12.493 0.000 #&gt; x5 ~ #&gt; oplx (ex) 0.024 0.002 12.493 0.000 #&gt; x6 ~ #&gt; oplx (ex) 0.024 0.002 12.493 0.000 #&gt; x7 ~ #&gt; oplx (ex) 0.024 0.002 12.493 0.000 #&gt; x8 ~ #&gt; oplx (ex) 0.024 0.002 12.493 0.000 #&gt; x9 ~ #&gt; oplx (ex) 0.024 0.002 12.493 0.000 #&gt; x10 ~ #&gt; oplx (ex) 0.024 0.002 12.493 0.000 #&gt; x11 ~ #&gt; oplx (ex) 0.024 0.002 12.493 0.000 #&gt; y1 ~ #&gt; oply (ey) 0.029 0.002 14.613 0.000 #&gt; y2 ~ #&gt; oply (ey) 0.029 0.002 14.613 0.000 #&gt; y3 ~ #&gt; oply (ey) 0.029 0.002 14.613 0.000 #&gt; y4 ~ #&gt; oply (ey) 0.029 0.002 14.613 0.000 #&gt; y5 ~ #&gt; oply (ey) 0.029 0.002 14.613 0.000 #&gt; y6 ~ #&gt; oply (ey) 0.029 0.002 14.613 0.000 #&gt; y7 ~ #&gt; oply (ey) 0.029 0.002 14.613 0.000 #&gt; y8 ~ #&gt; oply (ey) 0.029 0.002 14.613 0.000 #&gt; y9 ~ #&gt; oply (ey) 0.029 0.002 14.613 0.000 #&gt; y10 ~ #&gt; oply (ey) 0.029 0.002 14.613 0.000 #&gt; y11 ~ #&gt; oply (ey) 0.029 0.002 14.613 0.000 #&gt; x2 ~ #&gt; x1 (ax) 0.569 0.007 79.995 0.000 #&gt; y1 (bx) 0.176 0.007 23.736 0.000 #&gt; x3 ~ #&gt; x2 (ax) 0.569 0.007 79.995 0.000 #&gt; y2 (bx) 0.176 0.007 23.736 0.000 #&gt; x4 ~ #&gt; x3 (ax) 0.569 0.007 79.995 0.000 #&gt; y3 (bx) 0.176 0.007 23.736 0.000 #&gt; x5 ~ #&gt; x4 (ax) 0.569 0.007 79.995 0.000 #&gt; y4 (bx) 0.176 0.007 23.736 0.000 #&gt; x6 ~ #&gt; x5 (ax) 0.569 0.007 79.995 0.000 #&gt; y5 (bx) 0.176 0.007 23.736 0.000 #&gt; x7 ~ #&gt; x6 (ax) 0.569 0.007 79.995 0.000 #&gt; y6 (bx) 0.176 0.007 23.736 0.000 #&gt; x8 ~ #&gt; x7 (ax) 0.569 0.007 79.995 0.000 #&gt; y7 (bx) 0.176 0.007 23.736 0.000 #&gt; x9 ~ #&gt; x8 (ax) 0.569 0.007 79.995 0.000 #&gt; y8 (bx) 0.176 0.007 23.736 0.000 #&gt; x10 ~ #&gt; x9 (ax) 0.569 0.007 79.995 0.000 #&gt; y9 (bx) 0.176 0.007 23.736 0.000 #&gt; x11 ~ #&gt; x10 (ax) 0.569 0.007 79.995 0.000 #&gt; y10 (bx) 0.176 0.007 23.736 0.000 #&gt; y2 ~ #&gt; x1 (by) 0.152 0.007 21.425 0.000 #&gt; y1 (ay) 0.572 0.007 79.358 0.000 #&gt; y3 ~ #&gt; x2 (by) 0.152 0.007 21.425 0.000 #&gt; y2 (ay) 0.572 0.007 79.358 0.000 #&gt; y4 ~ #&gt; x3 (by) 0.152 0.007 21.425 0.000 #&gt; y3 (ay) 0.572 0.007 79.358 0.000 #&gt; y5 ~ #&gt; x4 (by) 0.152 0.007 21.425 0.000 #&gt; y4 (ay) 0.572 0.007 79.358 0.000 #&gt; y6 ~ #&gt; x5 (by) 0.152 0.007 21.425 0.000 #&gt; y5 (ay) 0.572 0.007 79.358 0.000 #&gt; y7 ~ #&gt; x6 (by) 0.152 0.007 21.425 0.000 #&gt; y6 (ay) 0.572 0.007 79.358 0.000 #&gt; y8 ~ #&gt; x7 (by) 0.152 0.007 21.425 0.000 #&gt; y7 (ay) 0.572 0.007 79.358 0.000 #&gt; y9 ~ #&gt; x8 (by) 0.152 0.007 21.425 0.000 #&gt; y8 (ay) 0.572 0.007 79.358 0.000 #&gt; y10 ~ #&gt; x9 (by) 0.152 0.007 21.425 0.000 #&gt; y9 (ay) 0.572 0.007 79.358 0.000 #&gt; y11 ~ #&gt; x10 (by) 0.152 0.007 21.425 0.000 #&gt; y10 (ay) 0.572 0.007 79.358 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 ~~ #&gt; .y1 0.383 0.023 16.611 0.000 #&gt; .x2 ~~ #&gt; .y2 0.101 0.015 6.608 0.000 #&gt; .x3 ~~ #&gt; .y3 0.104 0.015 6.808 0.000 #&gt; .x4 ~~ #&gt; .y4 0.071 0.014 5.063 0.000 #&gt; .x5 ~~ #&gt; .y5 0.086 0.014 6.086 0.000 #&gt; .x6 ~~ #&gt; .y6 0.073 0.014 5.334 0.000 #&gt; .x7 ~~ #&gt; .y7 0.055 0.014 4.040 0.000 #&gt; .x8 ~~ #&gt; .y8 0.075 0.015 5.110 0.000 #&gt; .x9 ~~ #&gt; .y9 0.041 0.015 2.712 0.007 #&gt; .x10 ~~ #&gt; .y10 0.089 0.015 5.882 0.000 #&gt; .x11 ~~ #&gt; .y11 0.101 0.015 6.750 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.917 0.032 28.541 0.000 #&gt; .y1 0.927 0.031 30.028 0.000 #&gt; .x2 0.038 0.030 1.266 0.206 #&gt; .y2 0.043 0.029 1.499 0.134 #&gt; .x3 0.009 0.030 0.313 0.754 #&gt; .y3 0.054 0.029 1.898 0.058 #&gt; .x4 0.010 0.029 0.330 0.741 #&gt; .y4 0.025 0.029 0.873 0.382 #&gt; .x5 0.008 0.030 0.266 0.790 #&gt; .y5 0.025 0.029 0.871 0.384 #&gt; .x6 0.007 0.030 0.222 0.825 #&gt; .y6 0.025 0.028 0.893 0.372 #&gt; .x7 0.029 0.030 0.976 0.329 #&gt; .y7 0.050 0.028 1.765 0.078 #&gt; .x8 -0.036 0.030 -1.210 0.226 #&gt; .y8 -0.012 0.029 -0.406 0.685 #&gt; .x9 -0.046 0.030 -1.500 0.134 #&gt; .y9 0.004 0.030 0.124 0.902 #&gt; .x10 0.115 0.031 3.711 0.000 #&gt; .y10 0.112 0.030 3.768 0.000 #&gt; .x11 0.001 0.031 0.017 0.986 #&gt; .y11 0.039 0.030 1.313 0.189 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.946 0.031 30.278 0.000 #&gt; .y1 0.869 0.029 30.124 0.000 #&gt; .x2 0.567 0.021 27.152 0.000 #&gt; .y2 0.551 0.020 27.172 0.000 #&gt; .x3 0.548 0.021 26.509 0.000 #&gt; .y3 0.504 0.019 26.222 0.000 #&gt; .x4 0.460 0.018 25.860 0.000 #&gt; .y4 0.485 0.019 25.470 0.000 #&gt; .x5 0.477 0.019 25.760 0.000 #&gt; .y5 0.463 0.018 25.114 0.000 #&gt; .x6 0.505 0.019 26.222 0.000 #&gt; .y6 0.442 0.017 26.327 0.000 #&gt; .x7 0.488 0.019 25.910 0.000 #&gt; .y7 0.451 0.017 26.076 0.000 #&gt; .x8 0.479 0.019 25.427 0.000 #&gt; .y8 0.484 0.019 25.571 0.000 #&gt; .x9 0.501 0.019 25.754 0.000 #&gt; .y9 0.506 0.020 25.472 0.000 #&gt; .x10 0.511 0.021 24.631 0.000 #&gt; .y10 0.454 0.019 24.284 0.000 #&gt; .x11 0.492 0.020 24.422 0.000 #&gt; .y11 0.451 0.019 24.049 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 -0.003 0.010 -0.301 0.763 #&gt; dif2 0.024 0.010 2.265 0.024 #&gt; dif3 -0.004 0.003 -1.575 0.115 #&gt; #&gt; lavaan 0.6-9 ended normally after 68 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 117 #&gt; Number of equality constraints 56 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1290 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 5137.978 #&gt; Degrees of freedom 258 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; x1 ~ #&gt; oplx (ex) 0.004 0.002 2.542 0.011 #&gt; x2 ~ #&gt; oplx (ex) 0.004 0.002 2.542 0.011 #&gt; x3 ~ #&gt; oplx (ex) 0.004 0.002 2.542 0.011 #&gt; x4 ~ #&gt; oplx (ex) 0.004 0.002 2.542 0.011 #&gt; x5 ~ #&gt; oplx (ex) 0.004 0.002 2.542 0.011 #&gt; x6 ~ #&gt; oplx (ex) 0.004 0.002 2.542 0.011 #&gt; x7 ~ #&gt; oplx (ex) 0.004 0.002 2.542 0.011 #&gt; x8 ~ #&gt; oplx (ex) 0.004 0.002 2.542 0.011 #&gt; x9 ~ #&gt; oplx (ex) 0.004 0.002 2.542 0.011 #&gt; x10 ~ #&gt; oplx (ex) 0.004 0.002 2.542 0.011 #&gt; x11 ~ #&gt; oplx (ex) 0.004 0.002 2.542 0.011 #&gt; y1 ~ #&gt; oply (ey) 0.004 0.002 2.244 0.025 #&gt; y2 ~ #&gt; oply (ey) 0.004 0.002 2.244 0.025 #&gt; y3 ~ #&gt; oply (ey) 0.004 0.002 2.244 0.025 #&gt; y4 ~ #&gt; oply (ey) 0.004 0.002 2.244 0.025 #&gt; y5 ~ #&gt; oply (ey) 0.004 0.002 2.244 0.025 #&gt; y6 ~ #&gt; oply (ey) 0.004 0.002 2.244 0.025 #&gt; y7 ~ #&gt; oply (ey) 0.004 0.002 2.244 0.025 #&gt; y8 ~ #&gt; oply (ey) 0.004 0.002 2.244 0.025 #&gt; y9 ~ #&gt; oply (ey) 0.004 0.002 2.244 0.025 #&gt; y10 ~ #&gt; oply (ey) 0.004 0.002 2.244 0.025 #&gt; y11 ~ #&gt; oply (ey) 0.004 0.002 2.244 0.025 #&gt; x2 ~ #&gt; x1 (ax) 0.663 0.007 101.002 0.000 #&gt; y1 (bx) 0.192 0.007 29.543 0.000 #&gt; x3 ~ #&gt; x2 (ax) 0.663 0.007 101.002 0.000 #&gt; y2 (bx) 0.192 0.007 29.543 0.000 #&gt; x4 ~ #&gt; x3 (ax) 0.663 0.007 101.002 0.000 #&gt; y3 (bx) 0.192 0.007 29.543 0.000 #&gt; x5 ~ #&gt; x4 (ax) 0.663 0.007 101.002 0.000 #&gt; y4 (bx) 0.192 0.007 29.543 0.000 #&gt; x6 ~ #&gt; x5 (ax) 0.663 0.007 101.002 0.000 #&gt; y5 (bx) 0.192 0.007 29.543 0.000 #&gt; x7 ~ #&gt; x6 (ax) 0.663 0.007 101.002 0.000 #&gt; y6 (bx) 0.192 0.007 29.543 0.000 #&gt; x8 ~ #&gt; x7 (ax) 0.663 0.007 101.002 0.000 #&gt; y7 (bx) 0.192 0.007 29.543 0.000 #&gt; x9 ~ #&gt; x8 (ax) 0.663 0.007 101.002 0.000 #&gt; y8 (bx) 0.192 0.007 29.543 0.000 #&gt; x10 ~ #&gt; x9 (ax) 0.663 0.007 101.002 0.000 #&gt; y9 (bx) 0.192 0.007 29.543 0.000 #&gt; x11 ~ #&gt; x10 (ax) 0.663 0.007 101.002 0.000 #&gt; y10 (bx) 0.192 0.007 29.543 0.000 #&gt; y2 ~ #&gt; x1 (by) 0.172 0.007 25.776 0.000 #&gt; y1 (ay) 0.697 0.006 109.007 0.000 #&gt; y3 ~ #&gt; x2 (by) 0.172 0.007 25.776 0.000 #&gt; y2 (ay) 0.697 0.006 109.007 0.000 #&gt; y4 ~ #&gt; x3 (by) 0.172 0.007 25.776 0.000 #&gt; y3 (ay) 0.697 0.006 109.007 0.000 #&gt; y5 ~ #&gt; x4 (by) 0.172 0.007 25.776 0.000 #&gt; y4 (ay) 0.697 0.006 109.007 0.000 #&gt; y6 ~ #&gt; x5 (by) 0.172 0.007 25.776 0.000 #&gt; y5 (ay) 0.697 0.006 109.007 0.000 #&gt; y7 ~ #&gt; x6 (by) 0.172 0.007 25.776 0.000 #&gt; y6 (ay) 0.697 0.006 109.007 0.000 #&gt; y8 ~ #&gt; x7 (by) 0.172 0.007 25.776 0.000 #&gt; y7 (ay) 0.697 0.006 109.007 0.000 #&gt; y9 ~ #&gt; x8 (by) 0.172 0.007 25.776 0.000 #&gt; y8 (ay) 0.697 0.006 109.007 0.000 #&gt; y10 ~ #&gt; x9 (by) 0.172 0.007 25.776 0.000 #&gt; y9 (ay) 0.697 0.006 109.007 0.000 #&gt; y11 ~ #&gt; x10 (by) 0.172 0.007 25.776 0.000 #&gt; y10 (ay) 0.697 0.006 109.007 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 ~~ #&gt; .y1 0.659 0.029 22.944 0.000 #&gt; .x2 ~~ #&gt; .y2 0.081 0.012 6.888 0.000 #&gt; .x3 ~~ #&gt; .y3 0.086 0.011 7.794 0.000 #&gt; .x4 ~~ #&gt; .y4 0.086 0.011 7.827 0.000 #&gt; .x5 ~~ #&gt; .y5 0.078 0.010 7.479 0.000 #&gt; .x6 ~~ #&gt; .y6 0.066 0.009 7.415 0.000 #&gt; .x7 ~~ #&gt; .y7 0.056 0.009 6.178 0.000 #&gt; .x8 ~~ #&gt; .y8 0.062 0.010 6.021 0.000 #&gt; .x9 ~~ #&gt; .y9 0.075 0.011 7.104 0.000 #&gt; .x10 ~~ #&gt; .y10 0.096 0.013 7.549 0.000 #&gt; .x11 ~~ #&gt; .y11 0.101 0.012 8.429 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 4.254 0.029 144.503 0.000 #&gt; .y1 4.277 0.029 145.918 0.000 #&gt; .x2 0.588 0.033 17.686 0.000 #&gt; .y2 0.563 0.032 17.659 0.000 #&gt; .x3 0.566 0.033 17.314 0.000 #&gt; .y3 0.505 0.032 15.659 0.000 #&gt; .x4 0.608 0.033 18.669 0.000 #&gt; .y4 0.576 0.032 17.810 0.000 #&gt; .x5 0.596 0.033 18.115 0.000 #&gt; .y5 0.556 0.032 17.357 0.000 #&gt; .x6 0.599 0.033 18.210 0.000 #&gt; .y6 0.547 0.031 17.557 0.000 #&gt; .x7 0.617 0.033 18.895 0.000 #&gt; .y7 0.562 0.032 17.722 0.000 #&gt; .x8 0.600 0.033 18.089 0.000 #&gt; .y8 0.551 0.033 16.924 0.000 #&gt; .x9 0.539 0.034 16.020 0.000 #&gt; .y9 0.510 0.032 15.819 0.000 #&gt; .x10 0.572 0.034 16.738 0.000 #&gt; .y10 0.519 0.033 15.711 0.000 #&gt; .x11 0.627 0.034 18.473 0.000 #&gt; .y11 0.575 0.033 17.293 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 1.062 0.034 31.012 0.000 #&gt; .y1 1.112 0.036 30.945 0.000 #&gt; .x2 0.440 0.016 26.796 0.000 #&gt; .y2 0.392 0.015 26.698 0.000 #&gt; .x3 0.361 0.014 26.107 0.000 #&gt; .y3 0.406 0.016 25.958 0.000 #&gt; .x4 0.339 0.013 26.022 0.000 #&gt; .y4 0.402 0.016 25.847 0.000 #&gt; .x5 0.340 0.013 25.598 0.000 #&gt; .y5 0.346 0.014 25.297 0.000 #&gt; .x6 0.344 0.013 26.425 0.000 #&gt; .y6 0.271 0.011 25.766 0.000 #&gt; .x7 0.308 0.012 25.913 0.000 #&gt; .y7 0.306 0.012 25.227 0.000 #&gt; .x8 0.325 0.013 25.062 0.000 #&gt; .y8 0.349 0.014 24.754 0.000 #&gt; .x9 0.369 0.015 25.268 0.000 #&gt; .y9 0.320 0.013 25.193 0.000 #&gt; .x10 0.395 0.017 23.634 0.000 #&gt; .y10 0.358 0.015 23.933 0.000 #&gt; .x11 0.362 0.015 24.042 0.000 #&gt; .y11 0.367 0.015 23.806 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 -0.034 0.010 -3.489 0.000 #&gt; dif2 0.020 0.010 2.051 0.040 #&gt; dif3 0.000 0.002 0.218 0.827 #&gt; #&gt; lavaan 0.6-9 ended normally after 54 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 117 #&gt; Number of equality constraints 56 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1309 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 4146.492 #&gt; Degrees of freedom 258 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; x1 ~ #&gt; oplx (ex) -0.020 0.002 -10.112 0.000 #&gt; x2 ~ #&gt; oplx (ex) -0.020 0.002 -10.112 0.000 #&gt; x3 ~ #&gt; oplx (ex) -0.020 0.002 -10.112 0.000 #&gt; x4 ~ #&gt; oplx (ex) -0.020 0.002 -10.112 0.000 #&gt; x5 ~ #&gt; oplx (ex) -0.020 0.002 -10.112 0.000 #&gt; x6 ~ #&gt; oplx (ex) -0.020 0.002 -10.112 0.000 #&gt; x7 ~ #&gt; oplx (ex) -0.020 0.002 -10.112 0.000 #&gt; x8 ~ #&gt; oplx (ex) -0.020 0.002 -10.112 0.000 #&gt; x9 ~ #&gt; oplx (ex) -0.020 0.002 -10.112 0.000 #&gt; x10 ~ #&gt; oplx (ex) -0.020 0.002 -10.112 0.000 #&gt; x11 ~ #&gt; oplx (ex) -0.020 0.002 -10.112 0.000 #&gt; y1 ~ #&gt; oply (ey) -0.014 0.002 -6.762 0.000 #&gt; y2 ~ #&gt; oply (ey) -0.014 0.002 -6.762 0.000 #&gt; y3 ~ #&gt; oply (ey) -0.014 0.002 -6.762 0.000 #&gt; y4 ~ #&gt; oply (ey) -0.014 0.002 -6.762 0.000 #&gt; y5 ~ #&gt; oply (ey) -0.014 0.002 -6.762 0.000 #&gt; y6 ~ #&gt; oply (ey) -0.014 0.002 -6.762 0.000 #&gt; y7 ~ #&gt; oply (ey) -0.014 0.002 -6.762 0.000 #&gt; y8 ~ #&gt; oply (ey) -0.014 0.002 -6.762 0.000 #&gt; y9 ~ #&gt; oply (ey) -0.014 0.002 -6.762 0.000 #&gt; y10 ~ #&gt; oply (ey) -0.014 0.002 -6.762 0.000 #&gt; y11 ~ #&gt; oply (ey) -0.014 0.002 -6.762 0.000 #&gt; x2 ~ #&gt; x1 (ax) 0.606 0.007 87.759 0.000 #&gt; y1 (bx) 0.143 0.008 18.295 0.000 #&gt; x3 ~ #&gt; x2 (ax) 0.606 0.007 87.759 0.000 #&gt; y2 (bx) 0.143 0.008 18.295 0.000 #&gt; x4 ~ #&gt; x3 (ax) 0.606 0.007 87.759 0.000 #&gt; y3 (bx) 0.143 0.008 18.295 0.000 #&gt; x5 ~ #&gt; x4 (ax) 0.606 0.007 87.759 0.000 #&gt; y4 (bx) 0.143 0.008 18.295 0.000 #&gt; x6 ~ #&gt; x5 (ax) 0.606 0.007 87.759 0.000 #&gt; y5 (bx) 0.143 0.008 18.295 0.000 #&gt; x7 ~ #&gt; x6 (ax) 0.606 0.007 87.759 0.000 #&gt; y6 (bx) 0.143 0.008 18.295 0.000 #&gt; x8 ~ #&gt; x7 (ax) 0.606 0.007 87.759 0.000 #&gt; y7 (bx) 0.143 0.008 18.295 0.000 #&gt; x9 ~ #&gt; x8 (ax) 0.606 0.007 87.759 0.000 #&gt; y8 (bx) 0.143 0.008 18.295 0.000 #&gt; x10 ~ #&gt; x9 (ax) 0.606 0.007 87.759 0.000 #&gt; y9 (bx) 0.143 0.008 18.295 0.000 #&gt; x11 ~ #&gt; x10 (ax) 0.606 0.007 87.759 0.000 #&gt; y10 (bx) 0.143 0.008 18.295 0.000 #&gt; y2 ~ #&gt; x1 (by) 0.152 0.007 21.574 0.000 #&gt; y1 (ay) 0.539 0.008 71.026 0.000 #&gt; y3 ~ #&gt; x2 (by) 0.152 0.007 21.574 0.000 #&gt; y2 (ay) 0.539 0.008 71.026 0.000 #&gt; y4 ~ #&gt; x3 (by) 0.152 0.007 21.574 0.000 #&gt; y3 (ay) 0.539 0.008 71.026 0.000 #&gt; y5 ~ #&gt; x4 (by) 0.152 0.007 21.574 0.000 #&gt; y4 (ay) 0.539 0.008 71.026 0.000 #&gt; y6 ~ #&gt; x5 (by) 0.152 0.007 21.574 0.000 #&gt; y5 (ay) 0.539 0.008 71.026 0.000 #&gt; y7 ~ #&gt; x6 (by) 0.152 0.007 21.574 0.000 #&gt; y6 (ay) 0.539 0.008 71.026 0.000 #&gt; y8 ~ #&gt; x7 (by) 0.152 0.007 21.574 0.000 #&gt; y7 (ay) 0.539 0.008 71.026 0.000 #&gt; y9 ~ #&gt; x8 (by) 0.152 0.007 21.574 0.000 #&gt; y8 (ay) 0.539 0.008 71.026 0.000 #&gt; y10 ~ #&gt; x9 (by) 0.152 0.007 21.574 0.000 #&gt; y9 (ay) 0.539 0.008 71.026 0.000 #&gt; y11 ~ #&gt; x10 (by) 0.152 0.007 21.574 0.000 #&gt; y10 (ay) 0.539 0.008 71.026 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 ~~ #&gt; .y1 0.421 0.025 16.634 0.000 #&gt; .x2 ~~ #&gt; .y2 0.140 0.016 8.814 0.000 #&gt; .x3 ~~ #&gt; .y3 0.099 0.016 6.276 0.000 #&gt; .x4 ~~ #&gt; .y4 0.111 0.017 6.581 0.000 #&gt; .x5 ~~ #&gt; .y5 0.064 0.014 4.462 0.000 #&gt; .x6 ~~ #&gt; .y6 0.133 0.016 8.390 0.000 #&gt; .x7 ~~ #&gt; .y7 0.091 0.014 6.399 0.000 #&gt; .x8 ~~ #&gt; .y8 0.085 0.016 5.396 0.000 #&gt; .x9 ~~ #&gt; .y9 0.102 0.017 6.129 0.000 #&gt; .x10 ~~ #&gt; .y10 0.093 0.017 5.451 0.000 #&gt; .x11 ~~ #&gt; .y11 0.071 0.017 4.076 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 3.891 0.034 115.721 0.000 #&gt; .y1 3.968 0.032 123.603 0.000 #&gt; .x2 1.174 0.045 26.127 0.000 #&gt; .y2 1.431 0.043 33.089 0.000 #&gt; .x3 1.144 0.045 25.458 0.000 #&gt; .y3 1.332 0.044 30.327 0.000 #&gt; .x4 1.145 0.045 25.352 0.000 #&gt; .y4 1.375 0.044 31.356 0.000 #&gt; .x5 1.262 0.045 28.302 0.000 #&gt; .y5 1.425 0.043 32.801 0.000 #&gt; .x6 1.065 0.046 23.342 0.000 #&gt; .y6 1.296 0.044 29.325 0.000 #&gt; .x7 1.163 0.045 26.035 0.000 #&gt; .y7 1.349 0.043 31.216 0.000 #&gt; .x8 1.211 0.045 26.754 0.000 #&gt; .y8 1.402 0.044 31.804 0.000 #&gt; .x9 1.196 0.046 26.224 0.000 #&gt; .y9 1.366 0.045 30.539 0.000 #&gt; .x10 1.185 0.046 25.819 0.000 #&gt; .y10 1.332 0.045 29.538 0.000 #&gt; .x11 1.237 0.046 26.737 0.000 #&gt; .y11 1.361 0.045 30.067 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 1.090 0.036 30.464 0.000 #&gt; .y1 0.914 0.030 30.174 0.000 #&gt; .x2 0.606 0.022 27.077 0.000 #&gt; .y2 0.539 0.020 27.048 0.000 #&gt; .x3 0.536 0.020 26.341 0.000 #&gt; .y3 0.559 0.021 26.210 0.000 #&gt; .x4 0.560 0.022 25.830 0.000 #&gt; .y4 0.534 0.021 25.457 0.000 #&gt; .x5 0.463 0.018 25.301 0.000 #&gt; .y5 0.479 0.019 25.243 0.000 #&gt; .x6 0.555 0.021 26.217 0.000 #&gt; .y6 0.534 0.020 26.107 0.000 #&gt; .x7 0.495 0.019 26.004 0.000 #&gt; .y7 0.447 0.017 25.646 0.000 #&gt; .x8 0.532 0.021 25.615 0.000 #&gt; .y8 0.508 0.020 25.372 0.000 #&gt; .x9 0.548 0.021 25.808 0.000 #&gt; .y9 0.542 0.022 25.161 0.000 #&gt; .x10 0.529 0.022 24.542 0.000 #&gt; .y10 0.533 0.022 24.184 0.000 #&gt; .x11 0.543 0.022 24.296 0.000 #&gt; .y11 0.522 0.022 23.746 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 0.067 0.011 6.294 0.000 #&gt; dif2 -0.009 0.011 -0.803 0.422 #&gt; dif3 -0.006 0.003 -2.275 0.023 3.6.7.2 RI-CLPM RICLPM &lt;- &#39; # Create between components (random intercepts) RIx =~ 1*x1 + 1*x2 + 1*x3 + 1*x4 + 1*x5 + 1*x6 + 1*x7 + 1*x8 + 1*x9 + 1*x10 + 1*x11 RIy =~ 1*y1 + 1*y2 + 1*y3 + 1*y4 + 1*y5 + 1*y6 + 1*y7 + 1*y8 + 1*y9 + 1*y10 + 1*y11 RIx ~ ex*oplx RIy ~ ey*oply # Create within-person centered variables wx1 =~ 1*x1 wx2 =~ 1*x2 wx3 =~ 1*x3 wx4 =~ 1*x4 wx5 =~ 1*x5 wx6 =~ 1*x6 wx7 =~ 1*x7 wx8 =~ 1*x8 wx9 =~ 1*x9 wx10 =~ 1*x10 wx11 =~ 1*x11 wy1 =~ 1*y1 wy2 =~ 1*y2 wy3 =~ 1*y3 wy4 =~ 1*y4 wy5 =~ 1*y5 wy6 =~ 1*y6 wy7 =~ 1*y7 wy8 =~ 1*y8 wy9 =~ 1*y9 wy10 =~ 1*y10 wy11 =~ 1*y11 # Estimate the lagged effects between the within-person centered variables. x2 ~ ax*x1 + bx*y1 x3 ~ ax*x2 + bx*y2 x4 ~ ax*x3 + bx*y3 x5 ~ ax*x4 + bx*y4 x6 ~ ax*x5 + bx*y5 x7 ~ ax*x6 + bx*y6 x8 ~ ax*x7 + bx*y7 x9 ~ ax*x8 + bx*y8 x10 ~ ax*x9 + bx*y9 x11 ~ ax*x10 + bx*y10 y2 ~ by*x1 + ay*y1 y3 ~ by*x2 + ay*y2 y4 ~ by*x3 + ay*y3 y5 ~ by*x4 + ay*y4 y6 ~ by*x5 + ay*y5 y7 ~ by*x6 + ay*y6 y8 ~ by*x7 + ay*y7 y9 ~ by*x8 + ay*y8 y10 ~ by*x9 + ay*y9 y11 ~ by*x10 + ay*y10 dif1 := ax - ay dif2 := bx - by dif3 := ex - ey # Estimate the (residual) covariance between the within-person centered variables wx1 ~~ wy1 # Covariance wx2 ~~ wy2 wx3 ~~ wy3 wx4 ~~ wy4 wx5 ~~ wy5 wx6 ~~ wy6 wx7 ~~ wy7 wx8 ~~ wy8 wx9 ~~ wy9 wx10 ~~ wy10 wx11 ~~ wy11 # Estimate the variance and covariance of the random intercepts. RIx ~~ RIx RIy ~~ RIy RIx ~~ RIy # Estimate the (residual) variance of the within-person centered variables. wx1 ~~ wx1 # Variances wy1 ~~ wy1 wx2 ~~ wx2 # Residual variances wy2 ~~ wy2 wx3 ~~ wx3 wy3 ~~ wy3 wx4 ~~ wx4 wy4 ~~ wy4 wx5 ~~ wx5 wy5 ~~ wy5 wx6 ~~ wx6 wy6 ~~ wy6 wx7 ~~ wx7 wy7 ~~ wy7 wx8 ~~ wx8 wy8 ~~ wy8 wx9 ~~ wx9 wy9 ~~ wy9 wx10 ~~ wx10 wy10 ~~ wy10 wx11 ~~ wx11 wy11 ~~ wy11 &#39; #Estimate models a bit faster: estimate &lt;- function(x) lavaan(RICLPM, data=x, missing = &quot;fiml.x&quot;, meanstructure = T ) library(future.apply) plan(multisession) results_temp &lt;- future_lapply(datalist_ori, estimate) summary(results_temp[[4]]) results[[21]] &lt;- results_temp[[1]] results[[22]] &lt;- results_temp[[2]] results[[23]] &lt;- results_temp[[3]] results[[24]] &lt;- results_temp[[4]] names(results)[21:24] &lt;- c(&quot;fitm2h2y1&quot;, &quot;fitm2h2y2&quot;,&quot;fitm2h2y3&quot;,&quot;fitm2h2y4&quot;) save(results, file=&quot;results.RData&quot;) load(&quot;addfiles/results.Rdata&quot;) summary(results[[21]]) summary(results[[22]]) summary(results[[23]]) summary(results[[24]]) #&gt; lavaan 0.6-9 ended normally after 32 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 78 #&gt; Number of equality constraints 36 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1415 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 1997.479 #&gt; Degrees of freedom 277 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (ex) 0.107 0.002 62.193 0.000 #&gt; RIy ~ #&gt; oply (ey) 0.108 0.002 66.277 0.000 #&gt; x2 ~ #&gt; x1 (ax) 0.131 0.010 13.320 0.000 #&gt; y1 (bx) 0.004 0.011 0.423 0.672 #&gt; x3 ~ #&gt; x2 (ax) 0.131 0.010 13.320 0.000 #&gt; y2 (bx) 0.004 0.011 0.423 0.672 #&gt; x4 ~ #&gt; x3 (ax) 0.131 0.010 13.320 0.000 #&gt; y3 (bx) 0.004 0.011 0.423 0.672 #&gt; x5 ~ #&gt; x4 (ax) 0.131 0.010 13.320 0.000 #&gt; y4 (bx) 0.004 0.011 0.423 0.672 #&gt; x6 ~ #&gt; x5 (ax) 0.131 0.010 13.320 0.000 #&gt; y5 (bx) 0.004 0.011 0.423 0.672 #&gt; x7 ~ #&gt; x6 (ax) 0.131 0.010 13.320 0.000 #&gt; y6 (bx) 0.004 0.011 0.423 0.672 #&gt; x8 ~ #&gt; x7 (ax) 0.131 0.010 13.320 0.000 #&gt; y7 (bx) 0.004 0.011 0.423 0.672 #&gt; x9 ~ #&gt; x8 (ax) 0.131 0.010 13.320 0.000 #&gt; y8 (bx) 0.004 0.011 0.423 0.672 #&gt; x10 ~ #&gt; x9 (ax) 0.131 0.010 13.320 0.000 #&gt; y9 (bx) 0.004 0.011 0.423 0.672 #&gt; x11 ~ #&gt; x10 (ax) 0.131 0.010 13.320 0.000 #&gt; y10 (bx) 0.004 0.011 0.423 0.672 #&gt; y2 ~ #&gt; x1 (by) 0.029 0.009 3.263 0.001 #&gt; y1 (ay) 0.100 0.010 9.719 0.000 #&gt; y3 ~ #&gt; x2 (by) 0.029 0.009 3.263 0.001 #&gt; y2 (ay) 0.100 0.010 9.719 0.000 #&gt; y4 ~ #&gt; x3 (by) 0.029 0.009 3.263 0.001 #&gt; y3 (ay) 0.100 0.010 9.719 0.000 #&gt; y5 ~ #&gt; x4 (by) 0.029 0.009 3.263 0.001 #&gt; y4 (ay) 0.100 0.010 9.719 0.000 #&gt; y6 ~ #&gt; x5 (by) 0.029 0.009 3.263 0.001 #&gt; y5 (ay) 0.100 0.010 9.719 0.000 #&gt; y7 ~ #&gt; x6 (by) 0.029 0.009 3.263 0.001 #&gt; y6 (ay) 0.100 0.010 9.719 0.000 #&gt; y8 ~ #&gt; x7 (by) 0.029 0.009 3.263 0.001 #&gt; y7 (ay) 0.100 0.010 9.719 0.000 #&gt; y9 ~ #&gt; x8 (by) 0.029 0.009 3.263 0.001 #&gt; y8 (ay) 0.100 0.010 9.719 0.000 #&gt; y10 ~ #&gt; x9 (by) 0.029 0.009 3.263 0.001 #&gt; y9 (ay) 0.100 0.010 9.719 0.000 #&gt; y11 ~ #&gt; x10 (by) 0.029 0.009 3.263 0.001 #&gt; y10 (ay) 0.100 0.010 9.719 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.208 0.026 8.085 0.000 #&gt; wx2 ~~ #&gt; wy2 0.109 0.020 5.432 0.000 #&gt; wx3 ~~ #&gt; wy3 0.136 0.019 7.010 0.000 #&gt; wx4 ~~ #&gt; wy4 0.092 0.018 5.055 0.000 #&gt; wx5 ~~ #&gt; wy5 0.087 0.021 4.158 0.000 #&gt; wx6 ~~ #&gt; wy6 0.057 0.016 3.567 0.000 #&gt; wx7 ~~ #&gt; wy7 0.094 0.016 5.990 0.000 #&gt; wx8 ~~ #&gt; wy8 0.074 0.018 4.163 0.000 #&gt; wx9 ~~ #&gt; wy9 0.096 0.019 5.030 0.000 #&gt; wx10 ~~ #&gt; wy10 0.032 0.020 1.629 0.103 #&gt; wx11 ~~ #&gt; wy11 0.101 0.021 4.810 0.000 #&gt; .RIx ~~ #&gt; .RIy 0.285 0.016 18.369 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; wx2 0.000 #&gt; wx3 0.000 #&gt; wx4 0.000 #&gt; wx5 0.000 #&gt; wx6 0.000 #&gt; wx7 0.000 #&gt; wx8 0.000 #&gt; wx9 0.000 #&gt; wx10 0.000 #&gt; wx11 0.000 #&gt; wy1 0.000 #&gt; wy2 0.000 #&gt; wy3 0.000 #&gt; wy4 0.000 #&gt; wy5 0.000 #&gt; wy6 0.000 #&gt; wy7 0.000 #&gt; wy8 0.000 #&gt; wy9 0.000 #&gt; wy10 0.000 #&gt; wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 0.610 0.024 25.889 0.000 #&gt; .RIy 0.411 0.017 23.879 0.000 #&gt; wx1 0.831 0.036 23.057 0.000 #&gt; wy1 0.706 0.031 22.464 0.000 #&gt; wx2 0.702 0.029 23.981 0.000 #&gt; wy2 0.567 0.024 23.148 0.000 #&gt; wx3 0.623 0.026 23.779 0.000 #&gt; wy3 0.542 0.024 22.994 0.000 #&gt; wx4 0.577 0.026 22.451 0.000 #&gt; wy4 0.463 0.021 21.622 0.000 #&gt; wx5 0.732 0.030 24.014 0.000 #&gt; wy5 0.546 0.024 23.234 0.000 #&gt; wx6 0.494 0.022 22.803 0.000 #&gt; wy6 0.410 0.019 21.986 0.000 #&gt; wx7 0.476 0.021 22.625 0.000 #&gt; wy7 0.438 0.020 22.399 0.000 #&gt; wx8 0.529 0.023 22.915 0.000 #&gt; wy8 0.490 0.022 22.687 0.000 #&gt; wx9 0.600 0.026 22.936 0.000 #&gt; wy9 0.493 0.022 21.983 0.000 #&gt; wx10 0.545 0.025 21.594 0.000 #&gt; wy10 0.490 0.023 21.146 0.000 #&gt; wx11 0.604 0.028 21.355 0.000 #&gt; wy11 0.523 0.026 20.465 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 0.031 0.014 2.157 0.031 #&gt; dif2 -0.025 0.013 -1.817 0.069 #&gt; dif3 -0.000 0.002 -0.137 0.891 #&gt; #&gt; lavaan 0.6-9 ended normally after 41 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 78 #&gt; Number of equality constraints 36 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1230 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 802.295 #&gt; Degrees of freedom 277 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (ex) 0.092 0.001 67.348 0.000 #&gt; RIy ~ #&gt; oply (ey) 0.102 0.001 74.487 0.000 #&gt; x2 ~ #&gt; x1 (ax) 0.064 0.009 7.024 0.000 #&gt; y1 (bx) 0.027 0.009 2.946 0.003 #&gt; x3 ~ #&gt; x2 (ax) 0.064 0.009 7.024 0.000 #&gt; y2 (bx) 0.027 0.009 2.946 0.003 #&gt; x4 ~ #&gt; x3 (ax) 0.064 0.009 7.024 0.000 #&gt; y3 (bx) 0.027 0.009 2.946 0.003 #&gt; x5 ~ #&gt; x4 (ax) 0.064 0.009 7.024 0.000 #&gt; y4 (bx) 0.027 0.009 2.946 0.003 #&gt; x6 ~ #&gt; x5 (ax) 0.064 0.009 7.024 0.000 #&gt; y5 (bx) 0.027 0.009 2.946 0.003 #&gt; x7 ~ #&gt; x6 (ax) 0.064 0.009 7.024 0.000 #&gt; y6 (bx) 0.027 0.009 2.946 0.003 #&gt; x8 ~ #&gt; x7 (ax) 0.064 0.009 7.024 0.000 #&gt; y7 (bx) 0.027 0.009 2.946 0.003 #&gt; x9 ~ #&gt; x8 (ax) 0.064 0.009 7.024 0.000 #&gt; y8 (bx) 0.027 0.009 2.946 0.003 #&gt; x10 ~ #&gt; x9 (ax) 0.064 0.009 7.024 0.000 #&gt; y9 (bx) 0.027 0.009 2.946 0.003 #&gt; x11 ~ #&gt; x10 (ax) 0.064 0.009 7.024 0.000 #&gt; y10 (bx) 0.027 0.009 2.946 0.003 #&gt; y2 ~ #&gt; x1 (by) 0.010 0.009 1.125 0.261 #&gt; y1 (ay) 0.065 0.009 7.188 0.000 #&gt; y3 ~ #&gt; x2 (by) 0.010 0.009 1.125 0.261 #&gt; y2 (ay) 0.065 0.009 7.188 0.000 #&gt; y4 ~ #&gt; x3 (by) 0.010 0.009 1.125 0.261 #&gt; y3 (ay) 0.065 0.009 7.188 0.000 #&gt; y5 ~ #&gt; x4 (by) 0.010 0.009 1.125 0.261 #&gt; y4 (ay) 0.065 0.009 7.188 0.000 #&gt; y6 ~ #&gt; x5 (by) 0.010 0.009 1.125 0.261 #&gt; y5 (ay) 0.065 0.009 7.188 0.000 #&gt; y7 ~ #&gt; x6 (by) 0.010 0.009 1.125 0.261 #&gt; y6 (ay) 0.065 0.009 7.188 0.000 #&gt; y8 ~ #&gt; x7 (by) 0.010 0.009 1.125 0.261 #&gt; y7 (ay) 0.065 0.009 7.188 0.000 #&gt; y9 ~ #&gt; x8 (by) 0.010 0.009 1.125 0.261 #&gt; y8 (ay) 0.065 0.009 7.188 0.000 #&gt; y10 ~ #&gt; x9 (by) 0.010 0.009 1.125 0.261 #&gt; y9 (ay) 0.065 0.009 7.188 0.000 #&gt; y11 ~ #&gt; x10 (by) 0.010 0.009 1.125 0.261 #&gt; y10 (ay) 0.065 0.009 7.188 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.063 0.013 4.778 0.000 #&gt; wx2 ~~ #&gt; wy2 0.045 0.013 3.552 0.000 #&gt; wx3 ~~ #&gt; wy3 0.027 0.011 2.439 0.015 #&gt; wx4 ~~ #&gt; wy4 0.043 0.011 3.995 0.000 #&gt; wx5 ~~ #&gt; wy5 0.043 0.011 3.939 0.000 #&gt; wx6 ~~ #&gt; wy6 0.023 0.011 2.175 0.030 #&gt; wx7 ~~ #&gt; wy7 0.032 0.011 2.881 0.004 #&gt; wx8 ~~ #&gt; wy8 0.029 0.011 2.593 0.010 #&gt; wx9 ~~ #&gt; wy9 0.012 0.013 0.971 0.331 #&gt; wx10 ~~ #&gt; wy10 0.043 0.013 3.403 0.001 #&gt; wx11 ~~ #&gt; wy11 0.063 0.013 4.896 0.000 #&gt; .RIx ~~ #&gt; .RIy 0.266 0.013 21.096 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; wx2 0.000 #&gt; wx3 0.000 #&gt; wx4 0.000 #&gt; wx5 0.000 #&gt; wx6 0.000 #&gt; wx7 0.000 #&gt; wx8 0.000 #&gt; wx9 0.000 #&gt; wx10 0.000 #&gt; wx11 0.000 #&gt; wy1 0.000 #&gt; wy2 0.000 #&gt; wy3 0.000 #&gt; wy4 0.000 #&gt; wy5 0.000 #&gt; wy6 0.000 #&gt; wy7 0.000 #&gt; wy8 0.000 #&gt; wy9 0.000 #&gt; wy10 0.000 #&gt; wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 0.467 0.017 27.763 0.000 #&gt; .RIy 0.442 0.016 28.012 0.000 #&gt; wx1 0.462 0.019 24.034 0.000 #&gt; wy1 0.389 0.017 23.503 0.000 #&gt; wx2 0.402 0.017 23.994 0.000 #&gt; wy2 0.424 0.017 24.485 0.000 #&gt; wx3 0.346 0.015 23.757 0.000 #&gt; wy3 0.313 0.013 23.292 0.000 #&gt; wx4 0.323 0.014 22.711 0.000 #&gt; wy4 0.322 0.014 22.565 0.000 #&gt; wx5 0.326 0.014 23.605 0.000 #&gt; wy5 0.332 0.014 23.560 0.000 #&gt; wx6 0.348 0.015 23.638 0.000 #&gt; wy6 0.304 0.013 23.253 0.000 #&gt; wx7 0.349 0.015 23.491 0.000 #&gt; wy7 0.328 0.014 23.564 0.000 #&gt; wx8 0.322 0.014 22.979 0.000 #&gt; wy8 0.327 0.014 23.170 0.000 #&gt; wx9 0.370 0.016 23.022 0.000 #&gt; wy9 0.373 0.016 22.741 0.000 #&gt; wx10 0.369 0.017 22.106 0.000 #&gt; wy10 0.338 0.016 21.599 0.000 #&gt; wx11 0.365 0.017 21.431 0.000 #&gt; wy11 0.328 0.016 20.622 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 -0.001 0.013 -0.050 0.960 #&gt; dif2 0.017 0.012 1.376 0.169 #&gt; dif3 -0.010 0.002 -6.291 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 65 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 78 #&gt; Number of equality constraints 36 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1290 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 5819.488 #&gt; Degrees of freedom 277 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (ex) 0.321 0.002 150.244 0.000 #&gt; RIy ~ #&gt; oply (ey) 0.334 0.002 148.908 0.000 #&gt; x2 ~ #&gt; x1 (ax) -0.029 0.008 -3.764 0.000 #&gt; y1 (bx) 0.062 0.008 7.961 0.000 #&gt; x3 ~ #&gt; x2 (ax) -0.029 0.008 -3.764 0.000 #&gt; y2 (bx) 0.062 0.008 7.961 0.000 #&gt; x4 ~ #&gt; x3 (ax) -0.029 0.008 -3.764 0.000 #&gt; y3 (bx) 0.062 0.008 7.961 0.000 #&gt; x5 ~ #&gt; x4 (ax) -0.029 0.008 -3.764 0.000 #&gt; y4 (bx) 0.062 0.008 7.961 0.000 #&gt; x6 ~ #&gt; x5 (ax) -0.029 0.008 -3.764 0.000 #&gt; y5 (bx) 0.062 0.008 7.961 0.000 #&gt; x7 ~ #&gt; x6 (ax) -0.029 0.008 -3.764 0.000 #&gt; y6 (bx) 0.062 0.008 7.961 0.000 #&gt; x8 ~ #&gt; x7 (ax) -0.029 0.008 -3.764 0.000 #&gt; y7 (bx) 0.062 0.008 7.961 0.000 #&gt; x9 ~ #&gt; x8 (ax) -0.029 0.008 -3.764 0.000 #&gt; y8 (bx) 0.062 0.008 7.961 0.000 #&gt; x10 ~ #&gt; x9 (ax) -0.029 0.008 -3.764 0.000 #&gt; y9 (bx) 0.062 0.008 7.961 0.000 #&gt; x11 ~ #&gt; x10 (ax) -0.029 0.008 -3.764 0.000 #&gt; y10 (bx) 0.062 0.008 7.961 0.000 #&gt; y2 ~ #&gt; x1 (by) 0.058 0.008 7.698 0.000 #&gt; y1 (ay) -0.023 0.008 -3.037 0.002 #&gt; y3 ~ #&gt; x2 (by) 0.058 0.008 7.698 0.000 #&gt; y2 (ay) -0.023 0.008 -3.037 0.002 #&gt; y4 ~ #&gt; x3 (by) 0.058 0.008 7.698 0.000 #&gt; y3 (ay) -0.023 0.008 -3.037 0.002 #&gt; y5 ~ #&gt; x4 (by) 0.058 0.008 7.698 0.000 #&gt; y4 (ay) -0.023 0.008 -3.037 0.002 #&gt; y6 ~ #&gt; x5 (by) 0.058 0.008 7.698 0.000 #&gt; y5 (ay) -0.023 0.008 -3.037 0.002 #&gt; y7 ~ #&gt; x6 (by) 0.058 0.008 7.698 0.000 #&gt; y6 (ay) -0.023 0.008 -3.037 0.002 #&gt; y8 ~ #&gt; x7 (by) 0.058 0.008 7.698 0.000 #&gt; y7 (ay) -0.023 0.008 -3.037 0.002 #&gt; y9 ~ #&gt; x8 (by) 0.058 0.008 7.698 0.000 #&gt; y8 (ay) -0.023 0.008 -3.037 0.002 #&gt; y10 ~ #&gt; x9 (by) 0.058 0.008 7.698 0.000 #&gt; y9 (ay) -0.023 0.008 -3.037 0.002 #&gt; y11 ~ #&gt; x10 (by) 0.058 0.008 7.698 0.000 #&gt; y10 (ay) -0.023 0.008 -3.037 0.002 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.076 0.011 7.020 0.000 #&gt; wx2 ~~ #&gt; wy2 0.040 0.009 4.729 0.000 #&gt; wx3 ~~ #&gt; wy3 0.071 0.009 7.893 0.000 #&gt; wx4 ~~ #&gt; wy4 0.031 0.008 3.893 0.000 #&gt; wx5 ~~ #&gt; wy5 0.038 0.007 5.179 0.000 #&gt; wx6 ~~ #&gt; wy6 0.036 0.007 5.318 0.000 #&gt; wx7 ~~ #&gt; wy7 0.033 0.007 4.761 0.000 #&gt; wx8 ~~ #&gt; wy8 0.024 0.007 3.179 0.001 #&gt; wx9 ~~ #&gt; wy9 0.033 0.008 4.099 0.000 #&gt; wx10 ~~ #&gt; wy10 0.061 0.010 6.083 0.000 #&gt; wx11 ~~ #&gt; wy11 0.067 0.010 7.022 0.000 #&gt; .RIx ~~ #&gt; .RIy 0.998 0.037 26.915 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; wx2 0.000 #&gt; wx3 0.000 #&gt; wx4 0.000 #&gt; wx5 0.000 #&gt; wx6 0.000 #&gt; wx7 0.000 #&gt; wx8 0.000 #&gt; wx9 0.000 #&gt; wx10 0.000 #&gt; wx11 0.000 #&gt; wy1 0.000 #&gt; wy2 0.000 #&gt; wy3 0.000 #&gt; wy4 0.000 #&gt; wy5 0.000 #&gt; wy6 0.000 #&gt; wy7 0.000 #&gt; wy8 0.000 #&gt; wy9 0.000 #&gt; wy10 0.000 #&gt; wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 1.565 0.043 36.053 0.000 #&gt; .RIy 1.627 0.045 36.277 0.000 #&gt; wx1 0.321 0.015 22.134 0.000 #&gt; wy1 0.327 0.014 22.765 0.000 #&gt; wx2 0.281 0.012 22.970 0.000 #&gt; wy2 0.237 0.011 22.474 0.000 #&gt; wx3 0.264 0.011 23.577 0.000 #&gt; wy3 0.302 0.013 23.913 0.000 #&gt; wx4 0.216 0.010 22.195 0.000 #&gt; wy4 0.238 0.011 22.435 0.000 #&gt; wx5 0.241 0.010 23.106 0.000 #&gt; wy5 0.194 0.009 22.701 0.000 #&gt; wx6 0.208 0.009 22.382 0.000 #&gt; wy6 0.177 0.008 21.961 0.000 #&gt; wx7 0.203 0.009 22.572 0.000 #&gt; wy7 0.198 0.009 22.483 0.000 #&gt; wx8 0.210 0.009 22.162 0.000 #&gt; wy8 0.219 0.010 22.772 0.000 #&gt; wx9 0.241 0.011 22.239 0.000 #&gt; wy9 0.209 0.009 22.067 0.000 #&gt; wx10 0.269 0.012 21.724 0.000 #&gt; wy10 0.260 0.012 21.617 0.000 #&gt; wx11 0.262 0.013 20.850 0.000 #&gt; wy11 0.243 0.012 20.395 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 -0.006 0.012 -0.528 0.597 #&gt; dif2 0.004 0.012 0.300 0.764 #&gt; dif3 -0.013 0.002 -6.125 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 45 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 78 #&gt; Number of equality constraints 36 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1309 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 6172.967 #&gt; Degrees of freedom 277 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (ex) 0.263 0.002 119.615 0.000 #&gt; RIy ~ #&gt; oply (ey) 0.286 0.002 129.685 0.000 #&gt; x2 ~ #&gt; x1 (ax) 0.016 0.008 2.021 0.043 #&gt; y1 (bx) 0.040 0.008 5.117 0.000 #&gt; x3 ~ #&gt; x2 (ax) 0.016 0.008 2.021 0.043 #&gt; y2 (bx) 0.040 0.008 5.117 0.000 #&gt; x4 ~ #&gt; x3 (ax) 0.016 0.008 2.021 0.043 #&gt; y3 (bx) 0.040 0.008 5.117 0.000 #&gt; x5 ~ #&gt; x4 (ax) 0.016 0.008 2.021 0.043 #&gt; y4 (bx) 0.040 0.008 5.117 0.000 #&gt; x6 ~ #&gt; x5 (ax) 0.016 0.008 2.021 0.043 #&gt; y5 (bx) 0.040 0.008 5.117 0.000 #&gt; x7 ~ #&gt; x6 (ax) 0.016 0.008 2.021 0.043 #&gt; y6 (bx) 0.040 0.008 5.117 0.000 #&gt; x8 ~ #&gt; x7 (ax) 0.016 0.008 2.021 0.043 #&gt; y7 (bx) 0.040 0.008 5.117 0.000 #&gt; x9 ~ #&gt; x8 (ax) 0.016 0.008 2.021 0.043 #&gt; y8 (bx) 0.040 0.008 5.117 0.000 #&gt; x10 ~ #&gt; x9 (ax) 0.016 0.008 2.021 0.043 #&gt; y9 (bx) 0.040 0.008 5.117 0.000 #&gt; x11 ~ #&gt; x10 (ax) 0.016 0.008 2.021 0.043 #&gt; y10 (bx) 0.040 0.008 5.117 0.000 #&gt; y2 ~ #&gt; x1 (by) 0.049 0.008 6.169 0.000 #&gt; y1 (ay) -0.003 0.008 -0.404 0.686 #&gt; y3 ~ #&gt; x2 (by) 0.049 0.008 6.169 0.000 #&gt; y2 (ay) -0.003 0.008 -0.404 0.686 #&gt; y4 ~ #&gt; x3 (by) 0.049 0.008 6.169 0.000 #&gt; y3 (ay) -0.003 0.008 -0.404 0.686 #&gt; y5 ~ #&gt; x4 (by) 0.049 0.008 6.169 0.000 #&gt; y4 (ay) -0.003 0.008 -0.404 0.686 #&gt; y6 ~ #&gt; x5 (by) 0.049 0.008 6.169 0.000 #&gt; y5 (ay) -0.003 0.008 -0.404 0.686 #&gt; y7 ~ #&gt; x6 (by) 0.049 0.008 6.169 0.000 #&gt; y6 (ay) -0.003 0.008 -0.404 0.686 #&gt; y8 ~ #&gt; x7 (by) 0.049 0.008 6.169 0.000 #&gt; y7 (ay) -0.003 0.008 -0.404 0.686 #&gt; y9 ~ #&gt; x8 (by) 0.049 0.008 6.169 0.000 #&gt; y8 (ay) -0.003 0.008 -0.404 0.686 #&gt; y10 ~ #&gt; x9 (by) 0.049 0.008 6.169 0.000 #&gt; y9 (ay) -0.003 0.008 -0.404 0.686 #&gt; y11 ~ #&gt; x10 (by) 0.049 0.008 6.169 0.000 #&gt; y10 (ay) -0.003 0.008 -0.404 0.686 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.133 0.017 7.847 0.000 #&gt; wx2 ~~ #&gt; wy2 0.041 0.013 3.141 0.002 #&gt; wx3 ~~ #&gt; wy3 0.064 0.013 5.012 0.000 #&gt; wx4 ~~ #&gt; wy4 0.035 0.012 2.818 0.005 #&gt; wx5 ~~ #&gt; wy5 0.018 0.012 1.514 0.130 #&gt; wx6 ~~ #&gt; wy6 0.058 0.012 4.811 0.000 #&gt; wx7 ~~ #&gt; wy7 0.052 0.012 4.514 0.000 #&gt; wx8 ~~ #&gt; wy8 0.044 0.013 3.515 0.000 #&gt; wx9 ~~ #&gt; wy9 0.035 0.013 2.638 0.008 #&gt; wx10 ~~ #&gt; wy10 0.048 0.014 3.412 0.001 #&gt; wx11 ~~ #&gt; wy11 0.053 0.016 3.301 0.001 #&gt; .RIx ~~ #&gt; .RIy 0.910 0.034 26.466 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; wx2 0.000 #&gt; wx3 0.000 #&gt; wx4 0.000 #&gt; wx5 0.000 #&gt; wx6 0.000 #&gt; wx7 0.000 #&gt; wx8 0.000 #&gt; wx9 0.000 #&gt; wx10 0.000 #&gt; wx11 0.000 #&gt; wy1 0.000 #&gt; wy2 0.000 #&gt; wy3 0.000 #&gt; wy4 0.000 #&gt; wy5 0.000 #&gt; wy6 0.000 #&gt; wy7 0.000 #&gt; wy8 0.000 #&gt; wy9 0.000 #&gt; wy10 0.000 #&gt; wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 1.475 0.043 34.155 0.000 #&gt; .RIy 1.355 0.040 34.092 0.000 #&gt; wx1 0.515 0.023 22.469 0.000 #&gt; wy1 0.499 0.022 22.710 0.000 #&gt; wx2 0.398 0.017 22.953 0.000 #&gt; wy2 0.394 0.017 22.918 0.000 #&gt; wx3 0.392 0.017 23.514 0.000 #&gt; wy3 0.408 0.017 23.379 0.000 #&gt; wx4 0.369 0.017 22.294 0.000 #&gt; wy4 0.320 0.015 21.716 0.000 #&gt; wx5 0.342 0.015 22.862 0.000 #&gt; wy5 0.349 0.015 22.930 0.000 #&gt; wx6 0.366 0.016 23.094 0.000 #&gt; wy6 0.351 0.015 22.843 0.000 #&gt; wx7 0.364 0.016 22.956 0.000 #&gt; wy7 0.305 0.014 22.303 0.000 #&gt; wx8 0.349 0.015 22.667 0.000 #&gt; wy8 0.383 0.017 22.891 0.000 #&gt; wx9 0.380 0.017 22.382 0.000 #&gt; wy9 0.383 0.017 22.293 0.000 #&gt; wx10 0.371 0.017 21.401 0.000 #&gt; wy10 0.388 0.018 21.242 0.000 #&gt; wx11 0.462 0.021 21.518 0.000 #&gt; wy11 0.383 0.019 20.177 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 0.019 0.012 1.649 0.099 #&gt; dif2 -0.009 0.012 -0.781 0.435 #&gt; dif3 -0.024 0.002 -10.618 0.000 3.6.7.3 SC-RI-CLPM SCCLPM &lt;- &#39; # Create between components (random intercepts) RIx =~ 1*x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 + x10 + x11 RIy =~ 1*y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 + y9 + y10 + y11 RIx ~ ex*oplx RIy ~ ey*oply # Create within-person centered variables wx1 =~ 1*x1 wx2 =~ 1*x2 wx3 =~ 1*x3 wx4 =~ 1*x4 wx5 =~ 1*x5 wx6 =~ 1*x6 wx7 =~ 1*x7 wx8 =~ 1*x8 wx9 =~ 1*x9 wx10 =~ 1*x10 wx11 =~ 1*x11 wy1 =~ 1*y1 wy2 =~ 1*y2 wy3 =~ 1*y3 wy4 =~ 1*y4 wy5 =~ 1*y5 wy6 =~ 1*y6 wy7 =~ 1*y7 wy8 =~ 1*y8 wy9 =~ 1*y9 wy10 =~ 1*y10 wy11 =~ 1*y11 # Estimate the lagged effects between the within-person centered variables. x2 ~ ax*x1 + bx*y1 x3 ~ ax*x2 + bx*y2 x4 ~ ax*x3 + bx*y3 x5 ~ ax*x4 + bx*y4 x6 ~ ax*x5 + bx*y5 x7 ~ ax*x6 + bx*y6 x8 ~ ax*x7 + bx*y7 x9 ~ ax*x8 + bx*y8 x10 ~ ax*x9 + bx*y9 x11 ~ ax*x10 + bx*y10 y2 ~ by*x1 + ay*y1 y3 ~ by*x2 + ay*y2 y4 ~ by*x3 + ay*y3 y5 ~ by*x4 + ay*y4 y6 ~ by*x5 + ay*y5 y7 ~ by*x6 + ay*y6 y8 ~ by*x7 + ay*y7 y9 ~ by*x8 + ay*y8 y10 ~ by*x9 + ay*y9 y11 ~ by*x10 + ay*y10 dif1 := ax - ay dif2 := bx - by dif3 := ex - ey # Estimate the (residual) covariance between the within-person centered variables wx1 ~~ wy1 # Covariance wx2 ~~ wy2 wx3 ~~ wy3 wx4 ~~ wy4 wx5 ~~ wy5 wx6 ~~ wy6 wx7 ~~ wy7 wx8 ~~ wy8 wx9 ~~ wy9 wx10 ~~ wy10 wx11 ~~ wy11 # Estimate the variance and covariance of the random intercepts. RIx ~~ RIx RIy ~~ RIy RIx ~~ RIy # Estimate the (residual) variance of the within-person centered variables. wx1 ~~ wx1 # Variances wy1 ~~ wy1 wx2 ~~ wx2 # Residual variances wy2 ~~ wy2 wx3 ~~ wx3 wy3 ~~ wy3 wx4 ~~ wx4 wy4 ~~ wy4 wx5 ~~ wx5 wy5 ~~ wy5 wx6 ~~ wx6 wy6 ~~ wy6 wx7 ~~ wx7 wy7 ~~ wy7 wx8 ~~ wx8 wy8 ~~ wy8 wx9 ~~ wx9 wy9 ~~ wy9 wx10 ~~ wx10 wy10 ~~ wy10 wx11 ~~ wx11 wy11 ~~ wy11 &#39; #Estimate models a bit faster: estimate &lt;- function(x) lavaan(SCCLPM, data=x, missing = &quot;fiml.x&quot;, meanstructure = T ) library(future.apply) plan(multisession) results_temp &lt;- future_lapply(datalist_ori, estimate) summary(results_temp[[4]]) results[[25]] &lt;- results_temp[[1]] results[[26]] &lt;- results_temp[[2]] results[[27]] &lt;- results_temp[[3]] results[[28]] &lt;- results_temp[[4]] names(results)[25:28] &lt;- c(&quot;fitm3h2y1&quot;, &quot;fitm3h2y2&quot;,&quot;fitm3h2y3&quot;,&quot;fitm3h2y4&quot;) save(results, file=&quot;results.RData&quot;) load(&quot;addfiles/results.Rdata&quot;) summary(results[[25]]) summary(results[[26]]) summary(results[[27]]) summary(results[[28]]) #&gt; lavaan 0.6-9 ended normally after 51 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 98 #&gt; Number of equality constraints 36 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1415 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 1144.862 #&gt; Degrees of freedom 257 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 0.763 0.021 36.188 0.000 #&gt; x3 0.767 0.021 37.058 0.000 #&gt; x4 0.637 0.021 30.424 0.000 #&gt; x5 0.782 0.021 37.498 0.000 #&gt; x6 0.666 0.020 33.092 0.000 #&gt; x7 0.626 0.019 33.115 0.000 #&gt; x8 0.685 0.019 35.811 0.000 #&gt; x9 0.626 0.020 32.096 0.000 #&gt; x10 0.709 0.020 35.892 0.000 #&gt; x11 0.741 0.021 35.665 0.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 0.775 0.022 35.802 0.000 #&gt; y3 0.776 0.021 36.197 0.000 #&gt; y4 0.625 0.022 28.846 0.000 #&gt; y5 0.672 0.021 32.760 0.000 #&gt; y6 0.652 0.020 32.553 0.000 #&gt; y7 0.617 0.019 31.933 0.000 #&gt; y8 0.661 0.019 34.008 0.000 #&gt; y9 0.657 0.020 32.846 0.000 #&gt; y10 0.735 0.020 36.116 0.000 #&gt; y11 0.762 0.022 35.406 0.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (ex) 0.126 0.002 61.856 0.000 #&gt; RIy ~ #&gt; oply (ey) 0.128 0.002 65.198 0.000 #&gt; x2 ~ #&gt; x1 (ax) 0.218 0.012 18.700 0.000 #&gt; y1 (bx) 0.064 0.011 5.987 0.000 #&gt; x3 ~ #&gt; x2 (ax) 0.218 0.012 18.700 0.000 #&gt; y2 (bx) 0.064 0.011 5.987 0.000 #&gt; x4 ~ #&gt; x3 (ax) 0.218 0.012 18.700 0.000 #&gt; y3 (bx) 0.064 0.011 5.987 0.000 #&gt; x5 ~ #&gt; x4 (ax) 0.218 0.012 18.700 0.000 #&gt; y4 (bx) 0.064 0.011 5.987 0.000 #&gt; x6 ~ #&gt; x5 (ax) 0.218 0.012 18.700 0.000 #&gt; y5 (bx) 0.064 0.011 5.987 0.000 #&gt; x7 ~ #&gt; x6 (ax) 0.218 0.012 18.700 0.000 #&gt; y6 (bx) 0.064 0.011 5.987 0.000 #&gt; x8 ~ #&gt; x7 (ax) 0.218 0.012 18.700 0.000 #&gt; y7 (bx) 0.064 0.011 5.987 0.000 #&gt; x9 ~ #&gt; x8 (ax) 0.218 0.012 18.700 0.000 #&gt; y8 (bx) 0.064 0.011 5.987 0.000 #&gt; x10 ~ #&gt; x9 (ax) 0.218 0.012 18.700 0.000 #&gt; y9 (bx) 0.064 0.011 5.987 0.000 #&gt; x11 ~ #&gt; x10 (ax) 0.218 0.012 18.700 0.000 #&gt; y10 (bx) 0.064 0.011 5.987 0.000 #&gt; y2 ~ #&gt; x1 (by) 0.078 0.009 8.823 0.000 #&gt; y1 (ay) 0.190 0.012 15.440 0.000 #&gt; y3 ~ #&gt; x2 (by) 0.078 0.009 8.823 0.000 #&gt; y2 (ay) 0.190 0.012 15.440 0.000 #&gt; y4 ~ #&gt; x3 (by) 0.078 0.009 8.823 0.000 #&gt; y3 (ay) 0.190 0.012 15.440 0.000 #&gt; y5 ~ #&gt; x4 (by) 0.078 0.009 8.823 0.000 #&gt; y4 (ay) 0.190 0.012 15.440 0.000 #&gt; y6 ~ #&gt; x5 (by) 0.078 0.009 8.823 0.000 #&gt; y5 (ay) 0.190 0.012 15.440 0.000 #&gt; y7 ~ #&gt; x6 (by) 0.078 0.009 8.823 0.000 #&gt; y6 (ay) 0.190 0.012 15.440 0.000 #&gt; y8 ~ #&gt; x7 (by) 0.078 0.009 8.823 0.000 #&gt; y7 (ay) 0.190 0.012 15.440 0.000 #&gt; y9 ~ #&gt; x8 (by) 0.078 0.009 8.823 0.000 #&gt; y8 (ay) 0.190 0.012 15.440 0.000 #&gt; y10 ~ #&gt; x9 (by) 0.078 0.009 8.823 0.000 #&gt; y9 (ay) 0.190 0.012 15.440 0.000 #&gt; y11 ~ #&gt; x10 (by) 0.078 0.009 8.823 0.000 #&gt; y10 (ay) 0.190 0.012 15.440 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.124 0.022 5.562 0.000 #&gt; wx2 ~~ #&gt; wy2 0.088 0.020 4.470 0.000 #&gt; wx3 ~~ #&gt; wy3 0.088 0.018 4.835 0.000 #&gt; wx4 ~~ #&gt; wy4 0.099 0.018 5.406 0.000 #&gt; wx5 ~~ #&gt; wy5 0.103 0.021 4.818 0.000 #&gt; wx6 ~~ #&gt; wy6 0.069 0.017 4.166 0.000 #&gt; wx7 ~~ #&gt; wy7 0.086 0.015 5.582 0.000 #&gt; wx8 ~~ #&gt; wy8 0.080 0.018 4.432 0.000 #&gt; wx9 ~~ #&gt; wy9 0.096 0.019 5.013 0.000 #&gt; wx10 ~~ #&gt; wy10 0.038 0.020 1.888 0.059 #&gt; wx11 ~~ #&gt; wy11 0.086 0.021 4.206 0.000 #&gt; .RIx ~~ #&gt; .RIy 0.327 0.020 16.012 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; wx2 0.000 #&gt; wx3 0.000 #&gt; wx4 0.000 #&gt; wx5 0.000 #&gt; wx6 0.000 #&gt; wx7 0.000 #&gt; wx8 0.000 #&gt; wx9 0.000 #&gt; wx10 0.000 #&gt; wx11 0.000 #&gt; wy1 0.000 #&gt; wy2 0.000 #&gt; wy3 0.000 #&gt; wy4 0.000 #&gt; wy5 0.000 #&gt; wy6 0.000 #&gt; wy7 0.000 #&gt; wy8 0.000 #&gt; wy9 0.000 #&gt; wy10 0.000 #&gt; wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 0.834 0.034 24.576 0.000 #&gt; .RIy 0.547 0.024 22.904 0.000 #&gt; wx1 0.689 0.032 21.686 0.000 #&gt; wy1 0.587 0.028 21.267 0.000 #&gt; wx2 0.684 0.029 23.634 0.000 #&gt; wy2 0.553 0.024 22.641 0.000 #&gt; wx3 0.577 0.025 23.050 0.000 #&gt; wy3 0.491 0.022 22.141 0.000 #&gt; wx4 0.613 0.027 23.124 0.000 #&gt; wy4 0.491 0.022 22.394 0.000 #&gt; wx5 0.714 0.031 23.172 0.000 #&gt; wy5 0.572 0.024 23.422 0.000 #&gt; wx6 0.525 0.023 23.217 0.000 #&gt; wy6 0.439 0.020 22.490 0.000 #&gt; wx7 0.493 0.021 23.425 0.000 #&gt; wy7 0.442 0.019 23.077 0.000 #&gt; wx8 0.555 0.024 23.101 0.000 #&gt; wy8 0.507 0.022 22.998 0.000 #&gt; wx9 0.610 0.026 23.509 0.000 #&gt; wy9 0.516 0.023 22.383 0.000 #&gt; wx10 0.567 0.026 21.556 0.000 #&gt; wy10 0.500 0.024 20.918 0.000 #&gt; wx11 0.599 0.028 21.091 0.000 #&gt; wy11 0.496 0.025 20.045 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 0.028 0.017 1.649 0.099 #&gt; dif2 -0.014 0.013 -1.063 0.288 #&gt; dif3 -0.002 0.002 -0.695 0.487 #&gt; #&gt; lavaan 0.6-9 ended normally after 50 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 98 #&gt; Number of equality constraints 36 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1230 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 689.156 #&gt; Degrees of freedom 257 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 0.883 0.021 41.305 0.000 #&gt; x3 0.889 0.021 41.388 0.000 #&gt; x4 0.890 0.022 40.779 0.000 #&gt; x5 0.900 0.022 41.363 0.000 #&gt; x6 0.883 0.022 39.901 0.000 #&gt; x7 0.902 0.022 40.899 0.000 #&gt; x8 0.858 0.022 39.387 0.000 #&gt; x9 0.839 0.022 38.446 0.000 #&gt; x10 0.906 0.022 40.350 0.000 #&gt; x11 0.873 0.023 38.683 0.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 0.875 0.021 42.424 0.000 #&gt; y3 0.902 0.020 44.815 0.000 #&gt; y4 0.897 0.021 42.970 0.000 #&gt; y5 0.902 0.021 43.112 0.000 #&gt; y6 0.894 0.021 43.056 0.000 #&gt; y7 0.895 0.021 43.037 0.000 #&gt; y8 0.858 0.021 41.292 0.000 #&gt; y9 0.852 0.021 40.471 0.000 #&gt; y10 0.886 0.021 42.419 0.000 #&gt; y11 0.889 0.021 41.479 0.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (ex) 0.096 0.002 61.519 0.000 #&gt; RIy ~ #&gt; oply (ey) 0.106 0.002 68.404 0.000 #&gt; x2 ~ #&gt; x1 (ax) 0.100 0.011 8.975 0.000 #&gt; y1 (bx) 0.053 0.010 5.351 0.000 #&gt; x3 ~ #&gt; x2 (ax) 0.100 0.011 8.975 0.000 #&gt; y2 (bx) 0.053 0.010 5.351 0.000 #&gt; x4 ~ #&gt; x3 (ax) 0.100 0.011 8.975 0.000 #&gt; y3 (bx) 0.053 0.010 5.351 0.000 #&gt; x5 ~ #&gt; x4 (ax) 0.100 0.011 8.975 0.000 #&gt; y4 (bx) 0.053 0.010 5.351 0.000 #&gt; x6 ~ #&gt; x5 (ax) 0.100 0.011 8.975 0.000 #&gt; y5 (bx) 0.053 0.010 5.351 0.000 #&gt; x7 ~ #&gt; x6 (ax) 0.100 0.011 8.975 0.000 #&gt; y6 (bx) 0.053 0.010 5.351 0.000 #&gt; x8 ~ #&gt; x7 (ax) 0.100 0.011 8.975 0.000 #&gt; y7 (bx) 0.053 0.010 5.351 0.000 #&gt; x9 ~ #&gt; x8 (ax) 0.100 0.011 8.975 0.000 #&gt; y8 (bx) 0.053 0.010 5.351 0.000 #&gt; x10 ~ #&gt; x9 (ax) 0.100 0.011 8.975 0.000 #&gt; y9 (bx) 0.053 0.010 5.351 0.000 #&gt; x11 ~ #&gt; x10 (ax) 0.100 0.011 8.975 0.000 #&gt; y10 (bx) 0.053 0.010 5.351 0.000 #&gt; y2 ~ #&gt; x1 (by) 0.036 0.009 3.775 0.000 #&gt; y1 (ay) 0.104 0.011 9.128 0.000 #&gt; y3 ~ #&gt; x2 (by) 0.036 0.009 3.775 0.000 #&gt; y2 (ay) 0.104 0.011 9.128 0.000 #&gt; y4 ~ #&gt; x3 (by) 0.036 0.009 3.775 0.000 #&gt; y3 (ay) 0.104 0.011 9.128 0.000 #&gt; y5 ~ #&gt; x4 (by) 0.036 0.009 3.775 0.000 #&gt; y4 (ay) 0.104 0.011 9.128 0.000 #&gt; y6 ~ #&gt; x5 (by) 0.036 0.009 3.775 0.000 #&gt; y5 (ay) 0.104 0.011 9.128 0.000 #&gt; y7 ~ #&gt; x6 (by) 0.036 0.009 3.775 0.000 #&gt; y6 (ay) 0.104 0.011 9.128 0.000 #&gt; y8 ~ #&gt; x7 (by) 0.036 0.009 3.775 0.000 #&gt; y7 (ay) 0.104 0.011 9.128 0.000 #&gt; y9 ~ #&gt; x8 (by) 0.036 0.009 3.775 0.000 #&gt; y8 (ay) 0.104 0.011 9.128 0.000 #&gt; y10 ~ #&gt; x9 (by) 0.036 0.009 3.775 0.000 #&gt; y9 (ay) 0.104 0.011 9.128 0.000 #&gt; y11 ~ #&gt; x10 (by) 0.036 0.009 3.775 0.000 #&gt; y10 (ay) 0.104 0.011 9.128 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.059 0.013 4.582 0.000 #&gt; wx2 ~~ #&gt; wy2 0.048 0.013 3.756 0.000 #&gt; wx3 ~~ #&gt; wy3 0.031 0.011 2.813 0.005 #&gt; wx4 ~~ #&gt; wy4 0.044 0.011 4.011 0.000 #&gt; wx5 ~~ #&gt; wy5 0.044 0.011 4.004 0.000 #&gt; wx6 ~~ #&gt; wy6 0.027 0.011 2.507 0.012 #&gt; wx7 ~~ #&gt; wy7 0.033 0.011 2.939 0.003 #&gt; wx8 ~~ #&gt; wy8 0.032 0.011 2.895 0.004 #&gt; wx9 ~~ #&gt; wy9 0.011 0.013 0.864 0.388 #&gt; wx10 ~~ #&gt; wy10 0.047 0.013 3.656 0.000 #&gt; wx11 ~~ #&gt; wy11 0.062 0.013 4.858 0.000 #&gt; .RIx ~~ #&gt; .RIy 0.276 0.014 20.076 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; wx2 0.000 #&gt; wx3 0.000 #&gt; wx4 0.000 #&gt; wx5 0.000 #&gt; wx6 0.000 #&gt; wx7 0.000 #&gt; wx8 0.000 #&gt; wx9 0.000 #&gt; wx10 0.000 #&gt; wx11 0.000 #&gt; wy1 0.000 #&gt; wy2 0.000 #&gt; wy3 0.000 #&gt; wy4 0.000 #&gt; wy5 0.000 #&gt; wy6 0.000 #&gt; wy7 0.000 #&gt; wy8 0.000 #&gt; wy9 0.000 #&gt; wy10 0.000 #&gt; wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 0.517 0.020 25.624 0.000 #&gt; .RIy 0.482 0.018 26.510 0.000 #&gt; wx1 0.449 0.019 23.603 0.000 #&gt; wy1 0.377 0.016 23.042 0.000 #&gt; wx2 0.407 0.017 23.965 0.000 #&gt; wy2 0.429 0.017 24.601 0.000 #&gt; wx3 0.354 0.015 23.575 0.000 #&gt; wy3 0.317 0.014 23.087 0.000 #&gt; wx4 0.325 0.014 22.599 0.000 #&gt; wy4 0.327 0.015 22.411 0.000 #&gt; wx5 0.327 0.014 23.315 0.000 #&gt; wy5 0.335 0.014 23.309 0.000 #&gt; wx6 0.355 0.015 23.545 0.000 #&gt; wy6 0.308 0.013 23.106 0.000 #&gt; wx7 0.349 0.015 23.275 0.000 #&gt; wy7 0.331 0.014 23.418 0.000 #&gt; wx8 0.329 0.014 23.031 0.000 #&gt; wy8 0.334 0.014 23.240 0.000 #&gt; wx9 0.373 0.016 23.223 0.000 #&gt; wy9 0.378 0.017 22.877 0.000 #&gt; wx10 0.370 0.017 21.705 0.000 #&gt; wy10 0.344 0.016 21.448 0.000 #&gt; wx11 0.366 0.017 21.458 0.000 #&gt; wy11 0.326 0.016 20.542 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 -0.004 0.016 -0.256 0.798 #&gt; dif2 0.017 0.013 1.310 0.190 #&gt; dif3 -0.010 0.002 -5.415 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 103 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 98 #&gt; Number of equality constraints 36 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1290 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 5022.772 #&gt; Degrees of freedom 257 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 0.686 0.015 45.498 0.000 #&gt; x3 0.683 0.015 45.363 0.000 #&gt; x4 0.692 0.015 45.982 0.000 #&gt; x5 0.691 0.015 45.416 0.000 #&gt; x6 0.695 0.015 45.675 0.000 #&gt; x7 0.700 0.015 45.845 0.000 #&gt; x8 0.701 0.015 45.549 0.000 #&gt; x9 0.692 0.015 44.722 0.000 #&gt; x10 0.691 0.015 44.768 0.000 #&gt; x11 0.702 0.015 45.625 0.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 0.718 0.014 49.717 0.000 #&gt; y3 0.712 0.015 48.836 0.000 #&gt; y4 0.724 0.015 49.767 0.000 #&gt; y5 0.726 0.015 49.612 0.000 #&gt; y6 0.726 0.015 49.526 0.000 #&gt; y7 0.729 0.015 49.555 0.000 #&gt; y8 0.729 0.015 49.069 0.000 #&gt; y9 0.722 0.015 48.614 0.000 #&gt; y10 0.720 0.015 48.332 0.000 #&gt; y11 0.730 0.015 49.132 0.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (ex) 0.328 0.002 150.866 0.000 #&gt; RIy ~ #&gt; oply (ey) 0.341 0.002 149.368 0.000 #&gt; x2 ~ #&gt; x1 (ax) 0.149 0.012 12.205 0.000 #&gt; y1 (bx) 0.170 0.009 19.343 0.000 #&gt; x3 ~ #&gt; x2 (ax) 0.149 0.012 12.205 0.000 #&gt; y2 (bx) 0.170 0.009 19.343 0.000 #&gt; x4 ~ #&gt; x3 (ax) 0.149 0.012 12.205 0.000 #&gt; y3 (bx) 0.170 0.009 19.343 0.000 #&gt; x5 ~ #&gt; x4 (ax) 0.149 0.012 12.205 0.000 #&gt; y4 (bx) 0.170 0.009 19.343 0.000 #&gt; x6 ~ #&gt; x5 (ax) 0.149 0.012 12.205 0.000 #&gt; y5 (bx) 0.170 0.009 19.343 0.000 #&gt; x7 ~ #&gt; x6 (ax) 0.149 0.012 12.205 0.000 #&gt; y6 (bx) 0.170 0.009 19.343 0.000 #&gt; x8 ~ #&gt; x7 (ax) 0.149 0.012 12.205 0.000 #&gt; y7 (bx) 0.170 0.009 19.343 0.000 #&gt; x9 ~ #&gt; x8 (ax) 0.149 0.012 12.205 0.000 #&gt; y8 (bx) 0.170 0.009 19.343 0.000 #&gt; x10 ~ #&gt; x9 (ax) 0.149 0.012 12.205 0.000 #&gt; y9 (bx) 0.170 0.009 19.343 0.000 #&gt; x11 ~ #&gt; x10 (ax) 0.149 0.012 12.205 0.000 #&gt; y10 (bx) 0.170 0.009 19.343 0.000 #&gt; y2 ~ #&gt; x1 (by) 0.156 0.009 17.816 0.000 #&gt; y1 (ay) 0.137 0.012 11.742 0.000 #&gt; y3 ~ #&gt; x2 (by) 0.156 0.009 17.816 0.000 #&gt; y2 (ay) 0.137 0.012 11.742 0.000 #&gt; y4 ~ #&gt; x3 (by) 0.156 0.009 17.816 0.000 #&gt; y3 (ay) 0.137 0.012 11.742 0.000 #&gt; y5 ~ #&gt; x4 (by) 0.156 0.009 17.816 0.000 #&gt; y4 (ay) 0.137 0.012 11.742 0.000 #&gt; y6 ~ #&gt; x5 (by) 0.156 0.009 17.816 0.000 #&gt; y5 (ay) 0.137 0.012 11.742 0.000 #&gt; y7 ~ #&gt; x6 (by) 0.156 0.009 17.816 0.000 #&gt; y6 (ay) 0.137 0.012 11.742 0.000 #&gt; y8 ~ #&gt; x7 (by) 0.156 0.009 17.816 0.000 #&gt; y7 (ay) 0.137 0.012 11.742 0.000 #&gt; y9 ~ #&gt; x8 (by) 0.156 0.009 17.816 0.000 #&gt; y8 (ay) 0.137 0.012 11.742 0.000 #&gt; y10 ~ #&gt; x9 (by) 0.156 0.009 17.816 0.000 #&gt; y9 (ay) 0.137 0.012 11.742 0.000 #&gt; y11 ~ #&gt; x10 (by) 0.156 0.009 17.816 0.000 #&gt; y10 (ay) 0.137 0.012 11.742 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.065 0.012 5.508 0.000 #&gt; wx2 ~~ #&gt; wy2 0.052 0.009 5.578 0.000 #&gt; wx3 ~~ #&gt; wy3 0.071 0.009 7.727 0.000 #&gt; wx4 ~~ #&gt; wy4 0.041 0.008 5.019 0.000 #&gt; wx5 ~~ #&gt; wy5 0.050 0.008 6.279 0.000 #&gt; wx6 ~~ #&gt; wy6 0.047 0.007 6.644 0.000 #&gt; wx7 ~~ #&gt; wy7 0.042 0.007 5.804 0.000 #&gt; wx8 ~~ #&gt; wy8 0.037 0.008 4.603 0.000 #&gt; wx9 ~~ #&gt; wy9 0.041 0.008 5.043 0.000 #&gt; wx10 ~~ #&gt; wy10 0.072 0.010 6.895 0.000 #&gt; wx11 ~~ #&gt; wy11 0.067 0.009 7.105 0.000 #&gt; .RIx ~~ #&gt; .RIy 0.934 0.038 24.664 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; wx2 0.000 #&gt; wx3 0.000 #&gt; wx4 0.000 #&gt; wx5 0.000 #&gt; wx6 0.000 #&gt; wx7 0.000 #&gt; wx8 0.000 #&gt; wx9 0.000 #&gt; wx10 0.000 #&gt; wx11 0.000 #&gt; wy1 0.000 #&gt; wy2 0.000 #&gt; wy3 0.000 #&gt; wy4 0.000 #&gt; wy5 0.000 #&gt; wy6 0.000 #&gt; wy7 0.000 #&gt; wy8 0.000 #&gt; wy9 0.000 #&gt; wy10 0.000 #&gt; wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 1.626 0.046 35.382 0.000 #&gt; .RIy 1.690 0.047 35.742 0.000 #&gt; wx1 0.325 0.016 20.142 0.000 #&gt; wy1 0.337 0.016 21.188 0.000 #&gt; wx2 0.307 0.013 22.959 0.000 #&gt; wy2 0.256 0.011 22.443 0.000 #&gt; wx3 0.268 0.011 23.451 0.000 #&gt; wy3 0.308 0.013 23.721 0.000 #&gt; wx4 0.224 0.010 22.193 0.000 #&gt; wy4 0.254 0.011 22.403 0.000 #&gt; wx5 0.256 0.011 22.825 0.000 #&gt; wy5 0.213 0.009 22.451 0.000 #&gt; wx6 0.227 0.010 22.418 0.000 #&gt; wy6 0.185 0.008 21.951 0.000 #&gt; wx7 0.215 0.010 22.461 0.000 #&gt; wy7 0.209 0.009 22.363 0.000 #&gt; wx8 0.225 0.010 21.896 0.000 #&gt; wy8 0.234 0.010 22.539 0.000 #&gt; wx9 0.256 0.011 22.348 0.000 #&gt; wy9 0.215 0.010 22.082 0.000 #&gt; wx10 0.289 0.013 21.592 0.000 #&gt; wy10 0.269 0.013 21.477 0.000 #&gt; wx11 0.256 0.012 20.810 0.000 #&gt; wy11 0.245 0.012 20.453 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 0.012 0.017 0.718 0.473 #&gt; dif2 0.014 0.013 1.094 0.274 #&gt; dif3 -0.013 0.002 -5.771 0.000 #&gt; #&gt; lavaan 0.6-9 ended normally after 69 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 98 #&gt; Number of equality constraints 36 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1309 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 5574.731 #&gt; Degrees of freedom 257 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 0.706 0.016 43.047 0.000 #&gt; x3 0.703 0.017 42.527 0.000 #&gt; x4 0.705 0.017 42.711 0.000 #&gt; x5 0.733 0.016 44.506 0.000 #&gt; x6 0.697 0.017 41.287 0.000 #&gt; x7 0.710 0.016 43.026 0.000 #&gt; x8 0.721 0.017 43.497 0.000 #&gt; x9 0.726 0.017 43.078 0.000 #&gt; x10 0.725 0.017 42.690 0.000 #&gt; x11 0.738 0.017 43.165 0.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 0.781 0.016 49.839 0.000 #&gt; y3 0.765 0.016 48.052 0.000 #&gt; y4 0.776 0.016 48.942 0.000 #&gt; y5 0.788 0.016 49.499 0.000 #&gt; y6 0.761 0.016 46.910 0.000 #&gt; y7 0.763 0.016 48.250 0.000 #&gt; y8 0.773 0.016 48.345 0.000 #&gt; y9 0.769 0.016 47.589 0.000 #&gt; y10 0.762 0.016 46.777 0.000 #&gt; y11 0.765 0.016 47.086 0.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (ex) 0.271 0.002 119.594 0.000 #&gt; RIy ~ #&gt; oply (ey) 0.296 0.002 130.188 0.000 #&gt; x2 ~ #&gt; x1 (ax) 0.161 0.012 13.435 0.000 #&gt; y1 (bx) 0.142 0.009 15.367 0.000 #&gt; x3 ~ #&gt; x2 (ax) 0.161 0.012 13.435 0.000 #&gt; y2 (bx) 0.142 0.009 15.367 0.000 #&gt; x4 ~ #&gt; x3 (ax) 0.161 0.012 13.435 0.000 #&gt; y3 (bx) 0.142 0.009 15.367 0.000 #&gt; x5 ~ #&gt; x4 (ax) 0.161 0.012 13.435 0.000 #&gt; y4 (bx) 0.142 0.009 15.367 0.000 #&gt; x6 ~ #&gt; x5 (ax) 0.161 0.012 13.435 0.000 #&gt; y5 (bx) 0.142 0.009 15.367 0.000 #&gt; x7 ~ #&gt; x6 (ax) 0.161 0.012 13.435 0.000 #&gt; y6 (bx) 0.142 0.009 15.367 0.000 #&gt; x8 ~ #&gt; x7 (ax) 0.161 0.012 13.435 0.000 #&gt; y7 (bx) 0.142 0.009 15.367 0.000 #&gt; x9 ~ #&gt; x8 (ax) 0.161 0.012 13.435 0.000 #&gt; y8 (bx) 0.142 0.009 15.367 0.000 #&gt; x10 ~ #&gt; x9 (ax) 0.161 0.012 13.435 0.000 #&gt; y9 (bx) 0.142 0.009 15.367 0.000 #&gt; x11 ~ #&gt; x10 (ax) 0.161 0.012 13.435 0.000 #&gt; y10 (bx) 0.142 0.009 15.367 0.000 #&gt; y2 ~ #&gt; x1 (by) 0.127 0.009 13.616 0.000 #&gt; y1 (ay) 0.120 0.012 10.183 0.000 #&gt; y3 ~ #&gt; x2 (by) 0.127 0.009 13.616 0.000 #&gt; y2 (ay) 0.120 0.012 10.183 0.000 #&gt; y4 ~ #&gt; x3 (by) 0.127 0.009 13.616 0.000 #&gt; y3 (ay) 0.120 0.012 10.183 0.000 #&gt; y5 ~ #&gt; x4 (by) 0.127 0.009 13.616 0.000 #&gt; y4 (ay) 0.120 0.012 10.183 0.000 #&gt; y6 ~ #&gt; x5 (by) 0.127 0.009 13.616 0.000 #&gt; y5 (ay) 0.120 0.012 10.183 0.000 #&gt; y7 ~ #&gt; x6 (by) 0.127 0.009 13.616 0.000 #&gt; y6 (ay) 0.120 0.012 10.183 0.000 #&gt; y8 ~ #&gt; x7 (by) 0.127 0.009 13.616 0.000 #&gt; y7 (ay) 0.120 0.012 10.183 0.000 #&gt; y9 ~ #&gt; x8 (by) 0.127 0.009 13.616 0.000 #&gt; y8 (ay) 0.120 0.012 10.183 0.000 #&gt; y10 ~ #&gt; x9 (by) 0.127 0.009 13.616 0.000 #&gt; y9 (ay) 0.120 0.012 10.183 0.000 #&gt; y11 ~ #&gt; x10 (by) 0.127 0.009 13.616 0.000 #&gt; y10 (ay) 0.120 0.012 10.183 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.131 0.018 7.457 0.000 #&gt; wx2 ~~ #&gt; wy2 0.070 0.014 5.062 0.000 #&gt; wx3 ~~ #&gt; wy3 0.070 0.013 5.276 0.000 #&gt; wx4 ~~ #&gt; wy4 0.060 0.013 4.499 0.000 #&gt; wx5 ~~ #&gt; wy5 0.017 0.012 1.424 0.154 #&gt; wx6 ~~ #&gt; wy6 0.087 0.013 6.756 0.000 #&gt; wx7 ~~ #&gt; wy7 0.059 0.012 5.030 0.000 #&gt; wx8 ~~ #&gt; wy8 0.057 0.013 4.372 0.000 #&gt; wx9 ~~ #&gt; wy9 0.050 0.014 3.650 0.000 #&gt; wx10 ~~ #&gt; wy10 0.067 0.015 4.543 0.000 #&gt; wx11 ~~ #&gt; wy11 0.046 0.016 2.927 0.003 #&gt; .RIx ~~ #&gt; .RIy 0.869 0.036 24.206 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; wx1 0.000 #&gt; wx2 0.000 #&gt; wx3 0.000 #&gt; wx4 0.000 #&gt; wx5 0.000 #&gt; wx6 0.000 #&gt; wx7 0.000 #&gt; wx8 0.000 #&gt; wx9 0.000 #&gt; wx10 0.000 #&gt; wx11 0.000 #&gt; wy1 0.000 #&gt; wy2 0.000 #&gt; wy3 0.000 #&gt; wy4 0.000 #&gt; wy5 0.000 #&gt; wy6 0.000 #&gt; wy7 0.000 #&gt; wy8 0.000 #&gt; wy9 0.000 #&gt; wy10 0.000 #&gt; wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 1.589 0.048 33.316 0.000 #&gt; .RIy 1.404 0.042 33.553 0.000 #&gt; wx1 0.504 0.024 21.037 0.000 #&gt; wy1 0.497 0.023 21.768 0.000 #&gt; wx2 0.434 0.019 23.110 0.000 #&gt; wy2 0.410 0.018 22.933 0.000 #&gt; wx3 0.405 0.017 23.521 0.000 #&gt; wy3 0.423 0.018 23.368 0.000 #&gt; wx4 0.397 0.018 22.445 0.000 #&gt; wy4 0.348 0.016 21.687 0.000 #&gt; wx5 0.341 0.015 22.330 0.000 #&gt; wy5 0.353 0.016 22.553 0.000 #&gt; wx6 0.397 0.017 23.271 0.000 #&gt; wy6 0.383 0.017 22.925 0.000 #&gt; wx7 0.374 0.016 23.056 0.000 #&gt; wy7 0.317 0.014 22.339 0.000 #&gt; wx8 0.374 0.017 22.579 0.000 #&gt; wy8 0.396 0.017 22.750 0.000 #&gt; wx9 0.397 0.018 22.364 0.000 #&gt; wy9 0.398 0.018 22.288 0.000 #&gt; wx10 0.392 0.019 21.191 0.000 #&gt; wy10 0.404 0.019 21.155 0.000 #&gt; wx11 0.439 0.021 21.345 0.000 #&gt; wy11 0.384 0.019 20.288 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 0.041 0.017 2.442 0.015 #&gt; dif2 0.015 0.014 1.103 0.270 #&gt; dif3 -0.025 0.002 -10.223 0.000 3.6.7.4 LT-RI-CLPM LTRICLPM &lt;- &#39; # Create between components (random intercepts) RIx =~ 1*x1 + 1*x2 + 1*x3 + 1*x4 + 1*x5 + 1*x6 + 1*x7 + 1*x8 + 1*x9 + 1*x10 + 1*x11 RIy =~ 1*y1 + 1*y2 + 1*y3 + 1*y4 + 1*y5 + 1*y6 + 1*y7 + 1*y8 + 1*y9 + 1*y10 + 1*y11 RIx ~ ex*oplx RIy ~ ey*oply #Random slopes RIsx =~ 1*x1 + 2*x2 + 3*x3 + 4*x4 + 5*x5 + 6*x6 + 7*x7 + 8*x8 + 9*x9 + 10*x10 + 11*x11 RIsy =~ 1*y1 + 2*y2 + 3*y3 + 4*y4 + 5*y5 + 6*y6 + 7*y7 + 8*y8 + 9*y9 + 10*y10 + 11*y11 RIsx ~ fx*oplx RIsy ~ fy*oply # Create within-person centered variables wx1 =~ 1*x1 wx2 =~ 1*x2 wx3 =~ 1*x3 wx4 =~ 1*x4 wx5 =~ 1*x5 wx6 =~ 1*x6 wx7 =~ 1*x7 wx8 =~ 1*x8 wx9 =~ 1*x9 wx10 =~ 1*x10 wx11 =~ 1*x11 wy1 =~ 1*y1 wy2 =~ 1*y2 wy3 =~ 1*y3 wy4 =~ 1*y4 wy5 =~ 1*y5 wy6 =~ 1*y6 wy7 =~ 1*y7 wy8 =~ 1*y8 wy9 =~ 1*y9 wy10 =~ 1*y10 wy11 =~ 1*y11 # Estimate the lagged effects between the within-person centered variables. x2 ~ ax*x1 + bx*y1 x3 ~ ax*x2 + bx*y2 x4 ~ ax*x3 + bx*y3 x5 ~ ax*x4 + bx*y4 x6 ~ ax*x5 + bx*y5 x7 ~ ax*x6 + bx*y6 x8 ~ ax*x7 + bx*y7 x9 ~ ax*x8 + bx*y8 x10 ~ ax*x9 + bx*y9 x11 ~ ax*x10 + bx*y10 y2 ~ by*x1 + ay*y1 y3 ~ by*x2 + ay*y2 y4 ~ by*x3 + ay*y3 y5 ~ by*x4 + ay*y4 y6 ~ by*x5 + ay*y5 y7 ~ by*x6 + ay*y6 y8 ~ by*x7 + ay*y7 y9 ~ by*x8 + ay*y8 y10 ~ by*x9 + ay*y9 y11 ~ by*x10 + ay*y10 dif1 := ax - ay dif2 := bx - by dif3 := ex - ey dif4 := fx - fy # Estimate the (residual) covariance between the within-person centered variables wx1 ~~ wy1 # Covariance wx2 ~~ wy2 wx3 ~~ wy3 wx4 ~~ wy4 wx5 ~~ wy5 wx6 ~~ wy6 wx7 ~~ wy7 wx8 ~~ wy8 wx9 ~~ wy9 wx10 ~~ wy10 wx11 ~~ wy11 # Estimate the variance and covariance of the random intercepts and random slopes. RIx ~~ RIx RIy ~~ RIy RIx ~~ RIy #covariance intercepts: interpretation SELECTION RIsx ~~ RIsx RIsy ~~ RIsy RIsx ~~ RIsy #covariance slopes: interpretation COMMON CONTEXT RIx ~~ RIsx #covariance intercept/slope: interpretation regression to the mean RIy ~~ RIsy RIx ~~ RIsy #cross-covariance: interpretation INFLUENCE? RIy ~~ RIsx # Estimate the (residual) variance of the within-person centered variables. wx1 ~~ wx1 # Variances wy1 ~~ wy1 wx2 ~~ wx2 # Residual variances wy2 ~~ wy2 wx3 ~~ wx3 wy3 ~~ wy3 wx4 ~~ wx4 wy4 ~~ wy4 wx5 ~~ wx5 wy5 ~~ wy5 wx6 ~~ wx6 wy6 ~~ wy6 wx7 ~~ wx7 wy7 ~~ wy7 wx8 ~~ wx8 wy8 ~~ wy8 wx9 ~~ wx9 wy9 ~~ wy9 wx10 ~~ wx10 wy10 ~~ wy10 wx11 ~~ wx11 wy11 ~~ wy11 &#39; #Estimate models a bit faster: estimate &lt;- function(x) lavaan(LTRICLPM, data=x, missing = &quot;fiml.x&quot;, meanstructure = T ) library(future.apply) plan(multisession) results_temp &lt;- future_lapply(datalist_ori, estimate) summary(results_temp[[4]]) results[[29]] &lt;- results_temp[[1]] results[[30]] &lt;- results_temp[[2]] results[[31]] &lt;- results_temp[[3]] results[[32]] &lt;- results_temp[[4]] names(results)[29:32] &lt;- c(&quot;fitm4h2y1&quot;, &quot;fitm4h2y2&quot;,&quot;fitm4h2y3&quot;,&quot;fitm4h2y4&quot;) save(results, file=&quot;results.RData&quot;) load(&quot;addfiles/results.Rdata&quot;) summary(results[[29]]) summary(results[[30]]) summary(results[[31]]) summary(results[[32]]) #&gt; lavaan 0.6-9 ended normally after 105 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 87 #&gt; Number of equality constraints 36 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1415 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 1478.192 #&gt; Degrees of freedom 268 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; RIsx =~ #&gt; x1 1.000 #&gt; x2 2.000 #&gt; x3 3.000 #&gt; x4 4.000 #&gt; x5 5.000 #&gt; x6 6.000 #&gt; x7 7.000 #&gt; x8 8.000 #&gt; x9 9.000 #&gt; x10 10.000 #&gt; x11 11.000 #&gt; RIsy =~ #&gt; y1 1.000 #&gt; y2 2.000 #&gt; y3 3.000 #&gt; y4 4.000 #&gt; y5 5.000 #&gt; y6 6.000 #&gt; y7 7.000 #&gt; y8 8.000 #&gt; y9 9.000 #&gt; y10 10.000 #&gt; y11 11.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (ex) 0.126 0.002 63.991 0.000 #&gt; RIy ~ #&gt; oply (ey) 0.123 0.002 65.556 0.000 #&gt; RIsx ~ #&gt; oplx (fx) -0.003 0.000 -11.534 0.000 #&gt; RIsy ~ #&gt; oply (fy) -0.002 0.000 -9.901 0.000 #&gt; x2 ~ #&gt; x1 (ax) 0.076 0.011 7.263 0.000 #&gt; y1 (bx) 0.024 0.011 2.168 0.030 #&gt; x3 ~ #&gt; x2 (ax) 0.076 0.011 7.263 0.000 #&gt; y2 (bx) 0.024 0.011 2.168 0.030 #&gt; x4 ~ #&gt; x3 (ax) 0.076 0.011 7.263 0.000 #&gt; y3 (bx) 0.024 0.011 2.168 0.030 #&gt; x5 ~ #&gt; x4 (ax) 0.076 0.011 7.263 0.000 #&gt; y4 (bx) 0.024 0.011 2.168 0.030 #&gt; x6 ~ #&gt; x5 (ax) 0.076 0.011 7.263 0.000 #&gt; y5 (bx) 0.024 0.011 2.168 0.030 #&gt; x7 ~ #&gt; x6 (ax) 0.076 0.011 7.263 0.000 #&gt; y6 (bx) 0.024 0.011 2.168 0.030 #&gt; x8 ~ #&gt; x7 (ax) 0.076 0.011 7.263 0.000 #&gt; y7 (bx) 0.024 0.011 2.168 0.030 #&gt; x9 ~ #&gt; x8 (ax) 0.076 0.011 7.263 0.000 #&gt; y8 (bx) 0.024 0.011 2.168 0.030 #&gt; x10 ~ #&gt; x9 (ax) 0.076 0.011 7.263 0.000 #&gt; y9 (bx) 0.024 0.011 2.168 0.030 #&gt; x11 ~ #&gt; x10 (ax) 0.076 0.011 7.263 0.000 #&gt; y10 (bx) 0.024 0.011 2.168 0.030 #&gt; y2 ~ #&gt; x1 (by) 0.027 0.009 2.926 0.003 #&gt; y1 (ay) 0.072 0.011 6.631 0.000 #&gt; y3 ~ #&gt; x2 (by) 0.027 0.009 2.926 0.003 #&gt; y2 (ay) 0.072 0.011 6.631 0.000 #&gt; y4 ~ #&gt; x3 (by) 0.027 0.009 2.926 0.003 #&gt; y3 (ay) 0.072 0.011 6.631 0.000 #&gt; y5 ~ #&gt; x4 (by) 0.027 0.009 2.926 0.003 #&gt; y4 (ay) 0.072 0.011 6.631 0.000 #&gt; y6 ~ #&gt; x5 (by) 0.027 0.009 2.926 0.003 #&gt; y5 (ay) 0.072 0.011 6.631 0.000 #&gt; y7 ~ #&gt; x6 (by) 0.027 0.009 2.926 0.003 #&gt; y6 (ay) 0.072 0.011 6.631 0.000 #&gt; y8 ~ #&gt; x7 (by) 0.027 0.009 2.926 0.003 #&gt; y7 (ay) 0.072 0.011 6.631 0.000 #&gt; y9 ~ #&gt; x8 (by) 0.027 0.009 2.926 0.003 #&gt; y8 (ay) 0.072 0.011 6.631 0.000 #&gt; y10 ~ #&gt; x9 (by) 0.027 0.009 2.926 0.003 #&gt; y9 (ay) 0.072 0.011 6.631 0.000 #&gt; y11 ~ #&gt; x10 (by) 0.027 0.009 2.926 0.003 #&gt; y10 (ay) 0.072 0.011 6.631 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.114 0.023 5.062 0.000 #&gt; wx2 ~~ #&gt; wy2 0.052 0.019 2.801 0.005 #&gt; wx3 ~~ #&gt; wy3 0.092 0.018 5.091 0.000 #&gt; wx4 ~~ #&gt; wy4 0.100 0.018 5.440 0.000 #&gt; wx5 ~~ #&gt; wy5 0.081 0.021 3.936 0.000 #&gt; wx6 ~~ #&gt; wy6 0.061 0.016 3.837 0.000 #&gt; wx7 ~~ #&gt; wy7 0.079 0.015 5.258 0.000 #&gt; wx8 ~~ #&gt; wy8 0.064 0.017 3.698 0.000 #&gt; wx9 ~~ #&gt; wy9 0.068 0.018 3.746 0.000 #&gt; wx10 ~~ #&gt; wy10 0.036 0.020 1.799 0.072 #&gt; wx11 ~~ #&gt; wy11 0.119 0.023 5.245 0.000 #&gt; .RIx ~~ #&gt; .RIy 0.282 0.025 11.367 0.000 #&gt; .RIsx ~~ #&gt; .RIsy 0.002 0.000 4.159 0.000 #&gt; .RIx ~~ #&gt; .RIsx -0.022 0.004 -5.929 0.000 #&gt; .RIy ~~ #&gt; .RIsy -0.013 0.003 -4.447 0.000 #&gt; .RIx ~~ #&gt; .RIsy -0.003 0.003 -1.121 0.262 #&gt; .RIy ~~ #&gt; .RIsx -0.003 0.003 -1.109 0.267 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; .RIsx 0.000 #&gt; .RIsy 0.000 #&gt; wx1 0.000 #&gt; wx2 0.000 #&gt; wx3 0.000 #&gt; wx4 0.000 #&gt; wx5 0.000 #&gt; wx6 0.000 #&gt; wx7 0.000 #&gt; wx8 0.000 #&gt; wx9 0.000 #&gt; wx10 0.000 #&gt; wx11 0.000 #&gt; wy1 0.000 #&gt; wy2 0.000 #&gt; wy3 0.000 #&gt; wy4 0.000 #&gt; wy5 0.000 #&gt; wy6 0.000 #&gt; wy7 0.000 #&gt; wy8 0.000 #&gt; wy9 0.000 #&gt; wy10 0.000 #&gt; wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 0.746 0.039 19.316 0.000 #&gt; .RIy 0.482 0.029 16.560 0.000 #&gt; .RIsx 0.005 0.001 9.142 0.000 #&gt; .RIsy 0.003 0.000 6.925 0.000 #&gt; wx1 0.634 0.031 20.236 0.000 #&gt; wy1 0.584 0.028 20.528 0.000 #&gt; wx2 0.591 0.027 21.858 0.000 #&gt; wy2 0.495 0.023 21.300 0.000 #&gt; wx3 0.546 0.024 22.466 0.000 #&gt; wy3 0.485 0.022 21.877 0.000 #&gt; wx4 0.572 0.026 22.085 0.000 #&gt; wy4 0.466 0.022 21.369 0.000 #&gt; wx5 0.697 0.029 23.839 0.000 #&gt; wy5 0.543 0.023 23.169 0.000 #&gt; wx6 0.492 0.022 22.859 0.000 #&gt; wy6 0.407 0.019 21.982 0.000 #&gt; wx7 0.448 0.020 22.275 0.000 #&gt; wy7 0.423 0.019 22.154 0.000 #&gt; wx8 0.498 0.022 22.311 0.000 #&gt; wy8 0.472 0.021 22.196 0.000 #&gt; wx9 0.531 0.025 21.539 0.000 #&gt; wy9 0.453 0.022 20.814 0.000 #&gt; wx10 0.490 0.025 19.383 0.000 #&gt; wy10 0.467 0.024 19.368 0.000 #&gt; wx11 0.570 0.030 18.710 0.000 #&gt; wy11 0.503 0.028 18.049 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 0.004 0.015 0.270 0.787 #&gt; dif2 -0.004 0.014 -0.267 0.789 #&gt; dif3 0.003 0.002 1.494 0.135 #&gt; dif4 -0.001 0.000 -1.904 0.057 #&gt; #&gt; lavaan 0.6-9 ended normally after 103 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 87 #&gt; Number of equality constraints 36 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1230 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 671.759 #&gt; Degrees of freedom 268 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; RIsx =~ #&gt; x1 1.000 #&gt; x2 2.000 #&gt; x3 3.000 #&gt; x4 4.000 #&gt; x5 5.000 #&gt; x6 6.000 #&gt; x7 7.000 #&gt; x8 8.000 #&gt; x9 9.000 #&gt; x10 10.000 #&gt; x11 11.000 #&gt; RIsy =~ #&gt; y1 1.000 #&gt; y2 2.000 #&gt; y3 3.000 #&gt; y4 4.000 #&gt; y5 5.000 #&gt; y6 6.000 #&gt; y7 7.000 #&gt; y8 8.000 #&gt; y9 9.000 #&gt; y10 10.000 #&gt; y11 11.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (ex) 0.094 0.002 60.054 0.000 #&gt; RIy ~ #&gt; oply (ey) 0.104 0.002 65.067 0.000 #&gt; RIsx ~ #&gt; oplx (fx) -0.000 0.000 -1.214 0.225 #&gt; RIsy ~ #&gt; oply (fy) -0.000 0.000 -0.749 0.454 #&gt; x2 ~ #&gt; x1 (ax) 0.045 0.010 4.475 0.000 #&gt; y1 (bx) 0.030 0.010 3.179 0.001 #&gt; x3 ~ #&gt; x2 (ax) 0.045 0.010 4.475 0.000 #&gt; y2 (bx) 0.030 0.010 3.179 0.001 #&gt; x4 ~ #&gt; x3 (ax) 0.045 0.010 4.475 0.000 #&gt; y3 (bx) 0.030 0.010 3.179 0.001 #&gt; x5 ~ #&gt; x4 (ax) 0.045 0.010 4.475 0.000 #&gt; y4 (bx) 0.030 0.010 3.179 0.001 #&gt; x6 ~ #&gt; x5 (ax) 0.045 0.010 4.475 0.000 #&gt; y5 (bx) 0.030 0.010 3.179 0.001 #&gt; x7 ~ #&gt; x6 (ax) 0.045 0.010 4.475 0.000 #&gt; y6 (bx) 0.030 0.010 3.179 0.001 #&gt; x8 ~ #&gt; x7 (ax) 0.045 0.010 4.475 0.000 #&gt; y7 (bx) 0.030 0.010 3.179 0.001 #&gt; x9 ~ #&gt; x8 (ax) 0.045 0.010 4.475 0.000 #&gt; y8 (bx) 0.030 0.010 3.179 0.001 #&gt; x10 ~ #&gt; x9 (ax) 0.045 0.010 4.475 0.000 #&gt; y9 (bx) 0.030 0.010 3.179 0.001 #&gt; x11 ~ #&gt; x10 (ax) 0.045 0.010 4.475 0.000 #&gt; y10 (bx) 0.030 0.010 3.179 0.001 #&gt; y2 ~ #&gt; x1 (by) 0.016 0.009 1.735 0.083 #&gt; y1 (ay) 0.045 0.010 4.525 0.000 #&gt; y3 ~ #&gt; x2 (by) 0.016 0.009 1.735 0.083 #&gt; y2 (ay) 0.045 0.010 4.525 0.000 #&gt; y4 ~ #&gt; x3 (by) 0.016 0.009 1.735 0.083 #&gt; y3 (ay) 0.045 0.010 4.525 0.000 #&gt; y5 ~ #&gt; x4 (by) 0.016 0.009 1.735 0.083 #&gt; y4 (ay) 0.045 0.010 4.525 0.000 #&gt; y6 ~ #&gt; x5 (by) 0.016 0.009 1.735 0.083 #&gt; y5 (ay) 0.045 0.010 4.525 0.000 #&gt; y7 ~ #&gt; x6 (by) 0.016 0.009 1.735 0.083 #&gt; y6 (ay) 0.045 0.010 4.525 0.000 #&gt; y8 ~ #&gt; x7 (by) 0.016 0.009 1.735 0.083 #&gt; y7 (ay) 0.045 0.010 4.525 0.000 #&gt; y9 ~ #&gt; x8 (by) 0.016 0.009 1.735 0.083 #&gt; y8 (ay) 0.045 0.010 4.525 0.000 #&gt; y10 ~ #&gt; x9 (by) 0.016 0.009 1.735 0.083 #&gt; y9 (ay) 0.045 0.010 4.525 0.000 #&gt; y11 ~ #&gt; x10 (by) 0.016 0.009 1.735 0.083 #&gt; y10 (ay) 0.045 0.010 4.525 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.039 0.013 3.029 0.002 #&gt; wx2 ~~ #&gt; wy2 0.041 0.013 3.218 0.001 #&gt; wx3 ~~ #&gt; wy3 0.020 0.011 1.816 0.069 #&gt; wx4 ~~ #&gt; wy4 0.038 0.011 3.536 0.000 #&gt; wx5 ~~ #&gt; wy5 0.040 0.011 3.770 0.000 #&gt; wx6 ~~ #&gt; wy6 0.024 0.011 2.267 0.023 #&gt; wx7 ~~ #&gt; wy7 0.032 0.011 2.846 0.004 #&gt; wx8 ~~ #&gt; wy8 0.026 0.011 2.409 0.016 #&gt; wx9 ~~ #&gt; wy9 0.010 0.013 0.829 0.407 #&gt; wx10 ~~ #&gt; wy10 0.041 0.013 3.203 0.001 #&gt; wx11 ~~ #&gt; wy11 0.054 0.013 4.179 0.000 #&gt; .RIx ~~ #&gt; .RIy 0.345 0.020 17.055 0.000 #&gt; .RIsx ~~ #&gt; .RIsy 0.001 0.000 3.214 0.001 #&gt; .RIx ~~ #&gt; .RIsx -0.013 0.002 -5.989 0.000 #&gt; .RIy ~~ #&gt; .RIsy -0.016 0.002 -7.090 0.000 #&gt; .RIx ~~ #&gt; .RIsy -0.010 0.002 -5.070 0.000 #&gt; .RIy ~~ #&gt; .RIsx -0.007 0.002 -3.652 0.000 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; .RIsx 0.000 #&gt; .RIsy 0.000 #&gt; wx1 0.000 #&gt; wx2 0.000 #&gt; wx3 0.000 #&gt; wx4 0.000 #&gt; wx5 0.000 #&gt; wx6 0.000 #&gt; wx7 0.000 #&gt; wx8 0.000 #&gt; wx9 0.000 #&gt; wx10 0.000 #&gt; wx11 0.000 #&gt; wy1 0.000 #&gt; wy2 0.000 #&gt; wy3 0.000 #&gt; wy4 0.000 #&gt; wy5 0.000 #&gt; wy6 0.000 #&gt; wy7 0.000 #&gt; wy8 0.000 #&gt; wy9 0.000 #&gt; wy10 0.000 #&gt; wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 0.579 0.027 21.592 0.000 #&gt; .RIy 0.577 0.026 21.949 0.000 #&gt; .RIsx 0.001 0.000 5.600 0.000 #&gt; .RIsy 0.002 0.000 6.184 0.000 #&gt; wx1 0.416 0.019 21.689 0.000 #&gt; wy1 0.339 0.017 20.520 0.000 #&gt; wx2 0.383 0.017 22.667 0.000 #&gt; wy2 0.404 0.017 23.284 0.000 #&gt; wx3 0.332 0.015 22.866 0.000 #&gt; wy3 0.298 0.013 22.324 0.000 #&gt; wx4 0.312 0.014 22.295 0.000 #&gt; wy4 0.313 0.014 22.208 0.000 #&gt; wx5 0.322 0.014 23.460 0.000 #&gt; wy5 0.328 0.014 23.418 0.000 #&gt; wx6 0.347 0.015 23.609 0.000 #&gt; wy6 0.301 0.013 23.213 0.000 #&gt; wx7 0.347 0.015 23.416 0.000 #&gt; wy7 0.324 0.014 23.455 0.000 #&gt; wx8 0.315 0.014 22.654 0.000 #&gt; wy8 0.320 0.014 22.812 0.000 #&gt; wx9 0.360 0.016 22.425 0.000 #&gt; wy9 0.361 0.016 22.117 0.000 #&gt; wx10 0.348 0.017 20.643 0.000 #&gt; wy10 0.318 0.016 20.315 0.000 #&gt; wx11 0.340 0.017 19.702 0.000 #&gt; wy11 0.300 0.016 18.672 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 0.000 0.014 0.026 0.980 #&gt; dif2 0.014 0.013 1.153 0.249 #&gt; dif3 -0.009 0.002 -5.209 0.000 #&gt; dif4 -0.000 0.000 -0.337 0.736 #&gt; #&gt; lavaan 0.6-9 ended normally after 123 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 87 #&gt; Number of equality constraints 36 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1290 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 5437.473 #&gt; Degrees of freedom 268 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; RIsx =~ #&gt; x1 1.000 #&gt; x2 2.000 #&gt; x3 3.000 #&gt; x4 4.000 #&gt; x5 5.000 #&gt; x6 6.000 #&gt; x7 7.000 #&gt; x8 8.000 #&gt; x9 9.000 #&gt; x10 10.000 #&gt; x11 11.000 #&gt; RIsy =~ #&gt; y1 1.000 #&gt; y2 2.000 #&gt; y3 3.000 #&gt; y4 4.000 #&gt; y5 5.000 #&gt; y6 6.000 #&gt; y7 7.000 #&gt; y8 8.000 #&gt; y9 9.000 #&gt; y10 10.000 #&gt; y11 11.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (ex) 0.322 0.002 144.287 0.000 #&gt; RIy ~ #&gt; oply (ey) 0.335 0.002 141.169 0.000 #&gt; RIsx ~ #&gt; oplx (fx) 0.000 0.000 2.606 0.009 #&gt; RIsy ~ #&gt; oply (fy) 0.000 0.000 2.659 0.008 #&gt; x2 ~ #&gt; x1 (ax) -0.060 0.008 -7.469 0.000 #&gt; y1 (bx) 0.080 0.008 10.062 0.000 #&gt; x3 ~ #&gt; x2 (ax) -0.060 0.008 -7.469 0.000 #&gt; y2 (bx) 0.080 0.008 10.062 0.000 #&gt; x4 ~ #&gt; x3 (ax) -0.060 0.008 -7.469 0.000 #&gt; y3 (bx) 0.080 0.008 10.062 0.000 #&gt; x5 ~ #&gt; x4 (ax) -0.060 0.008 -7.469 0.000 #&gt; y4 (bx) 0.080 0.008 10.062 0.000 #&gt; x6 ~ #&gt; x5 (ax) -0.060 0.008 -7.469 0.000 #&gt; y5 (bx) 0.080 0.008 10.062 0.000 #&gt; x7 ~ #&gt; x6 (ax) -0.060 0.008 -7.469 0.000 #&gt; y6 (bx) 0.080 0.008 10.062 0.000 #&gt; x8 ~ #&gt; x7 (ax) -0.060 0.008 -7.469 0.000 #&gt; y7 (bx) 0.080 0.008 10.062 0.000 #&gt; x9 ~ #&gt; x8 (ax) -0.060 0.008 -7.469 0.000 #&gt; y8 (bx) 0.080 0.008 10.062 0.000 #&gt; x10 ~ #&gt; x9 (ax) -0.060 0.008 -7.469 0.000 #&gt; y9 (bx) 0.080 0.008 10.062 0.000 #&gt; x11 ~ #&gt; x10 (ax) -0.060 0.008 -7.469 0.000 #&gt; y10 (bx) 0.080 0.008 10.062 0.000 #&gt; y2 ~ #&gt; x1 (by) 0.079 0.008 10.245 0.000 #&gt; y1 (ay) -0.055 0.008 -7.091 0.000 #&gt; y3 ~ #&gt; x2 (by) 0.079 0.008 10.245 0.000 #&gt; y2 (ay) -0.055 0.008 -7.091 0.000 #&gt; y4 ~ #&gt; x3 (by) 0.079 0.008 10.245 0.000 #&gt; y3 (ay) -0.055 0.008 -7.091 0.000 #&gt; y5 ~ #&gt; x4 (by) 0.079 0.008 10.245 0.000 #&gt; y4 (ay) -0.055 0.008 -7.091 0.000 #&gt; y6 ~ #&gt; x5 (by) 0.079 0.008 10.245 0.000 #&gt; y5 (ay) -0.055 0.008 -7.091 0.000 #&gt; y7 ~ #&gt; x6 (by) 0.079 0.008 10.245 0.000 #&gt; y6 (ay) -0.055 0.008 -7.091 0.000 #&gt; y8 ~ #&gt; x7 (by) 0.079 0.008 10.245 0.000 #&gt; y7 (ay) -0.055 0.008 -7.091 0.000 #&gt; y9 ~ #&gt; x8 (by) 0.079 0.008 10.245 0.000 #&gt; y8 (ay) -0.055 0.008 -7.091 0.000 #&gt; y10 ~ #&gt; x9 (by) 0.079 0.008 10.245 0.000 #&gt; y9 (ay) -0.055 0.008 -7.091 0.000 #&gt; y11 ~ #&gt; x10 (by) 0.079 0.008 10.245 0.000 #&gt; y10 (ay) -0.055 0.008 -7.091 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.065 0.010 6.202 0.000 #&gt; wx2 ~~ #&gt; wy2 0.034 0.008 4.100 0.000 #&gt; wx3 ~~ #&gt; wy3 0.065 0.009 7.512 0.000 #&gt; wx4 ~~ #&gt; wy4 0.030 0.008 3.877 0.000 #&gt; wx5 ~~ #&gt; wy5 0.038 0.007 5.206 0.000 #&gt; wx6 ~~ #&gt; wy6 0.037 0.007 5.555 0.000 #&gt; wx7 ~~ #&gt; wy7 0.032 0.007 4.668 0.000 #&gt; wx8 ~~ #&gt; wy8 0.024 0.007 3.368 0.001 #&gt; wx9 ~~ #&gt; wy9 0.033 0.008 4.281 0.000 #&gt; wx10 ~~ #&gt; wy10 0.063 0.010 6.512 0.000 #&gt; wx11 ~~ #&gt; wy11 0.055 0.009 5.930 0.000 #&gt; .RIx ~~ #&gt; .RIy 1.047 0.044 23.905 0.000 #&gt; .RIsx ~~ #&gt; .RIsy 0.000 0.000 1.325 0.185 #&gt; .RIx ~~ #&gt; .RIsx -0.012 0.003 -4.256 0.000 #&gt; .RIy ~~ #&gt; .RIsy -0.017 0.003 -5.846 0.000 #&gt; .RIx ~~ #&gt; .RIsy -0.006 0.003 -2.163 0.031 #&gt; .RIy ~~ #&gt; .RIsx -0.002 0.003 -0.755 0.450 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; .RIsx 0.000 #&gt; .RIsy 0.000 #&gt; wx1 0.000 #&gt; wx2 0.000 #&gt; wx3 0.000 #&gt; wx4 0.000 #&gt; wx5 0.000 #&gt; wx6 0.000 #&gt; wx7 0.000 #&gt; wx8 0.000 #&gt; wx9 0.000 #&gt; wx10 0.000 #&gt; wx11 0.000 #&gt; wy1 0.000 #&gt; wy2 0.000 #&gt; wy3 0.000 #&gt; wy4 0.000 #&gt; wy5 0.000 #&gt; wy6 0.000 #&gt; wy7 0.000 #&gt; wy8 0.000 #&gt; wy9 0.000 #&gt; wy10 0.000 #&gt; wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 1.665 0.052 31.990 0.000 #&gt; .RIy 1.780 0.055 32.356 0.000 #&gt; .RIsx 0.002 0.000 9.412 0.000 #&gt; .RIsy 0.002 0.000 10.107 0.000 #&gt; wx1 0.281 0.014 19.895 0.000 #&gt; wy1 0.283 0.014 20.425 0.000 #&gt; wx2 0.249 0.012 20.973 0.000 #&gt; wy2 0.205 0.010 20.001 0.000 #&gt; wx3 0.239 0.011 22.202 0.000 #&gt; wy3 0.282 0.012 22.850 0.000 #&gt; wx4 0.206 0.010 21.568 0.000 #&gt; wy4 0.229 0.010 21.933 0.000 #&gt; wx5 0.235 0.010 22.805 0.000 #&gt; wy5 0.186 0.008 22.334 0.000 #&gt; wx6 0.206 0.009 22.323 0.000 #&gt; wy6 0.176 0.008 21.927 0.000 #&gt; wx7 0.200 0.009 22.423 0.000 #&gt; wy7 0.194 0.009 22.241 0.000 #&gt; wx8 0.204 0.009 21.698 0.000 #&gt; wy8 0.208 0.009 22.250 0.000 #&gt; wx9 0.222 0.011 21.147 0.000 #&gt; wy9 0.190 0.009 20.769 0.000 #&gt; wx10 0.243 0.012 20.177 0.000 #&gt; wy10 0.235 0.012 19.995 0.000 #&gt; wx11 0.218 0.012 17.891 0.000 #&gt; wy11 0.197 0.012 17.115 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 -0.005 0.012 -0.381 0.703 #&gt; dif2 0.000 0.012 0.034 0.973 #&gt; dif3 -0.013 0.002 -5.475 0.000 #&gt; dif4 -0.000 0.000 -0.120 0.905 #&gt; #&gt; lavaan 0.6-9 ended normally after 96 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 87 #&gt; Number of equality constraints 36 #&gt; #&gt; Number of observations 3283 #&gt; Number of missing patterns 1309 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 5808.751 #&gt; Degrees of freedom 268 #&gt; P-value (Chi-square) 0.000 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; Latent Variables: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx =~ #&gt; x1 1.000 #&gt; x2 1.000 #&gt; x3 1.000 #&gt; x4 1.000 #&gt; x5 1.000 #&gt; x6 1.000 #&gt; x7 1.000 #&gt; x8 1.000 #&gt; x9 1.000 #&gt; x10 1.000 #&gt; x11 1.000 #&gt; RIy =~ #&gt; y1 1.000 #&gt; y2 1.000 #&gt; y3 1.000 #&gt; y4 1.000 #&gt; y5 1.000 #&gt; y6 1.000 #&gt; y7 1.000 #&gt; y8 1.000 #&gt; y9 1.000 #&gt; y10 1.000 #&gt; y11 1.000 #&gt; RIsx =~ #&gt; x1 1.000 #&gt; x2 2.000 #&gt; x3 3.000 #&gt; x4 4.000 #&gt; x5 5.000 #&gt; x6 6.000 #&gt; x7 7.000 #&gt; x8 8.000 #&gt; x9 9.000 #&gt; x10 10.000 #&gt; x11 11.000 #&gt; RIsy =~ #&gt; y1 1.000 #&gt; y2 2.000 #&gt; y3 3.000 #&gt; y4 4.000 #&gt; y5 5.000 #&gt; y6 6.000 #&gt; y7 7.000 #&gt; y8 8.000 #&gt; y9 9.000 #&gt; y10 10.000 #&gt; y11 11.000 #&gt; wx1 =~ #&gt; x1 1.000 #&gt; wx2 =~ #&gt; x2 1.000 #&gt; wx3 =~ #&gt; x3 1.000 #&gt; wx4 =~ #&gt; x4 1.000 #&gt; wx5 =~ #&gt; x5 1.000 #&gt; wx6 =~ #&gt; x6 1.000 #&gt; wx7 =~ #&gt; x7 1.000 #&gt; wx8 =~ #&gt; x8 1.000 #&gt; wx9 =~ #&gt; x9 1.000 #&gt; wx10 =~ #&gt; x10 1.000 #&gt; wx11 =~ #&gt; x11 1.000 #&gt; wy1 =~ #&gt; y1 1.000 #&gt; wy2 =~ #&gt; y2 1.000 #&gt; wy3 =~ #&gt; y3 1.000 #&gt; wy4 =~ #&gt; y4 1.000 #&gt; wy5 =~ #&gt; y5 1.000 #&gt; wy6 =~ #&gt; y6 1.000 #&gt; wy7 =~ #&gt; y7 1.000 #&gt; wy8 =~ #&gt; y8 1.000 #&gt; wy9 =~ #&gt; y9 1.000 #&gt; wy10 =~ #&gt; y10 1.000 #&gt; wy11 =~ #&gt; y11 1.000 #&gt; #&gt; Regressions: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; RIx ~ #&gt; oplx (ex) 0.263 0.002 110.425 0.000 #&gt; RIy ~ #&gt; oply (ey) 0.290 0.002 122.043 0.000 #&gt; RIsx ~ #&gt; oplx (fx) 0.001 0.000 4.283 0.000 #&gt; RIsy ~ #&gt; oply (fy) -0.001 0.000 -2.612 0.009 #&gt; x2 ~ #&gt; x1 (ax) -0.024 0.008 -2.870 0.004 #&gt; y1 (bx) 0.059 0.008 7.391 0.000 #&gt; x3 ~ #&gt; x2 (ax) -0.024 0.008 -2.870 0.004 #&gt; y2 (bx) 0.059 0.008 7.391 0.000 #&gt; x4 ~ #&gt; x3 (ax) -0.024 0.008 -2.870 0.004 #&gt; y3 (bx) 0.059 0.008 7.391 0.000 #&gt; x5 ~ #&gt; x4 (ax) -0.024 0.008 -2.870 0.004 #&gt; y4 (bx) 0.059 0.008 7.391 0.000 #&gt; x6 ~ #&gt; x5 (ax) -0.024 0.008 -2.870 0.004 #&gt; y5 (bx) 0.059 0.008 7.391 0.000 #&gt; x7 ~ #&gt; x6 (ax) -0.024 0.008 -2.870 0.004 #&gt; y6 (bx) 0.059 0.008 7.391 0.000 #&gt; x8 ~ #&gt; x7 (ax) -0.024 0.008 -2.870 0.004 #&gt; y7 (bx) 0.059 0.008 7.391 0.000 #&gt; x9 ~ #&gt; x8 (ax) -0.024 0.008 -2.870 0.004 #&gt; y8 (bx) 0.059 0.008 7.391 0.000 #&gt; x10 ~ #&gt; x9 (ax) -0.024 0.008 -2.870 0.004 #&gt; y9 (bx) 0.059 0.008 7.391 0.000 #&gt; x11 ~ #&gt; x10 (ax) -0.024 0.008 -2.870 0.004 #&gt; y10 (bx) 0.059 0.008 7.391 0.000 #&gt; y2 ~ #&gt; x1 (by) 0.081 0.008 9.788 0.000 #&gt; y1 (ay) -0.034 0.008 -4.213 0.000 #&gt; y3 ~ #&gt; x2 (by) 0.081 0.008 9.788 0.000 #&gt; y2 (ay) -0.034 0.008 -4.213 0.000 #&gt; y4 ~ #&gt; x3 (by) 0.081 0.008 9.788 0.000 #&gt; y3 (ay) -0.034 0.008 -4.213 0.000 #&gt; y5 ~ #&gt; x4 (by) 0.081 0.008 9.788 0.000 #&gt; y4 (ay) -0.034 0.008 -4.213 0.000 #&gt; y6 ~ #&gt; x5 (by) 0.081 0.008 9.788 0.000 #&gt; y5 (ay) -0.034 0.008 -4.213 0.000 #&gt; y7 ~ #&gt; x6 (by) 0.081 0.008 9.788 0.000 #&gt; y6 (ay) -0.034 0.008 -4.213 0.000 #&gt; y8 ~ #&gt; x7 (by) 0.081 0.008 9.788 0.000 #&gt; y7 (ay) -0.034 0.008 -4.213 0.000 #&gt; y9 ~ #&gt; x8 (by) 0.081 0.008 9.788 0.000 #&gt; y8 (ay) -0.034 0.008 -4.213 0.000 #&gt; y10 ~ #&gt; x9 (by) 0.081 0.008 9.788 0.000 #&gt; y9 (ay) -0.034 0.008 -4.213 0.000 #&gt; y11 ~ #&gt; x10 (by) 0.081 0.008 9.788 0.000 #&gt; y10 (ay) -0.034 0.008 -4.213 0.000 #&gt; #&gt; Covariances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; wx1 ~~ #&gt; wy1 0.131 0.016 8.063 0.000 #&gt; wx2 ~~ #&gt; wy2 0.052 0.013 4.095 0.000 #&gt; wx3 ~~ #&gt; wy3 0.061 0.013 4.836 0.000 #&gt; wx4 ~~ #&gt; wy4 0.035 0.012 2.899 0.004 #&gt; wx5 ~~ #&gt; wy5 0.019 0.012 1.592 0.111 #&gt; wx6 ~~ #&gt; wy6 0.060 0.012 5.030 0.000 #&gt; wx7 ~~ #&gt; wy7 0.051 0.011 4.469 0.000 #&gt; wx8 ~~ #&gt; wy8 0.046 0.012 3.816 0.000 #&gt; wx9 ~~ #&gt; wy9 0.041 0.013 3.183 0.001 #&gt; wx10 ~~ #&gt; wy10 0.052 0.014 3.843 0.000 #&gt; wx11 ~~ #&gt; wy11 0.054 0.016 3.406 0.001 #&gt; .RIx ~~ #&gt; .RIy 0.966 0.044 21.726 0.000 #&gt; .RIsx ~~ #&gt; .RIsy -0.000 0.000 -1.221 0.222 #&gt; .RIx ~~ #&gt; .RIsx -0.026 0.004 -7.036 0.000 #&gt; .RIy ~~ #&gt; .RIsy -0.026 0.004 -7.098 0.000 #&gt; .RIx ~~ #&gt; .RIsy -0.006 0.003 -1.819 0.069 #&gt; .RIy ~~ #&gt; .RIsx -0.002 0.003 -0.509 0.611 #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; .RIx 0.000 #&gt; .RIy 0.000 #&gt; .RIsx 0.000 #&gt; .RIsy 0.000 #&gt; wx1 0.000 #&gt; wx2 0.000 #&gt; wx3 0.000 #&gt; wx4 0.000 #&gt; wx5 0.000 #&gt; wx6 0.000 #&gt; wx7 0.000 #&gt; wx8 0.000 #&gt; wx9 0.000 #&gt; wx10 0.000 #&gt; wx11 0.000 #&gt; wy1 0.000 #&gt; wy2 0.000 #&gt; wy3 0.000 #&gt; wy4 0.000 #&gt; wy5 0.000 #&gt; wy6 0.000 #&gt; wy7 0.000 #&gt; wy8 0.000 #&gt; wy9 0.000 #&gt; wy10 0.000 #&gt; wy11 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; .RIx 1.724 0.058 29.671 0.000 #&gt; .RIy 1.534 0.053 28.805 0.000 #&gt; .RIsx 0.003 0.000 9.076 0.000 #&gt; .RIsy 0.003 0.000 9.228 0.000 #&gt; wx1 0.443 0.022 20.212 0.000 #&gt; wy1 0.422 0.021 20.054 0.000 #&gt; wx2 0.356 0.017 21.061 0.000 #&gt; wy2 0.355 0.017 21.063 0.000 #&gt; wx3 0.365 0.016 22.428 0.000 #&gt; wy3 0.386 0.017 22.367 0.000 #&gt; wx4 0.348 0.016 21.685 0.000 #&gt; wy4 0.311 0.015 21.201 0.000 #&gt; wx5 0.339 0.015 22.638 0.000 #&gt; wy5 0.345 0.015 22.758 0.000 #&gt; wx6 0.360 0.016 23.032 0.000 #&gt; wy6 0.348 0.015 22.762 0.000 #&gt; wx7 0.365 0.016 22.850 0.000 #&gt; wy7 0.301 0.014 22.120 0.000 #&gt; wx8 0.333 0.015 22.202 0.000 #&gt; wy8 0.366 0.016 22.401 0.000 #&gt; wx9 0.356 0.017 21.419 0.000 #&gt; wy9 0.357 0.017 21.258 0.000 #&gt; wx10 0.322 0.017 19.326 0.000 #&gt; wy10 0.345 0.018 19.361 0.000 #&gt; wx11 0.401 0.021 19.206 0.000 #&gt; wy11 0.337 0.019 17.681 0.000 #&gt; .x1 0.000 #&gt; .x2 0.000 #&gt; .x3 0.000 #&gt; .x4 0.000 #&gt; .x5 0.000 #&gt; .x6 0.000 #&gt; .x7 0.000 #&gt; .x8 0.000 #&gt; .x9 0.000 #&gt; .x10 0.000 #&gt; .x11 0.000 #&gt; .y1 0.000 #&gt; .y2 0.000 #&gt; .y3 0.000 #&gt; .y4 0.000 #&gt; .y5 0.000 #&gt; .y6 0.000 #&gt; .y7 0.000 #&gt; .y8 0.000 #&gt; .y9 0.000 #&gt; .y10 0.000 #&gt; .y11 0.000 #&gt; #&gt; Defined Parameters: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; dif1 0.010 0.012 0.816 0.415 #&gt; dif2 -0.022 0.012 -1.787 0.074 #&gt; dif3 -0.027 0.002 -10.782 0.000 #&gt; dif4 0.001 0.000 5.018 0.000 3.6.8 Summary results hypo2 Table 3.5: Results Hypo2 CLPM RI-CLPM SC-RI-CLPM LT-RI-CLPM paths est se pvalue est se pvalue est se pvalue est se pvalue eu-integration Men:stability 0.60 0.01 0 0.13 0.01 0.00 0.22 0.01 0 0.08 0.01 0.00 Women:stability 0.52 0.01 0 0.10 0.01 0.00 0.19 0.01 0 0.07 0.01 0.00 Men:partner-effect 0.16 0.01 0 0.00 0.01 0.67 0.06 0.01 0 0.02 0.01 0.03 Women:partner-effect 0.15 0.01 0 0.03 0.01 0.00 0.08 0.01 0 0.03 0.01 0.00 immigrants Men:stability 0.57 0.01 0 0.06 0.01 0.00 0.10 0.01 0 0.05 0.01 0.00 Women:stability 0.57 0.01 0 0.06 0.01 0.00 0.10 0.01 0 0.04 0.01 0.00 Men:partner-effect 0.18 0.01 0 0.03 0.01 0.00 0.05 0.01 0 0.03 0.01 0.00 Women:partner-effect 0.15 0.01 0 0.01 0.01 0.26 0.04 0.01 0 0.02 0.01 0.08 euthanasia Men:stability 0.66 0.01 0 -0.03 0.01 0.00 0.15 0.01 0 -0.06 0.01 0.00 Women:stability 0.70 0.01 0 -0.02 0.01 0.00 0.14 0.01 0 -0.06 0.01 0.00 Men:partner-effect 0.19 0.01 0 0.06 0.01 0.00 0.17 0.01 0 0.08 0.01 0.00 Women:partner-effect 0.17 0.01 0 0.06 0.01 0.00 0.16 0.01 0 0.08 0.01 0.00 income_diff Men:stability 0.61 0.01 0 0.02 0.01 0.04 0.16 0.01 0 -0.02 0.01 0.00 Women:stability 0.54 0.01 0 0.00 0.01 0.69 0.12 0.01 0 -0.03 0.01 0.00 Men:partner-effect 0.14 0.01 0 0.04 0.01 0.00 0.14 0.01 0 0.06 0.01 0.00 Women:partner-effect 0.15 0.01 0 0.05 0.01 0.00 0.13 0.01 0 0.08 0.01 0.00 3.6.9 Conclusion hypo2 3.7 Assignment Look in the literature for other measures of opinion homophily. Try to apply this measure to construct a similar table as 3.5. Have a look at the detailed results of the RI-CLPM estimated for hypothesis 1. Could you conclude, based on the error-covariance and/or error-correlation between the opinions of the spouses that opinion homophily increased? Motivate your answer. Have a look at the detailed results of the LT-RI-CLPM and focus on the variance and covariance of the random slopes of the partners. What does this tell you about opinion homophily within partners? Motivate your answer. You see that sections 3.6.6 and 3.6.9 are empty. Please fill in the blanks. Motivate your answer and discuss both selection and influence. You could argue that influence is only possible if people differ initially. Please select couples who are dissimilar at within-time 1 and do the descriptive and analysis part again. Of course, of course, not for all dependent variables and modelling specifications. Pick one dependent and focus on the RI-CLPM. Please test if influence processes depend on educational attainment. Thus, formulate a hypothesis and test this hypothesis. References "],["data-1.html", "Chapter 4 Data 4.1 Sampling 4.2 Ethical considerations 4.3 Measurement", " Chapter 4 Data 4.1 Sampling 4.2 Ethical considerations 4.3 Measurement "],["theory-1.html", "Chapter 5 Theory 5.1 Network Structures (TRIAD) 5.2 Type of explanations 5.3 Causes 5.4 Consequences 5.5 Assignment", " Chapter 5 Theory We already introduced egonets in section 1.4.1. We defined an egonet as the set of ties surrounding sampled individual units. (cf. Marsden 1990). Thus we have an ego with ties to one or more alters. Suppose that after we have identified the alters of ego, we asked ego the following follow-up question: How close are these people to each other? 1.very close 2.not close, but not total strangers to each other either 3.total strangers to each other 4.I dont know In this situation we also have information on ties between egos alters and, subsequently, we may observe triads between our alters. In the figure below I have made the triad green. Figure 5.1: A Triad within an egonet Let us discuss TRIADS in some more detail first, before we move on with our discussion on Egonets. 5.1 Network Structures (TRIAD) Let us start with all possible triad configurations if we have a (binary) undirected tie. See Figure 5.2. Figure 5.2: undirected triad configurations We observe an unconnected triad, a triad with a connected pair, an open triad and a closed triad. The open triad is also called a forbidden triad and actor i in such a triad is said to hold a brokerage position. How many isomorphs can you think of for a triad with one connected pair? We only have three nodes but network structures become complex quite quickly. See Figure 5.3 for the many possible configurations for triads when we consider two different type of undirected ties (i.e. multiplexity). Figure 5.3: Multiplex, undirected triad configurations By now, I am a bit tired of drawing all these nodes and relations. Luckily, the net is full of pictures of the possible directed triad configurations. I stole this one from an online workshop on Social Network Analysis for Anthropologists here. Figure 5.4: Directed triad configurations These triads all have unique names: last digit: number of dyads without ties second digit: number of dyads with one tie first digit: number of dyads with two ties (mutual dyads) specific subtype: C: cyclic D: downward U: upward T: transitive This triad census has been developed by Davis and Leinhardt (1967) and their original picture in the paper is too cool not to show here: Figure 5.5: Original Triad census by Davis and Leinhard (1967) Suppose we are trying to come up with an explanation for why we observe transitive triads (030T) in our network. We must realize that a transitive dyad may be the outcome of different evolution processes. See Figure 5.6. Figure 5.6: Different pathways to a transitive triad. That is, if we assume that each tie is made subsequently, thus not two tie are created at the same time.7 The reason why i closes the triad, may be very different from the reason why k closes the triad. It all depends on the social relation under consideration. The take home message is that we need longitudinal data if we would like to disentangle specific explanations. See Figure 5.6. Do you think both pathways are just as likely for (a) friendship relations, (b) who kicks whom relations and (c) who explains social network analysis to whom? relations? I could not find a nice picture of all possible directed triad configurations for two relations simultaneously. If you have time on your hands, please make one for me!  So, let us go back to egonets. 5.2 Type of explanations If social scientists seek explanations for why we observe specific social networks the explanations generally refer to four aspects, or theoretical dimensions, of social networks, namely: size: the number of nodes in the network structure: the relations in the network composition: characteristics of the nodes in the network evolution: change in size, structure and/or composition network growth tie evolution: structure &gt; structure node evolution: node attributes &gt; node attributes influence: structure &gt; node attributes selection: node attributes &gt; structure Where 1, 2 and 3 belong to the causes of social networks, evolution processes belong the the consequences of social networks. 5.3 Causes 5.3.1 Size A dyad is by definition constituted by just two nodes. The size of an egonet may vary. In section 1.4.1 we introduced the egonet formed by our Best Friend Forevers (BFF). Naturally, we dont have many BFFs. Perhaps it is even a bad example because dont you have at most only one BFF by definition? That said, my daughter (6 years old) claims that all her classmates are BFFs. Anyways. It turns out that if we ask a random sample of adults the following question From time to time, most people discuss important matters with other people. Looking back over the last six monthswho are the people with whom you discussed matters important to you? that there are not many people naming more than five persons.8 For example, have a look at the table below. Data is from a dataset called CrimeNL (Tolsma et al. 2015). Table 5.1: Number of confidants in CDN (row %) Zero One Two Three Four Five Mean SD All confidants 17.71 31.48 23.71 14.45 6.65 6.00 1.79 1.39 Higher educated confidants 39.91 31.53 15.26 7.69 3.50 2.11 1.10 1.23 Source: CrimeNL N=3.834 (own calculations) The network of our so-called confidants is called the Core-Discussion-Network (CDN). Our CDN network thus commonly consists of maximum 5 confidants. The same holds true if we would ask about our loved ones. 5.3.1.1 Dunbars number How would we get to know people with whom you form meaningful relations. That is, the people with whom you form stable social relations with and of whom you know how everyone is connected to one another. Perhaps we could use the following question: Who would you not feel embarrassed about joining uninvited for a drink if you happened to bump into them in a bar? My answer would definitely depend on whether it was asked to me before or after corona.  Let try a different question: Who do you send a Christmas card? Mind you, this question to tap into your meaningful relations was constructed before e-cards existed. According to Robin Dunbar most people are able to maintain stable social relations with approximately 100-200 people, with 150 being a typical number and it is hence known as Dunbars number (Dunbar 2010; Dunbar et al. 2015). According to Dunbar each layer of our social network has a typical size, where the size of each layer increasing as emotional closeness decreases: loved ones: 5 good friends: 15 friends: 50 meaningful contacts: 150 acquaintances: 500 people you can recognize: 1500 5.3.1.2 online friends Many people are active on online social networks like FaceBook, Instagram, Strava or what have you. According to this site, approximately 40% of U.S Facebook users in the United States (in 2016) had between 0-200 friends, 38% 200-500 friends and 21% 500+ friends. There are several crucial differences between the connections we have online versus offline. First of all, it does not cost many resources to make and maintain an online friendship. It may require some social media skills though, which I found out the hard way. After having joined FB at a time when youngster were already moving on to other online communities, it annoyed me to see all kind of uninteresting stories of distant relatives about their cats. I consequently decided to unfriend these persons. This was not appreciated by some other relatives. It turned out I should simply have hidden their content from my timeline. There apparently is a social norm not to unfriend people on FB. We already discussed that selection processes should be seen as distinct from deselection processes. I would argue that especially with respect to online social relations within the selection part we need to distinguish processes explaining sending friendship invitations and accepting/declining friendship invitations. But notwithstanding these differences, online social networks consist of a series of embedded layers just as offline personal social networks. 5.3.1.3 weak ties Social network analysts have struggled how to measure weaker social network relationships, and, by extension, extended social network size for quite a while. We discuss the scaling-up method in more detail here 7.3.1. 5.3.2 Composition to do Homophily relation to social capital literature 5.3.3 Structure In this section we will discuss several network measures. In this chapter we focus on egonets but many measures are also relevant for complete networks! To illustrate some different ways how we could describe egonets we will use egonets based on co-authorships. We start with randomly sampling two social scientists from the total pool of all social scientists. Currently rolling a diceand who did we sample: Bas Hofstra Jochem Tolsma From these two sampled social scientists we will use the webscraping techniques described in Chapter 11 to collect 1.5 degree co-author egonetwork.9 See the figures 5.7 and @ref(fig:bh ) below for a graphical summary of the networks. Figure 5.7: 1.5 degree co-author egonetwork of JOCHEM TOLSMA Figure 5.8: 1.5 degree co-author egonetwork of BAS HOFSTRA require(rvest) page &lt;- read_html(&quot;https://scholar.google.com/citations?view_op=list_colleagues&amp;hl=en&amp;user=K51iiIAAAAAJ&quot;) Coauthors &lt;- page %&gt;% html_nodes(css=&quot;a&quot;) %&gt;% html_text() affiliation &lt;- page %&gt;% html_nodes(css=&quot;.gs_ai_aff&quot;) %&gt;% html_text() get_scholar_id_fix(last_name=&quot;van%der%Brug&quot;, first_name=&quot;Wouter&quot;) get_scholar_id(last_name=&quot;van der Brug&quot;, first_name=&quot;Wouter&quot;) %&gt;% html_attr(&quot;id&quot;) id &lt;- page %&gt;% html_nodes(css=&quot;div.gsc_ucoar.gs_scl&quot;) %&gt;% html_attr(&quot;id&quot;) affiliation &lt;- gsub(pattern=&quot;gsc_ucoar-&quot;, replacement=&quot;&quot;, x=affiliation ) https://scholar.google.com/citations?view_op=list_colleagues&amp;hl=en&amp;user=gHuTzXcAAAAAJ&quot; %&gt;% html_text() affiliation &lt;- page %&gt;% html_nodes(css=&quot;div&quot;) %&gt;% html_text() Coauthors &lt;- as.data.frame(Coauthors) Coauthors Density Density is defined as all observed relations divided by all possible relations. Look at the examples below. Are you able to calculate the density of the networks yourself? Figure 5.9: Different densities? The density in Bas network turns out to be: 0.24. The density in Jochems network turns out to be: 0.23. For comparison, if we look at friendship networks among pupils in classrooms, we generally observe a density within the range of .2 and .4. Degree centrality Closely related to density is the concept of degree. The number of ingoing (indegree), outgoing (outdegree) or undirected (degree) relations from each node. In real social networks, we generally observe a right-skewed degree distribution (most people have some friends, few people have many friends). Centrality measures, like degree, can be measured at the node-level. For the graph as a whole, we may calculate the average centrality score but every node-level centrality measure also has its specific graph-level analogue. In what follows we focus on node-level centrality scores. At the node-level we may calculate the raw measure but to facilitate interpretation we will use normalized measures. There may be more than one way by which the raw scores can be normalized. If you use an R package to calculate normalized centrality scores (e.g. igraph), please be aware of the applied normalization. People in a network with relatively many degree are called more central and (normalized) degree centrality is formally defined as: \\[ C_D(v_i) = \\frac{deg(v_i) - min(deg(v))} {max(deg(v)) - min(deg(v))}, \\] where \\(C_D(v_i)\\) is degree centrality of \\(v_i\\), vertex i, and deg stands for degree. \\(max(deg(v))\\) is the maximal observed degree. \\(min(deg(v))\\) is the minimal observed degree. A different normalization approach would be to divide the node degree by the maximum degree (either the theoretical maximum, or the maximal observed degree). Suppose you want to compare the degree centrality of all co-authors in Bas network. Which normalization approach would you take? Suppose you want to compare the degree centrality of Bas in Bass network with the degree centrality of Jochem in Jochems network. Which normalization approach would you take. Closeness centrality Closely related to degree centrality is (normalized) closeness centrality: \\[ C_C(v_i) = \\frac{N}{\\sum_{j}d(v_j, v_i)}, \\] with N the number of nodes and d stands for distance. Betweenness centrality A final important measure of centrality I would like to discuss is called betweenness. It is defined as: \\[ C_B(v_i) = \\frac{\\sigma_{v_j,v_k}(v_i)}{\\sum_{j\\neq k\\neq i}\\sigma(v_j,v_k)}, \\] where \\(\\sigma(v_j,v_k)\\) is the number of shortest paths between vertices j and k, \\(\\sigma_{v_j,v_k}(v_i)\\), are the number of these shortest paths that pass through vertex \\(v_i\\) . One way to normalize this measure is as follows: \\[ C_{B_{normalized}}(v_i) = \\frac{C_B(v_i) - min(C_B(v))}{max(C_B(v))-min(C_B(v))} \\] Clustering Clustering is an interesting concept. We have immediately an intuitive understanding of it, people lump together in separate groups. But how should we go about defining it more formally? The clustering coefficient for \\(v_i\\) is defined as the observed ties between all direct neighbors of \\(v_i\\) divided by all possible ties between all direct neighbors of \\(v_i\\). Direct neighbours are connected to \\(v_i\\) via an ingoing and/or outgoing relation. For undirected networks, the clustering coefficient is the same as the transitivity index: the number of transitive triads divided by all possible transitive triads. For directed graphs not so. Bas transitivity network in his network turns out to be: 0.13. Jochems transitivity in his network turns out to be: 0.15. 5.3.4 Stability TO DO: upload lecture 5.4 Consequences Please brush off your knowledge on dyadic influence processes (see section 2.3.2). What is the added complexity of egonets? Phrased otherwise, why would some egonets exert more influence than other egonets? The size of egonets may differ (i.e. node set). The structure of egonets may differ (i.e. tie set). The composition of egonetes may differ (i.e. attribute set). The evolution (stability) of egonets may differ. Surprisingly, there is relatively little literature on influence processes going on in egonets. The literature is mainly concerned with dyadic influence process or on influence processes going on in socionets. We also need to be aware that the debate on the consequences of egonets is dominated by researchers interest in the topic of social capital. My definition of social capital is: Social capital is the extent to which our egocentric networks gives us access to different forms of capital or resources which we may use to our own benefit. Please compare my definition with Nan Lins definition: The resources embedded in a social structure that are accessed and/or mobilized in purposive actions. (Lin 2002). Social capital is one of the most heavily disputed concepts in the social sciences. For more definitions see socialcapitalresearch.com. The literature on how egonets may influence our opinions and attitudes is sparse. We will discuss egonet influence processes during class and I will upload a lecture with my take on this asap. TO DO: upload lecture 5.5 Assignment Please prepare a short (5-10 min.) presentation on recent developments in the literature that deals with causes for egonets. More specifically pick one topic: egonets and size (i.e. node set). Start with Paik and Sanchagrin (2013). egonets and structure (i.e. tie set). Start with Adamic and Adar (2003). egonets and composition (i.e. attribute set) Start with Hofstra et al. (2017). egonets and evolution / stability. Start with Small, Pamphile, and McMahan (2015). References "],["methods-1.html", "Chapter 6 Methods 6.1 Causes 6.2 Consequences 6.3 Research questions 6.4 Data 6.5 Disaggregation method 6.6 Aggregation method 6.7 Micro-macro model 6.8 Random Intercept Cross-Lagged Micro-Macro Model RI-CLP-MM 6.9 Assignment", " Chapter 6 Methods .button1 { background-color: #f44336; /* Red */ border: none; color: white; padding: 15px 32px; text-align: center; text-decoration: none; display: inline-block; font-size: 16px; margin: 4px 2px; cursor: pointer; } .button1:hover { box-shadow: 0 12px 16px 0 rgba(0,0,0,0.24), 0 17px 50px 0 rgba(0,0,0,0.19); } .button1 {border-radius: 12px;} .button1 {width: 100%;} 6.1 Causes 6.2 Consequences In this part we we start with estimating a micro-macro model. See Figure 6.1 which I adapted from Bennink et al. (2016) for the basic idea. We have to realize that our data has a hierarchical structure: observations (confidants/ties) at the lowest-level (level-one, micro-level, tie-level or confidant-level) are nested in a higher level (level-two, macro-level, network-level or ego-level) and that these observations at the confidant-level are interdependent. We need to take these interdependencies into account. Moreover, if we wish to relate characteristics of our CDN to characteristics of our egos - and yes that is our wish in this section -, our dependent variable is at the macro-level and we have to estimate a micro-macro model. Please read Croon and Veldhoven (2007) and Bennink et al. (2016).10 Figure 6.1: Basic micro-macro model note: Adapted from Bennink et al. (2016) In chapter 3 we investigated how spouses influence each others political opinion. In this chapter we continue our discussion but now with respect to our confidants. Suppose we want to investigate how our confidants influence our political opinions. Unfortunately, not many surveys that map the Core Discussion Network include name interpreter questions on the political opinions of the named confidants. However, one of the most important determinant for someones political opinion is his/her educational attainment. There are several theoretical reasons why the educational attainment of our confidants would impact our own opinions. To mention just a few: Education of alter is a proxy of alters opinions and alters opinions may influence our opinions. Alters with different educational levels have different life experiences and the life experiences of our alters may influence our opinions. Alters with different educational levels have different knowledge on topics, sharing knowledge on these topics may influence our opinions. 6.3 Research questions This leads to the following research question: To what extent does the educational level of our confidants influence our political opinions? To what extent does the impact of the educational level of our confidants on our political opinion depend on: ego characteristics (e.g. educational level)? characteristics of our Core Discussion Network as a whole (e.g size)? other characteristics of our confidants (e.g age or gender)? For each ego (at each time point) we may have information on one to five confidants. As already stated above, these observations are interdependent and we need to take this into account. Naturally, we also need to be aware that our own educational-level (and political opinion) will influence with whom we discuss important matters. Thus, we need to take into account selection effects. 6.4 Data We will use the data from the LISS panel. More concretely, we will use: 11 waves (2008-2014, 2016-2019) Filter on respondents older than 25. We have already constructed a dataset for you guys and gals to work with which contains information on more than 13000 respondents. Dont forget it is a panel data set. This means we have more observations for the same respondent (and his/her CDN) over time. Please download this data file to your working directory. liss_cdn 6.4.1 Variables Variables of interest and value labels: Ego-level: ego_id educ gender age eu: opinion of ego on eu_integration: 0 = eu integration has gone too far / 4 = eu eu_integration: 0 = eu integration has gone too far / 4 = eu integration should go further immigrants: 0 = immigrants should adjust / 4 immigrants can retain their own culture. euthanasia: 1 = euthanasia should be forbidden / 5 euthanasia should be permitted income_diff: 1 differences in income should increase / 5 differences in income should decrease confidant-level: educ_alterx gender_alterx age_alterx The x refers to the x-mentioned confidant (1-5). In the wide dataset each variable ends with .y where .y refers to the survey wave. Thus educ_alter4.9 refers to the educational level in years of the fourth mentioned confidant in survey_wave 9 (i.e. 2017). For the original variables in Dutch see below: EU integratie De Europese integratie is te ver gegaan. 1 Helemaal oneens 2 Oneens 3 Niet eens, niet oneens 4 Eens 5 Helemaal eens opleiding Hoogste opleiding met diploma 1 basisonderwijs 2 vmbo 3 havo/vwo 4 mbo 5 hbo 6 wo 7 anders 8 (Nog) geen onderwijs afgerond 9 Volgt nog geen onderwijs Hierbij hebben wij opleiding gecategoriseerd in drie groepen: 1. Laag: basisonderwijs en vmbo 2. Midden: havo/vwo en mbo 3. Hoog: hbo en wo We nemen enkel mensen van 25 jaar en ouder mee. Van hen verwachten we dat ze klaar zijn met hun onderwijscarriere. 6.4.2 Preperation #### clean the environment ####. rm(list=ls()) #### packages ####. require(tidyverse) require(lavaan) ##### Data input ###. load(&#39;addfiles/liss_cdn.Rdata&#39;) liss_l &lt;- liss_cdn[[1]] liss_w &lt;- liss_cdn[[2]] Let us for now focus on the last wave. Thus wave 2019 (wave 11). In the literature two approaches are discussed to estimate a micro-macro model, a persons as variables approach and a multi-level approach. The persons as variables approach is - I hope - easiest to implement and for that we need the data in wide format (one row for each respondent). The idea is that the alter scores load on a latent variable at the ego-level. This latent variable has a random component at the ego-level (cf. random intercept in multi-level models). In a basic model with continous manifest variables at the micro-level, the latent variable at the macro-level is the (biased corrected) mean. 6.5 Disaggregation method But first let us estimate the wrong models. We will start with a disaggregation approach. We need to disaggregate our data so that each row refers to a specific combination of ego, survey_wave and alter. #we need to disaggregate our data. thus each ego, wave, alter per row. liss_ll &lt;- rbind(liss_l,liss_l,liss_l,liss_l,liss_l) liss_ll$index_alter &lt;- rep(1:5, each=length(liss_l[,1])) liss_ll$educ_alter &lt;- NA liss_ll$educ_alter &lt;- ifelse(liss_ll$index_alter == 1, liss_ll$educ_alter1, liss_ll$educ_alter) liss_ll$educ_alter &lt;- ifelse(liss_ll$index_alter == 2, liss_ll$educ_alter2, liss_ll$educ_alter) liss_ll$educ_alter &lt;- ifelse(liss_ll$index_alter == 3, liss_ll$educ_alter3, liss_ll$educ_alter) liss_ll$educ_alter &lt;- ifelse(liss_ll$index_alter == 4, liss_ll$educ_alter4, liss_ll$educ_alter) liss_ll$educ_alter &lt;- ifelse(liss_ll$index_alter == 5, liss_ll$educ_alter5, liss_ll$educ_alter) liss_ll_sel &lt;- liss_ll %&gt;% filter(survey_wave==11) model1 &lt;- &#39; euthanasia ~ educ_alter euthanasia ~ 1 euthanasia ~~ euthanasia &#39; fit1 &lt;- lavaan(model1, data = liss_ll_sel) summary(fit1) ## lavaan 0.6-9 ended normally after 9 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 3 ## ## Used Total ## Number of observations 10923 29925 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## euthanasia ~ ## educ_alter 0.009 0.003 2.726 0.006 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .euthanasia 4.335 0.042 102.363 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .euthanasia 0.921 0.012 73.902 0.000 6.6 Aggregation method We could also try to aggregate our confidant data. This means we calculate the mean educational level of our confidants solely based on the available data in the observed scores. liss_l &lt;- liss_l %&gt;% mutate(educ_alter_mean = rowMeans(cbind(educ_alter1, educ_alter2, educ_alter3, educ_alter4,educ_alter5), na.rm = TRUE)) #calculate the mean educational level of the alters. liss_l_sel &lt;- liss_l %&gt;% filter(survey_wave==11) model1 &lt;- &#39; euthanasia ~ educ_alter_mean euthanasia ~ 1 euthanasia ~~ euthanasia &#39; fit2 &lt;- lavaan(model1, data = liss_l_sel, missing = &quot;fiml&quot;) summary(fit2) ## lavaan 0.6-9 ended normally after 12 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 3 ## ## Used Total ## Number of observations 3743 5985 ## Number of missing patterns 2 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Observed ## Observed information based on Hessian ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## euthanasia ~ ## educ_alter_men 0.016 0.008 2.020 0.043 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .euthanasia 4.244 0.098 43.513 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .euthanasia 0.934 0.023 41.207 0.000 6.7 Micro-macro model Finally, let us estimate a better model. We will not use the observed mean value of the educational levels of the confidants for each ego but will calculate a bias corrected mean. liss_l_sel &lt;- liss_l %&gt;% filter(survey_wave==11) model &lt;- &quot; #latent variable FX =~ 1*educ_alter1 FX =~ 1*educ_alter2 FX =~ 1*educ_alter3 FX =~ 1*educ_alter4 FX =~ 1*educ_alter5 #variances educ_alter1 ~~ b*educ_alter1 educ_alter2 ~~ b*educ_alter2 educ_alter3 ~~ b*educ_alter3 educ_alter4 ~~ b*educ_alter4 educ_alter5 ~~ b*educ_alter5 FX ~~ FX euthanasia ~~ euthanasia #regression model euthanasia ~ FX euthanasia ~ 1 #intercepts/means educ_alter1 ~ e*1 educ_alter2 ~ e*1 educ_alter3 ~ e*1 educ_alter4 ~ e*1 educ_alter5 ~ e*1 &quot; fit3 &lt;- lavaan(model, data = liss_l_sel, missing = &quot;fiml&quot;, fixed.x = FALSE) summary(fit3) ## lavaan 0.6-9 ended normally after 22 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 14 ## Number of equality constraints 8 ## ## Used Total ## Number of observations 4922 5985 ## Number of missing patterns 50 ## ## Model Test User Model: ## ## Test statistic 46.488 ## Degrees of freedom 21 ## P-value (Chi-square) 0.001 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## FX =~ ## educ_alter1 1.000 ## educ_alter2 1.000 ## educ_alter3 1.000 ## educ_alter4 1.000 ## educ_alter5 1.000 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## euthanasia ~ ## FX 0.032 0.015 2.083 0.037 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .euthanasia 4.423 0.015 303.122 0.000 ## .educ_altr1 (e) 12.487 0.034 370.005 0.000 ## .educ_altr2 (e) 12.487 0.034 370.005 0.000 ## .educ_altr3 (e) 12.487 0.034 370.005 0.000 ## .educ_altr4 (e) 12.487 0.034 370.005 0.000 ## .educ_altr5 (e) 12.487 0.034 370.005 0.000 ## FX 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .educ_altr1 (b) 5.438 0.083 65.318 0.000 ## .educ_altr2 (b) 5.438 0.083 65.318 0.000 ## .educ_altr3 (b) 5.438 0.083 65.318 0.000 ## .educ_altr4 (b) 5.438 0.083 65.318 0.000 ## .educ_altr5 (b) 5.438 0.083 65.318 0.000 ## FX 2.322 0.099 23.418 0.000 ## .euthanasia 0.972 0.020 47.642 0.000 6.8 Random Intercept Cross-Lagged Micro-Macro Model RI-CLP-MM Of course we want to take into account selection effects. That is, egos opinion may also affect the educational level of his/her confidants. Luckily, you are very familiar by now with the RI-CLPM (if not, see section 3.6.2. Let us try to combine the micro-macro model with a RI-CLPM (lets call it an RI-CLP-MM). To illustrate I only use four waves: 6-9. 6.8.1 Measurement model We need to calculate the bias corrected means for each wave. I prefer to do that in a two-step procedure. myModel &lt;- &#39; FX6 =~ 1*educ_alter1.6 + 1*educ_alter2.6 + 1*educ_alter3.6 + 1*educ_alter4.6 + 1*educ_alter5.6 FX7 =~ 1*educ_alter1.7 + 1*educ_alter2.7 + 1*educ_alter3.7 + 1*educ_alter4.7 + 1*educ_alter5.7 FX8 =~ 1*educ_alter1.8 + 1*educ_alter2.8 + 1*educ_alter3.8 + 1*educ_alter4.8 + 1*educ_alter5.8 FX9 =~ 1*educ_alter1.9 + 1*educ_alter2.9 + 1*educ_alter3.9 + 1*educ_alter4.9 + 1*educ_alter5.9 #variances of latent variables FX6 ~~ FX6 FX7 ~~ FX7 FX8 ~~ FX8 FX9 ~~ FX9 #constrained variances of manifest variables educ_alter1.6 ~~ a*educ_alter1.6 educ_alter2.6 ~~ a*educ_alter2.6 educ_alter3.6 ~~ a*educ_alter3.6 educ_alter4.6 ~~ a*educ_alter4.6 educ_alter5.6 ~~ a*educ_alter5.6 educ_alter1.7 ~~ b*educ_alter1.7 educ_alter2.7 ~~ b*educ_alter2.7 educ_alter3.7 ~~ b*educ_alter3.7 educ_alter4.7 ~~ b*educ_alter4.7 educ_alter5.7 ~~ b*educ_alter5.7 educ_alter1.8 ~~ c*educ_alter1.8 educ_alter2.8 ~~ c*educ_alter2.8 educ_alter3.8 ~~ c*educ_alter3.8 educ_alter4.8 ~~ c*educ_alter4.8 educ_alter5.8 ~~ c*educ_alter5.8 educ_alter1.9 ~~ d*educ_alter1.9 educ_alter2.9 ~~ d*educ_alter2.9 educ_alter3.9 ~~ d*educ_alter3.9 educ_alter4.9 ~~ d*educ_alter4.9 educ_alter5.9 ~~ d*educ_alter5.9 #contrained intercepts of the manifest variables (structural changes are picked up by the latent variables) educ_alter1.6 ~ e*1 educ_alter2.6 ~ e*1 educ_alter3.6 ~ e*1 educ_alter4.6 ~ e*1 educ_alter5.6 ~ e*1 educ_alter1.7 ~ e*1 educ_alter2.7 ~ e*1 educ_alter3.7 ~ e*1 educ_alter4.7 ~ e*1 educ_alter5.7 ~ e*1 educ_alter1.8 ~ e*1 educ_alter2.8 ~ e*1 educ_alter3.8 ~ e*1 educ_alter4.8 ~ e*1 educ_alter5.8 ~ e*1 educ_alter1.9 ~ e*1 educ_alter2.9 ~ e*1 educ_alter3.9 ~ e*1 educ_alter4.9 ~ e*1 educ_alter5.9 ~ e*1 #free the means of the latent variables FX7 ~ 1 FX8 ~ 1 FX9 ~ 1 &#39; fit &lt;- lavaan(myModel, data = liss_w, missing = &#39;ML&#39;, fixed.x=FALSE, meanstructure = T) summary(fit, standardized = T) ## lavaan 0.6-9 ended normally after 38 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 47 ## Number of equality constraints 35 ## ## Used Total ## Number of observations 6582 13018 ## Number of missing patterns 1740 ## ## Model Test User Model: ## ## Test statistic 13075.965 ## Degrees of freedom 218 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## FX6 =~ ## educ_alter1.6 1.000 1.482 0.538 ## educ_alter2.6 1.000 1.482 0.538 ## educ_alter3.6 1.000 1.482 0.538 ## educ_alter4.6 1.000 1.482 0.538 ## educ_alter5.6 1.000 1.482 0.538 ## FX7 =~ ## educ_alter1.7 1.000 1.508 0.543 ## educ_alter2.7 1.000 1.508 0.543 ## educ_alter3.7 1.000 1.508 0.543 ## educ_alter4.7 1.000 1.508 0.543 ## educ_alter5.7 1.000 1.508 0.543 ## FX8 =~ ## educ_alter1.8 1.000 1.546 0.557 ## educ_alter2.8 1.000 1.546 0.557 ## educ_alter3.8 1.000 1.546 0.557 ## educ_alter4.8 1.000 1.546 0.557 ## educ_alter5.8 1.000 1.546 0.557 ## FX9 =~ ## educ_alter1.9 1.000 1.538 0.557 ## educ_alter2.9 1.000 1.538 0.557 ## educ_alter3.9 1.000 1.538 0.557 ## educ_alter4.9 1.000 1.538 0.557 ## educ_alter5.9 1.000 1.538 0.557 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .edc_ltr1.6 (e) 12.167 0.031 393.604 0.000 12.167 4.421 ## .edc_ltr2.6 (e) 12.167 0.031 393.604 0.000 12.167 4.421 ## .edc_ltr3.6 (e) 12.167 0.031 393.604 0.000 12.167 4.421 ## .edc_ltr4.6 (e) 12.167 0.031 393.604 0.000 12.167 4.421 ## .edc_ltr5.6 (e) 12.167 0.031 393.604 0.000 12.167 4.421 ## .edc_ltr1.7 (e) 12.167 0.031 393.604 0.000 12.167 4.386 ## .edc_ltr2.7 (e) 12.167 0.031 393.604 0.000 12.167 4.386 ## .edc_ltr3.7 (e) 12.167 0.031 393.604 0.000 12.167 4.386 ## .edc_ltr4.7 (e) 12.167 0.031 393.604 0.000 12.167 4.386 ## .edc_ltr5.7 (e) 12.167 0.031 393.604 0.000 12.167 4.386 ## .edc_ltr1.8 (e) 12.167 0.031 393.604 0.000 12.167 4.381 ## .edc_ltr2.8 (e) 12.167 0.031 393.604 0.000 12.167 4.381 ## .edc_ltr3.8 (e) 12.167 0.031 393.604 0.000 12.167 4.381 ## .edc_ltr4.8 (e) 12.167 0.031 393.604 0.000 12.167 4.381 ## .edc_ltr5.8 (e) 12.167 0.031 393.604 0.000 12.167 4.381 ## .edc_ltr1.9 (e) 12.167 0.031 393.604 0.000 12.167 4.406 ## .edc_ltr2.9 (e) 12.167 0.031 393.604 0.000 12.167 4.406 ## .edc_ltr3.9 (e) 12.167 0.031 393.604 0.000 12.167 4.406 ## .edc_ltr4.9 (e) 12.167 0.031 393.604 0.000 12.167 4.406 ## .edc_ltr5.9 (e) 12.167 0.031 393.604 0.000 12.167 4.406 ## FX7 0.057 0.043 1.343 0.179 0.038 0.038 ## FX8 0.199 0.044 4.472 0.000 0.129 0.129 ## FX9 0.248 0.045 5.520 0.000 0.161 0.161 ## FX6 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## FX6 2.195 0.089 24.725 0.000 1.000 1.000 ## FX7 2.273 0.085 26.832 0.000 1.000 1.000 ## FX8 2.390 0.093 25.606 0.000 1.000 1.000 ## FX9 2.366 0.095 24.867 0.000 1.000 1.000 ## .edc_ltr1.6 (a) 5.378 0.077 70.264 0.000 5.378 0.710 ## .edc_ltr2.6 (a) 5.378 0.077 70.264 0.000 5.378 0.710 ## .edc_ltr3.6 (a) 5.378 0.077 70.264 0.000 5.378 0.710 ## .edc_ltr4.6 (a) 5.378 0.077 70.264 0.000 5.378 0.710 ## .edc_ltr5.6 (a) 5.378 0.077 70.264 0.000 5.378 0.710 ## .edc_ltr1.7 (b) 5.423 0.072 75.373 0.000 5.423 0.705 ## .edc_ltr2.7 (b) 5.423 0.072 75.373 0.000 5.423 0.705 ## .edc_ltr3.7 (b) 5.423 0.072 75.373 0.000 5.423 0.705 ## .edc_ltr4.7 (b) 5.423 0.072 75.373 0.000 5.423 0.705 ## .edc_ltr5.7 (b) 5.423 0.072 75.373 0.000 5.423 0.705 ## .edc_ltr1.8 (c) 5.325 0.076 69.669 0.000 5.325 0.690 ## .edc_ltr2.8 (c) 5.325 0.076 69.669 0.000 5.325 0.690 ## .edc_ltr3.8 (c) 5.325 0.076 69.669 0.000 5.325 0.690 ## .edc_ltr4.8 (c) 5.325 0.076 69.669 0.000 5.325 0.690 ## .edc_ltr5.8 (c) 5.325 0.076 69.669 0.000 5.325 0.690 ## .edc_ltr1.9 (d) 5.259 0.078 67.621 0.000 5.259 0.690 ## .edc_ltr2.9 (d) 5.259 0.078 67.621 0.000 5.259 0.690 ## .edc_ltr3.9 (d) 5.259 0.078 67.621 0.000 5.259 0.690 ## .edc_ltr4.9 (d) 5.259 0.078 67.621 0.000 5.259 0.690 ## .edc_ltr5.9 (d) 5.259 0.078 67.621 0.000 5.259 0.690 We will extract the predicted values of the CFA and add them to our dataset liss_w. Lets have a look at the constructed variables. liss_w &lt;- data.frame(liss_w, predict(fit)) summary(liss_w$FX6) summary(liss_w$FX7) summary(liss_w$FX8) summary(liss_w$FX9) var(liss_w$FX6, na.rm=T) var(liss_w$FX7, na.rm=T) var(liss_w$FX8, na.rm=T) var(liss_w$FX9, na.rm=T) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -4.139 -0.483 0.000 0.000 0.262 2.573 6436 ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -4.156 -0.600 0.057 0.057 0.583 2.613 6436 ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -4.601 -0.331 0.199 0.199 0.499 2.713 6436 ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -4.196 -0.099 0.248 0.248 0.446 2.730 6436 ## [1] 0.7675647 ## [1] 0.9217957 ## [1] 0.8511977 ## [1] 0.795766 We thus observe an upward trend in the educational-level of the confidants of ego in subsequent waves. This could either be due to egos replacing lower educated confidants with higher educated confidants, due to the same confidants obtaining higher educational degrees over time or due to sample selection and that in subsequent waves more egos participate who happen to have higher educated confidants.11 6.8.2 The structural model RICLPM &lt;- &#39; # Create between components (random intercepts) RIx =~ 1*FX6 + 1*FX7 + 1*FX8 + 1*FX9 RIy =~ 1*euthanasia.6 + 1*euthanasia.7 + 1*euthanasia.8 + 1*euthanasia.9 # Create within-person centered variables wx6 =~ 1*FX6 wx7 =~ 1*FX7 wx8 =~ 1*FX8 wx9 =~ 1*FX9 wy6 =~ 1*euthanasia.6 wy7 =~ 1*euthanasia.7 wy8 =~ 1*euthanasia.8 wy9 =~ 1*euthanasia.9 # Estimate the lagged effects between the within-person centered variables. wx7 ~ a*wx6 + b*wy6 wx8 ~ a*wx7 + b*wy7 wx9 ~ a*wx8 + b*wy8 wy7 ~ c*wx6 + d*wy6 wy8 ~ c*wx7 + d*wy7 wy9 ~ c*wx8 + d*wy8 # Estimate the (residual) covariance between the within-person centered variables wx6 ~~ wy6 wx7 ~~ wy7 wx8 ~~ wy8 wx9 ~~ wy9 # Estimate the variance and covariance of the random intercepts. RIx ~~ RIx RIy ~~ RIy RIx ~~ RIy # Estimate the (residual) variance of the within-person centered variables. wx6 ~~ wx6 wy6 ~~ wy6 wx7 ~~ wx7 wy7 ~~ wy7 wx8 ~~ wx8 wy8 ~~ wy8 wx9 ~~ wx9 wy9 ~~ wy9 #include intercepts FX6 ~ 1 FX7 ~ 1 FX8 ~ 1 FX9 ~ 1 euthanasia.6 ~ 1 euthanasia.7 ~ 1 euthanasia.8 ~ 1 euthanasia.9 ~ 1 &#39; fit5 &lt;- lavaan(RICLPM, data=liss_w, missing = &quot;fiml.x&quot;, meanstructure = T ) summary(fit5, standardized = T) ## lavaan 0.6-9 ended normally after 43 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 35 ## Number of equality constraints 8 ## ## Used Total ## Number of observations 7199 13018 ## Number of missing patterns 31 ## ## Model Test User Model: ## ## Test statistic 118.791 ## Degrees of freedom 17 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## RIx =~ ## FX6 1.000 0.616 0.691 ## FX7 1.000 0.616 0.649 ## FX8 1.000 0.616 0.667 ## FX9 1.000 0.616 0.699 ## RIy =~ ## euthanasia.6 1.000 0.863 0.888 ## euthanasia.7 1.000 0.863 0.883 ## euthanasia.8 1.000 0.863 0.885 ## euthanasia.9 1.000 0.863 0.873 ## wx6 =~ ## FX6 1.000 0.645 0.723 ## wx7 =~ ## FX7 1.000 0.721 0.760 ## wx8 =~ ## FX8 1.000 0.688 0.745 ## wx9 =~ ## FX9 1.000 0.630 0.715 ## wy6 =~ ## euthanasia.6 1.000 0.446 0.459 ## wy7 =~ ## euthanasia.7 1.000 0.458 0.469 ## wy8 =~ ## euthanasia.8 1.000 0.455 0.466 ## wy9 =~ ## euthanasia.9 1.000 0.482 0.487 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## wx7 ~ ## wx6 (a) 0.209 0.013 15.988 0.000 0.187 0.187 ## wy6 (b) -0.006 0.020 -0.292 0.770 -0.004 -0.004 ## wx8 ~ ## wx7 (a) 0.209 0.013 15.988 0.000 0.219 0.219 ## wy7 (b) -0.006 0.020 -0.292 0.770 -0.004 -0.004 ## wx9 ~ ## wx8 (a) 0.209 0.013 15.988 0.000 0.228 0.228 ## wy8 (b) -0.006 0.020 -0.292 0.770 -0.004 -0.004 ## wy7 ~ ## wx6 (c) 0.006 0.010 0.614 0.539 0.009 0.009 ## wy6 (d) 0.075 0.017 4.324 0.000 0.073 0.073 ## wy8 ~ ## wx7 (c) 0.006 0.010 0.614 0.539 0.010 0.010 ## wy7 (d) 0.075 0.017 4.324 0.000 0.076 0.076 ## wy9 ~ ## wx8 (c) 0.006 0.010 0.614 0.539 0.009 0.009 ## wy8 (d) 0.075 0.017 4.324 0.000 0.071 0.071 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## wx6 ~~ ## wy6 -0.005 0.006 -0.825 0.409 -0.018 -0.018 ## .wx7 ~~ ## .wy7 -0.011 0.007 -1.581 0.114 -0.035 -0.035 ## .wx8 ~~ ## .wy8 -0.005 0.007 -0.728 0.467 -0.016 -0.016 ## .wx9 ~~ ## .wy9 0.002 0.006 0.335 0.738 0.006 0.006 ## RIx ~~ ## RIy 0.033 0.009 3.766 0.000 0.062 0.062 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .FX6 -0.000 0.011 -0.003 0.997 -0.000 -0.000 ## .FX7 0.057 0.012 4.868 0.000 0.057 0.060 ## .FX8 0.199 0.011 17.451 0.000 0.199 0.215 ## .FX9 0.248 0.011 22.852 0.000 0.248 0.282 ## .euthanasia.6 4.411 0.013 350.507 0.000 4.411 4.542 ## .euthanasia.7 4.431 0.013 349.557 0.000 4.431 4.536 ## .euthanasia.8 4.444 0.013 354.901 0.000 4.444 4.556 ## .euthanasia.9 4.411 0.013 341.314 0.000 4.411 4.464 ## RIx 0.000 0.000 0.000 ## RIy 0.000 0.000 0.000 ## wx6 0.000 0.000 0.000 ## .wx7 0.000 0.000 0.000 ## .wx8 0.000 0.000 0.000 ## .wx9 0.000 0.000 0.000 ## wy6 0.000 0.000 0.000 ## .wy7 0.000 0.000 0.000 ## .wy8 0.000 0.000 0.000 ## .wy9 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## RIx 0.380 0.010 36.770 0.000 1.000 1.000 ## RIy 0.744 0.015 50.581 0.000 1.000 1.000 ## wx6 0.416 0.010 41.960 0.000 1.000 1.000 ## wy6 0.199 0.006 31.333 0.000 1.000 1.000 ## .wx7 0.502 0.011 44.278 0.000 0.965 0.965 ## .wy7 0.209 0.007 30.909 0.000 0.995 0.995 ## .wx8 0.451 0.011 42.848 0.000 0.952 0.952 ## .wy8 0.206 0.007 30.201 0.000 0.994 0.994 ## .wx9 0.377 0.008 45.519 0.000 0.948 0.948 ## .wy9 0.231 0.007 35.362 0.000 0.995 0.995 ## .FX6 0.000 0.000 0.000 ## .FX7 0.000 0.000 0.000 ## .FX8 0.000 0.000 0.000 ## .FX9 0.000 0.000 0.000 ## .euthanasia.6 0.000 0.000 0.000 ## .euthanasia.7 0.000 0.000 0.000 ## .euthanasia.8 0.000 0.000 0.000 ## .euthanasia.9 0.000 0.000 0.000 6.8.3 Include egos educational level First construct a variable educ for ego. We take the educational level in years at wave 6, if missing we will take the score of wave 7, etc. We thus consider the educational level of ego as a time invariant variable. We want to: include the educational level of ego as predictor for the random intercept referring to egos opinion towards euthanasia include the educational level of ego as predictor for the random intercept referring to egos educational level of the CDN Before looking at the hidden code please try to: - construct the educational variable for ego - estimate the RI-CLPM - think of how and why parameter estimates will change Only click button after 5 minutes! liss_w &lt;- liss_w %&gt;% mutate(educ = educ.6, educ = ifelse(is.na(educ), educ.7, educ), educ = ifelse(is.na(educ), educ.8, educ), educ = ifelse(is.na(educ), educ.9, educ), ) RICLPM &lt;- &#39; # Create between components (random intercepts) RIx =~ 1*FX6 + 1*FX7 + 1*FX8 + 1*FX9 RIy =~ 1*euthanasia.6 + 1*euthanasia.7 + 1*euthanasia.8 + 1*euthanasia.9 RIx ~ educ RIy ~ educ # Create within-person centered variables wx6 =~ 1*FX6 wx7 =~ 1*FX7 wx8 =~ 1*FX8 wx9 =~ 1*FX9 wy6 =~ 1*euthanasia.6 wy7 =~ 1*euthanasia.7 wy8 =~ 1*euthanasia.8 wy9 =~ 1*euthanasia.9 # Estimate the lagged effects between the within-person centered variables. wx7 ~ a*wx6 + b*wy6 wx8 ~ a*wx7 + b*wy7 wx9 ~ a*wx8 + b*wy8 wy7 ~ c*wx6 + d*wy6 wy8 ~ c*wx7 + d*wy7 wy9 ~ c*wx8 + d*wy8 # Estimate the (residual) covariance between the within-person centered variables wx6 ~~ wy6 wx7 ~~ wy7 wx8 ~~ wy8 wx9 ~~ wy9 # Estimate the variance and covariance of the random intercepts. RIx ~~ RIx RIy ~~ RIy RIx ~~ RIy # Estimate the (residual) variance of the within-person centered variables. wx6 ~~ wx6 wy6 ~~ wy6 wx7 ~~ wx7 wy7 ~~ wy7 wx8 ~~ wx8 wy8 ~~ wy8 wx9 ~~ wx9 wy9 ~~ wy9 #include intercepts FX6 ~ 1 FX7 ~ 1 FX8 ~ 1 FX9 ~ 1 euthanasia.6 ~ 1 euthanasia.7 ~ 1 euthanasia.8 ~ 1 euthanasia.9 ~ 1 &#39; fit6 &lt;- lavaan(RICLPM, data=liss_w, missing = &quot;fiml.x&quot;, meanstructure = T ) summary(fit6, standardized = T) ## lavaan 0.6-9 ended normally after 56 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 37 ## Number of equality constraints 8 ## ## Used Total ## Number of observations 8167 13018 ## Number of missing patterns 55 ## ## Model Test User Model: ## ## Test statistic 168.715 ## Degrees of freedom 23 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Observed ## Observed information based on Hessian ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## RIx =~ ## FX6 1.000 0.622 0.695 ## FX7 1.000 0.622 0.659 ## FX8 1.000 0.622 0.673 ## FX9 1.000 0.622 0.700 ## RIy =~ ## euthanasia.6 1.000 0.863 0.889 ## euthanasia.7 1.000 0.863 0.883 ## euthanasia.8 1.000 0.863 0.885 ## euthanasia.9 1.000 0.863 0.873 ## wx6 =~ ## FX6 1.000 0.643 0.719 ## wx7 =~ ## FX7 1.000 0.710 0.752 ## wx8 =~ ## FX8 1.000 0.683 0.739 ## wx9 =~ ## FX9 1.000 0.634 0.714 ## wy6 =~ ## euthanasia.6 1.000 0.446 0.459 ## wy7 =~ ## euthanasia.7 1.000 0.459 0.469 ## wy8 =~ ## euthanasia.8 1.000 0.455 0.466 ## wy9 =~ ## euthanasia.9 1.000 0.482 0.487 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## RIx ~ ## educ 0.132 0.003 44.336 0.000 0.213 0.571 ## RIy ~ ## educ 0.016 0.004 3.676 0.000 0.018 0.049 ## wx7 ~ ## wx6 (a) 0.196 0.013 15.606 0.000 0.178 0.178 ## wy6 (b) -0.007 0.020 -0.372 0.710 -0.005 -0.005 ## wx8 ~ ## wx7 (a) 0.196 0.013 15.606 0.000 0.204 0.204 ## wy7 (b) -0.007 0.020 -0.372 0.710 -0.005 -0.005 ## wx9 ~ ## wx8 (a) 0.196 0.013 15.606 0.000 0.211 0.211 ## wy8 (b) -0.007 0.020 -0.372 0.710 -0.005 -0.005 ## wy7 ~ ## wx6 (c) 0.005 0.010 0.560 0.575 0.008 0.008 ## wy6 (d) 0.075 0.017 4.335 0.000 0.073 0.073 ## wy8 ~ ## wx7 (c) 0.005 0.010 0.560 0.575 0.009 0.009 ## wy7 (d) 0.075 0.017 4.335 0.000 0.076 0.076 ## wy9 ~ ## wx8 (c) 0.005 0.010 0.560 0.575 0.008 0.008 ## wy8 (d) 0.075 0.017 4.335 0.000 0.071 0.071 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## wx6 ~~ ## wy6 -0.006 0.006 -0.922 0.357 -0.020 -0.020 ## .wx7 ~~ ## .wy7 -0.011 0.007 -1.551 0.121 -0.034 -0.034 ## .wx8 ~~ ## .wy8 -0.006 0.007 -0.863 0.388 -0.019 -0.019 ## .wx9 ~~ ## .wy9 0.002 0.006 0.284 0.777 0.005 0.005 ## .RIx ~~ ## .RIy 0.021 0.008 2.722 0.006 0.048 0.048 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .FX6 -1.631 0.038 -42.753 0.000 -1.631 -1.823 ## .FX7 -1.574 0.038 -41.068 0.000 -1.574 -1.668 ## .FX8 -1.432 0.038 -37.441 0.000 -1.432 -1.551 ## .FX9 -1.382 0.038 -36.266 0.000 -1.382 -1.557 ## .euthanasia.6 4.221 0.053 79.084 0.000 4.221 4.346 ## .euthanasia.7 4.240 0.053 79.339 0.000 4.240 4.339 ## .euthanasia.8 4.253 0.054 79.473 0.000 4.253 4.359 ## .euthanasia.9 4.220 0.054 78.666 0.000 4.220 4.270 ## .RIx 0.000 0.000 0.000 ## .RIy 0.000 0.000 0.000 ## wx6 0.000 0.000 0.000 ## .wx7 0.000 0.000 0.000 ## .wx8 0.000 0.000 0.000 ## .wx9 0.000 0.000 0.000 ## wy6 0.000 0.000 0.000 ## .wy7 0.000 0.000 0.000 ## .wy8 0.000 0.000 0.000 ## .wy9 0.000 0.000 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .RIx 0.260 0.008 31.467 0.000 0.674 0.674 ## .RIy 0.743 0.015 50.574 0.000 0.998 0.998 ## wx6 0.414 0.009 43.574 0.000 1.000 1.000 ## wy6 0.199 0.006 31.334 0.000 1.000 1.000 ## .wx7 0.488 0.011 45.412 0.000 0.968 0.968 ## .wy7 0.209 0.007 30.914 0.000 0.995 0.995 ## .wx8 0.447 0.010 44.045 0.000 0.958 0.958 ## .wy8 0.206 0.007 30.208 0.000 0.994 0.994 ## .wx9 0.384 0.008 46.782 0.000 0.955 0.955 ## .wy9 0.231 0.007 35.361 0.000 0.995 0.995 ## .FX6 0.000 0.000 0.000 ## .FX7 0.000 0.000 0.000 ## .FX8 0.000 0.000 0.000 ## .FX9 0.000 0.000 0.000 ## .euthanasia.6 0.000 0.000 0.000 ## .euthanasia.7 0.000 0.000 0.000 ## .euthanasia.8 0.000 0.000 0.000 ## .euthanasia.9 0.000 0.000 0.000 6.9 Assignment Please give an interpretation of the most important parameter estimates of the micro-macro models (including the RI-CLP-MM). Does the educational level of our confidants influence our opinion towards euthanasia? Do you observe selection effects and how can they be explained? Try to answer the formulated research questions 6.3 you could try to combine the different opinions of ego in one latent variable to increase power. try to see if the influence of the educational level of the CDN depends on the size of the CDN (I would recommend taking a multi-group perspective) or on egos educational level in years (I would recommend introducing an interaction effect) to check whether influence processes depend on other characteristics of the alters is definitely not easy. The method is described in Bennink et al. (2016) but this is too difficult and not feasible in lavaan (perhaps in a two-step approach). You have to try to be creative. References "],["data-3.html", "Chapter 7 Data 7.1 Sampling 7.2 Ethical considerations 7.3 Measurement", " Chapter 7 Data 7.1 Sampling 7.2 Ethical considerations 7.3 Measurement 7.3.1 Scaling up Let us go back on the Dunbars number. What social relationships are beyond the 150 active relationships in Dunbars number? This is something that is often defined as the extended social network (Hofstra, Corten, and Tubergen 2021) or the acquaintanceship network (DiPrete et al. 2011). This layer encapsulates all former social ties  from the core to the active social ties  and it includes even the weakest social relationships. It is surprisingly difficult to measure those weaker ties. Imagine that you would have to recall all of your weaker social relationships. How many weaker ties would you be able to recall? Ten, 50, 100, or perhaps even 500? And in the unlikely case that you would be able to recall 500 social relationships, would you have time to write down in a questionnaire who these persons are? Social network analysts have struggled how to measure weaker social network relationships, and, by extension, extended social network size for quite a while. Scientists have used wildly varied approaches to measure extended network size: by asking respondents who they know from random phonebook pages, by counting the number of Christmas cards people send out see 5.3.1.1, or by summing up different network roles (e.g., how many accountants do you now?). More recently, scientists have started to gather data unobtrusively from social media where people themselves curated hundreds of their social ties in a list (Twitter followers, instagram follows, and so forth, see Part IV). The sizes of extended social networks obviously vary by the chosen methods to measure those networks. There are estimates up until network sizes of 5,520 (Freeman and Thompson 1989)]. Yet, for ego networks there have been methodological advances how to measure extended network size with surveys in a relatively standardised way: the network scale-up method (hereafter NSUM) (Killworth, Johnsen, et al. 1998; Killworth, McCarty, et al. 1998; McCormick, Salganik, and Zheng 2010). How does the NSUM work? Consider that there is a population of size N. You could ask respondents how many randomly drawn others n they know in that population. As N increases, however, the likelihood that two random person know one another becomes lower, and with a population N in the millions (like in many countries), that likelihood becomes extremely small. This problem can be tackled, however, by asking respondents how many others they know with a given characteristic. For instance, the NSUM asks How many people do you know named Bas? This is more informative than asking which of the ~18.7K Bass a respondent knows in the Netherlands. Now we know the fraction of Bass a respondent knows in a population. Say I know one Bas. This implies that I know 1/18700 of all Bass! If we assume that this fraction applies similarly to all other friend-categories in my network we can extrapolate the fraction the Dutch population: \\[\\frac{1}{18,700}\\cdot17 million\\approx909\\] According to this logic, I know 909 people. When you average this for a set of categories the estimated network size increases in accuracy  e.g., people named Bas, people attending university, and so forth. If we put this in an equation (see Bernard et al. 2010; McCormick, Salganik, and Zheng 2010), it looks like: \\[Basic\\;scale-up\\;estimator_i = \\frac{\\Sigma_{k=1}^Ky_{ik}}{\\Sigma_{k=1}^KN_{k}}\\] where \\(y_{ik}\\) is the total number of people an individual \\(i\\) claims to know in category \\(k\\), \\(N_k\\) is the size of the category \\(k\\), and \\(N\\) is the size of the entire population. Note that for each category k, you do need to know the population size. The NSUM often starts with relatively similar definitions on what constitutes a relationship. This is important to set a substantive network boundary (what are substantively meaningful connections to consider?) and a methodological boundary. One such network tie definition that scholars have used is, contacts whom individuals know on a first name basis, such they would have a friendly chat if they were to meet randomly (cf. McCarty et al. 2001: 29; DiPrete et al. 2011: 1242; Hofstra, Corten, and Tubergen 2021: 1277). Note, however, that one may choose another boundary that then influences how small (if asking for more know conditions) or large (if releasing know conditions) network size estimates will be. Practically, in surveys you could ask respondents how may people they know in a given context in a given year (say, the Netherlands in 2020) on a first name basis such that they would chat with if they were to meet randomly and ask for a battery of items how many people like that they know named Bas named Jochem currently attending university owning a Tesla and so forth You could ask these questions open where respondents fill in a number or with interval censoring where you help respondents answer (e.g., 0, 1-2, 3-5, 6-10, etc.). If you then apply the logic above you have estimated the extended network size with the basic scale-up estimator. The NSUM was originally developed to estimate sizes of hard-to-reach populations  populations for which it is hard to estimate how many there are in a population  such as, for instance, persons who inject drugs. You can use the same logic: if you have calculated an extended social network size of a respondent, you can divide the number of people one reports to know of the unknown population (e.g., how many people do you know who inject drugs?) by the extended network size. If you average that over many respondents and multiply it by the total population size, you know the size of the unknown population. Lets say I report to know 2 people who inject drugs, have an extended network size of 700 (estimated by the basic scale-up estimator above) in a population of 17 million. This may mean that there are (2/700)*17million=48.5K persons in the total population who inject drugs. Important to remember, therefore, is that there are subpopulations of known size (e.g., people named Bas) and subpopulations of unknown size (e.g., persons who inject drugs) and this can be utilised in the NSUM. The interdependence between social agents in populations can thus be utilised to address many problems. The basic scale-up estimator above has three issues (its called basic for a reason): recall errors, transmission biases, and barrier effects. Recall errors are when respondents err in providing estimate for how many people they know in a category; transmission biases are when people are unaware that they know persons in a category (e.g., unaware that someone is attending university); and barrier effects occur when the categories relate to characteristics of respondents (e.g., a Dutch majority respondent may know more Bass than a first-generation migrant). There are statistical techniques to account for those issues, though we do not detail those in this chapter. Maltiel et al. (2015) References "],["theory-2.html", "Chapter 8 Theory 8.1 Causes 8.2 Consequences", " Chapter 8 Theory 8.1 Causes 8.2 Consequences "],["methods-2.html", "Chapter 9 Methods 9.1 Causes 9.2 Consequences", " Chapter 9 Methods 9.1 Causes 9.2 Consequences "],["data-4.html", "Chapter 10 Data 10.1 Sampling 10.2 Ethical considerations 10.3 Measurement", " Chapter 10 Data 10.1 Sampling 10.2 Ethical considerations 10.3 Measurement "],["webintro.html", "Chapter 11 Webscraping for Social Scientists 11.1 Chapter overview 11.2 Promises and pitfalls 11.3 Where does this lead us? 11.4 Ethics 11.5 Webscraping techniques 11.6 Hands-on webscraping", " Chapter 11 Webscraping for Social Scientists 11.1 Chapter overview [The] technological revolution in mobile, Web, and Internet communications has the potential to revolutionize our understanding of ourselves and how we interact. Merton was right: Social science has still not found its Kepler. But three hundred years after Alexander Pope argued that the proper study of mankind should lie not in the heavens but in ourselves, we have finally found our telescope. Let the revolution begin (Watts 2011: 266) Watts already-famous quote predicts a revolution in the social sciences. He and others (see also Lazer et al. 2009) essentially argue that social science will be revolutionized by the unprecedented use of of the social internet. Given that people overwhelmingly adopted internet technologies and given that many of the platforms that offer these technologies automatically archive all kinds of behavior (Spiro 2016) such as clicks, messages, social media relationships, and so forth, there may be a treasure trove of data on the internet that social scientists can use for their research on social processes. In this chapter, we discuss some of the promises and pitfalls of webscraping so-called digital trace data (Golder and Macy 2014) on the internet for social network analysis. We are then going to discuss some different techniques that are often used for webscraping. Note that the fast-pace nature of the internet inherently means that by the time you read this text, some of the things we discuss will be outdated. (Which can be argued to be one of the pitfalls of social science research with webscraping!) We are also getting our hands dirty with a hands-on example of digital trace data that we are going to collect ourselves. So by the end of this chapter, you will be familiar with some of the unique opportunities and difficulties of webscraped (social network) data, have a birds-eye perspective on the different techniques for scraping the web for your own research, have knowledge on the ethics surrounding webscraping, and have more in-depth experience on one specific package for webscraping bibliometric data in R. In short, you will have firsthand knowledge on the current state-of-the-art in sociological data collection. There are really good, exhaustive resources for webscraping and computational sociology. See, for instance, the book by Robert Ackland (Ackland 2013). Yet, to get up to speed for this chapter, you can read the first chapter of Bas Hofstras dissertation (Hofstra 2017), Golder and Macys Annual Review of Sociology article (Golder and Macy 2014), and Lazer and colleagues Science article (Lazer et al. 2009). A very nice introduction to the field of computational social science can be found in Salganiks text book (Salganik 2019). An overview with recent applications is written by Edelman and colleagues (Edelmann et al. 2020). 11.1.1 Definitions Webscraping The process by which you collect data from the internet. This can entail different routes: manual data collection, automated data collection via code, use of application programming interfaces, and so forth. Digital footprints Automatically logged behavioral signals that actors  broadly construed: individuals, companies, organizations, groups, etc.  leave on the internet. This may imply many things, including the messages one leaves on Instagram posts, back-and-forth conversations on Whatsapp, companies job advertisements, university course texts, and so forth. All of these signals can capture some social process: networking on social media, signalling specific job requirements, or university course prerequisites. This also means that digital footprints can contain a lot of different and sometimes unstructured data types. Social network data is obvious: who is friends with whom on Facebook, who Tweets to whom, and so forth. Network data (not social) is also obvious. For instance, which website links to what other websites. (Sidenote: Googles page-rank algorithm made them succesful, and this page-rank algorithm is based on network centrality that essentially filters out influential websites quickly. In other words, Google became such an influential company because of network analyses.) It can also contain (unstructured) text data, which in itself signals a lot of interesting social processes that one may consider. Computational sociology Problem-driven, empirical sociology, but with the empirical part specifically containing some form of digital footprint data and/or some new methodological technique. Sociologists are usually (necessarily?) interested in digital footprints concerning some social process. Because digital footprints are often related to social network processes (e.g., befriending on Facebook, messaging on Twitter, etc.), a lot of computational sociology includes some form of social network analysis. Because this is often, though not always, the case, discussing webscraping in the context of this book on social network analyses makes perfect sense. Some claim Agent-Based Modelling to be part of computational sociology too, others not. Again others claim performing RSiena analyses is part of computational sociology, others not. Note that this definition-issue is somewhat of a useless moving target. Computational sociologys definition will be different next week depending on who you ask. In this book, we use a pragmatic definition. This means that you are a computational sociologists if you use digital footprint data and/or use relatively new methodological techniques in your research. Also note that there is a certain cause-effect sequence in the three definions above: using webscraping techniques to gather digital footprint data to study social problems makes you a computational sociologist. 11.2 Promises and pitfalls Like every data source in the social sciences and beyond, there are unique features as well as difficult challenges to webscraped data. In this subsection, we will discuss some of these advantages and challenges of webscraping and, by extension, digital footprint data. Like we discussed before, most of the research using webscraped digital footprint data concerns social networks and so we situate these promises and pitfalls in the context of social network analysis. Note, however, that some of the promises and pitfalls generalize to other types of digital footprint data too. 11.2.1 Promises (Social) networks One of the key advantages of scraping data from the internet is that it is relatively easy to collect sociologically interesting network data. This may sound like a surprising thing to note in a book on social network analyses. But imagine a world without the internet, and then imagine that you are a social scientist interested in weak tie dynamics. What toolset do you have available to collect data on and then study those weak ties? You would probably think about qualitative interviews or collecting survey data. For the purpose of studying weak ties, however, both of these methods of data collection suffer from some some weaknesses. For instance, it is incredibly hard for respondents to recall those social ties that are weakly related. So asking about weak ties will likely not yield very reliable and valid results if you not somehow account for that. That is, respondents will mostly acquiesce to naming those ties that they met recently or which are relatively stronger. (There are some techniques to circumvent some of these issues, but those have their own drawbacks too (see Hofstra, Corten, and Tubergen 2021). And those techniques are suited mostly to ego networks and not full social networks.) Many surveys are also restricted, in that respondents can only name five social ties. It is possible to collect sociometric data of entire contexts in surveys, for instance by presenting respondents with a class-roster or department-roster and asking them who their trustees, friends, etc. This book even devotes an entire section to such data. Yet, such a design is pretty expensive to set up  i.e., you may be limited to a fixed set of network contexts  and may be quite taxing to respondents. In contrast, an inherent feature of many places on the social internet is that individuals curate themselves who their connections are over a long time-span (e.g., friendships on Facebook) or leave many traces of interactions (e.g., mentioning someone on Twitter). Sometimes networks online are even pushed towards (triadic) network closure by recommendation systems on platforms like Facebook or Instagram. With some creativity on the scientists part, such data are relatively easy and cheap to collect. This may often lead to large and complete networks that are not restricted by relationship type (e.g., strong or weak), social context (e.g., family or school friends), respondent recall (acquiesce to strong ties), or social desirability bias (e.g., only nominating the popular kid in a class). An exemplary paper that circumvents some of the regular biases of social network research mentioned above is Hofstra et al. (2017) or Wimmer and Lewis (2010) who analyze segregation among weak ties by means of Facebook data. The benefits mentioned here for the analyses of social networks are also prime reasons as to why much research using digital traces incorporates some type of social network analysis. Dynamics A second advantage of digital trace data is that these data are often time-stamped (and sometimes geo-stamped). This means that the the researcher knows exactly when (and where) the digital trace  e.g., the social interaction on Twitter  occurred. So the researcher can potentially perform some sort of longitudinal analyses so as to come closer to causal estimates in inferential statistical models. In the context of webscraped social networks this is particularly useful so as to separate selection from influence in larger social networks. Gathering such longitudinal sociometric data for many social foci (e.g., school classes) is difficult (yet, definitely not impossible!), whereas collecting time-stamped social interactions on the internet may be somewhat easier. Note also that social network data collected in, for instance, school classes often puts the same time-stamp on a given network (e.g., the time that class was surveyed), whereas networks online may contain more-detailed time-stamps. These time-stamped (network) data can in some cases be considered relational events (Butts 2008). Signals A third advantage of webscraped data is that it can potentially capture behavioral and/or attitudinal signals that are otherwise hard to come by. Say you want to know about social network dynamics among drug traders. Those drug traders probably wont indicate in a survey that they are engaged in such illicit activities. Scraping data from the so-called dark web may be one of the only ways to study networks among drug traders (see Norbutas 2018 who does just that) apart from stitching together police reports which are likely to be confidential. Furthermore, survey respondents may be a bit hesitant to write about their own attitudes that are perceived to be socially undesirable (like severe ethnic prejudice). In that case, one may collect digital trace data on Twitter, where you can observe and then operationalize ethnic prejudice happening in real time. Not all online behaviors and/or attitudes are accurate proxies for offline attitudes/behavior of interest and are very particular to online settings. Therefore, the researcher must be aware, theorize, and ideally empirically show where and how their online data proxies their behavior-of-interest offline. Size Finally, and we discuss why this is both a blessing and a curse (see below), webscraping can lead to data that can contain a lot of observations (into the many millions!) or variables. Note that this is in and of itself not an advantage. More data are not always better data if they are biased. Yet, the sheer size of webscraped data  under appropriate sampling!  may make it easier to observe relationships between the variables of interest when they are small in magnitude. Such small effects may in smaller samples be swamped by random variability (cf. Golder and Macy 2014: 132) This does not mean one can go look into their big dataset for random relationships between variables, these relationships should be problematized and theorized first (just like any other problem-driven, hypothesis-testing social science study). 11.2.2 Pitfalls Sampling Scientists using digital trace data should think carefully about their target population vis-a-vis their sampling frame and realized sample. This is something that is not unique to digital trace data. Yet, it is easy to be so impressed by the sheer data size in studies using digital trace data that questions about generalizability of results sometimes tend to get overshadowed. That is not to say that it doesnt tell us anything informative, just that we do not necessarily know to what target population such results generalize. All types of selectivity can crawl into the data. For instance, if you want to study Facebook/Instagram/TikTok, you should be aware that such platforms tend to get disproportionately populated by certain demographics. If among 5 million Twitter users those in geographical region a display some behavior y more so than those in region b, it does not necessarily mean that regions a and b differ in y. It may be that Twitter is perceived to be a particularly good platform in a to display y, whereas in b people are indifferent to display y on Twitter or elsewhere. Selection into Twitter thus plays an important role in this example. This may happen for many digital trace data sources: biased selection into certain platforms, biased selection into privacy settings which can obscure what you can observe, biased selection into what people display online, and so forth. Ideally, you would have some anchor data set from which you know that it generalizes to a given target population and link that to some source of digital trace data. On the other end, one could attempt to study an entire population such that you are pretty certain that you can generalize your results to that population. Size Like we described above, data size is an advantage of digital trace data, yet simultaneously it is also a pitfall. Huge numbers of observations (again, into the many if not hundreds of millions) may be pretty difficult to manipulate and analyze. In some cases, the data become so large that it is necessary to move to computing clusters because your laptops memory cannot handle it anymore. Dependent on what type of data you analyze you thus might need to adjust your data workflow. Not many educational programs prepare students for storing, manipulating, and analyzing large data sets, and this requires slightly different programming/statistical skills than what were used to. It may be a solution to sample from these huge data sets, such that you, say, only analyze a random sample of 5%. Yet, sampling from social networks is especially hard because of the interdependent nature of networks; some of the inherently clustered structure of networks is lost when you only draw a subset of agents from the network. Data structure This point relates to the point above. Webscraped digital trace data is usually structured very differently compared to the flat data files social scientists are used to working with. Usually, we open a dataset with columns (variables) and rows (observations). Webscraped data is usually stored in nested structured such as XML or JSON or contains text data. Therefore, additional manipulation is needed before we arrive at the data formats that standard statistical packages can read/analyze. Sometimes the networks-of-interest are stored in text data, for instance if youre interested in letter-writing relationships. Hence, if you want to manipulate and analyze these data at scale, some form of programming becomes nearly unavoidable. (Luckily, we provide hands-on tools and examples in this book!) Unobserved variables Finally, webscraped digital trace data often do not contain the detailed demographic information that surveys (often) do provide. And this information often contains the key (control) variables in social science analyses. Imagine you scraped all customer reviews on the Etsy website because you want study how women and men reviewers judge the products of women and men creators differently. That is an important research question because it may show how gender dynamics in reviewing may (re)produce inequality between women and men creators. Yet, how do we know which reviewers are women or men? These labels do not come with the scraped data and some additional manipulation is needed. You could, for instance, attempt to predict whether a reviewer is a women or men by their first name. This is because first names are signals that relate strongly to gender. Yet, not every reviewer provides their first name but only a rather uninformative screenname. Furthermore, naming habits also vary between countries and the data do not inform you from which countries these reviewers originate. And what about age? What about income? Hence, there could be many factors related to the outcome you intend to study that are not readily available in digital trace data. This again requires some creativity on the researchers side, for instance by matching survey data with digital trace data, enrich data with other data sources, and so forth. 11.3 Where does this lead us? So if we sum up what we have learned thus far, what are the overarching benefits of webscraped digital trace data compared to more-traditional data sources? We can think of at least three of those: * (1) New tests of old social science hypotheses made possible by the availability of digital footprint data; * (2) Tests of newly derived social science hypotheses made possible by the availability of digital footprint data; * (3) Tests of new theories about the internet as social phenomenon by itself. Note that these three points are not mutually exclusive: a newly derived social science hypothesis might just as well be about the internet as social phenomenon by itself. Yet, for analytic purposes it is convenient to list these three as separate. An interesting example related to point (1) is the question whether social networks are small-worlds  i.e., highly clustered yet having a short average path length. A popular adage derives from this feature of social networks: individuals are all separated by approximately six degrees. This was traditionally studied by considering letter chains. Now one could study this with the entire Facebook network, and so one could actually test this hypothesis at much larger and complete scale than before (see XXX). With respect to point (2): you could derive new hypotheses about the conversational nature of massive collaborative projects by scraping and then studying all Wikipedia edits (something that was hard to study before). More recently, a lot of studies emerged on the role of fake news and echo chambers with respect to individual attitudes and polarization, which relates to point (3): how the internet as social phenomenon by itself can influence behavior/attitudes. The strongest computational social science papers leverage the strengths of digital trace data but simultaneously account for (or at the very least acknowledge) some of its weaknesses as we list above. 11.4 Ethics In webscraping digital footprins, one has to always consider ethics. In an ideal situation, a researcher has informed consent to study research subjects. In practice, however, the nature of webscraping digital footprints makes it very difficult to obtain such consent. Ethic review boards 11.5 Webscraping techniques 11.5.1 Manual Army of research assistants looking up information, saving that 11.5.2 APIs Utilizing structures in place 11.5.3 Crawling Designing own crawlers 11.6 Hands-on webscraping Now that we learned about computational social science, webscraped digital trace data, and some of its techniques, it is time to get our hands dirty ourselves. In what follows is a short tutorial on webscraping where we will be collecting data from webpages on the internet. We will use the specific use-case of sociology staff at Radboud University. What do they publish? Where? And with whom do they collaborate? We assume you followed the R tutorial in this book, or that you otherwise have at least some experience with coding in R. In the rest of this tutorial, we will switch between base R and Tidyverse (just a bit), whatever is most convenient. (Note that this will happen sometimes if you become an applied computational sociologist.) What are we are going to scrape specifically? This is a social network book so were obviously going to scrape social networks. Specifically, the social network of co-authors on the scientific papers of Radboud Universitys Department of Sociology Staff. Co-authors on a paper are the set of scholars who publish a scientific paper together in a journal. Who do sociology staff publish with? And are those co-authors connected with one another too? Are these social networks clustered in some way? To get at such data we need several things: a list of RUs sociology staff, a repository with their publication,s and the meta-information (titles, authors, etc.) of those publications. Substantively, this is key to look at things like how often scholars are cited or how social network dynamics in science work. (Not there is a whole body of research on the so-called Sociology of Science or the Science of Science. Because scientists are particularly good at documenting things, sociology/science of science type-of-work is some of the earliest work that you can label computational social science.) In the remainder of this chapter, we thus provide a tutorial in which we explain the packages needed to do what we write above. We will shortly describe the scholar and other packages in R, the data sources, and the (network) data structures you encounter and how to deal with them. Yet, there are many ways code-wise or data-wise with which you use to do very similar things like we do in this tutorial. 11.6.1 Staging your script Open a new R-script (via file &gt; new &gt; RScript (or simply hit Ctrl+Shift+N or Cmd+Shift+N if you work on Mac) Before you start scraping and analyzing, take all the precautionary steps noted in A So for this tutorial, your starting script will look something like this: ######################################### Title: Webscraping in R Author: Bas Hofstra Version: ######################################### 29-07-2021 # start with clean workspace rm(list = ls()) library(tidyverse) # I assume you already installed this one! install.packages(&quot;httr&quot;) require(httr) install.packages(&quot;xml2&quot;) require(xml2) install.packages(&quot;rvest&quot;) require(rvest) install.packages(&quot;devtools&quot;) require(devtools) # Note we&#39;re doing something different here. We&#39;re installing a *latest* version directly from # GitHub This is because the released version of this packages contains some errors! devtools::install_github(&quot;jkeirstead/scholar&quot;) require(scholar) # define workdirectory, note the double backslashes if you&#39;re on windows setwd(&#39;/yourpathhere)&#39; 11.6.2 Getting anchor data A first step is to get the anchor data. The anchor data are the first data we scrape with which we then link to further data sources. Our goal is to get to know (i) who the Radboud University Department of Sociology staff is, (ii) what they publish with respect to scientific work, and (iii) who they collaborate with. So that means at least three data sources we need to collect from somewhere. What would be a nice starting (read: anchor) point be? First, we have to know who is on the sociology staff. Lets check out the Radboud sociology website. There is lots of intriguing information, but not on who is who. There is, however, a specific link to the research staff. Here we do see a nice list on who is on the sociology staff! How do we get that data? It is actually quite simple, the package xml2 has a very nice function html_read() which simply extracts the source html of a webpage: # Let&#39;s first get the staff page read_html is a function that simply extracts html webpages and # puts them in xml format soc_staff &lt;- read_html(&quot;https://www.ru.nl/sociology/research/staff/&quot;) head(soc_staff) #&gt; $node #&gt; &lt;pointer: 0x0000000027260bb0&gt; #&gt; #&gt; $doc #&gt; &lt;pointer: 0x000000001400cd70&gt; That looks kinda weird. What type of object did we store it by putting the html into soc_staff? class(soc_staff) #&gt; [1] &quot;xml_document&quot; &quot;xml_node&quot; So it is is stored in something that R calls an XML object. Remember when we talked about that in the text above? Extensible Markup Language, XML, is a nested data structure where in each next sublayer of that structure new information is stored. Not important for now what that means specifically. But it is important to extract the relevant table that we saw on the sociology staff website. How do we do that? Go to the https://www.ru.nl/sociology/research/staff/ in Google Chrome and then press Inspect on the webpage (right click&gt;Inspect). You should see something like the screenshot below, right? Figure 11.1: Inspect element Look at the screenshot below, you should be able to see something like this. In the html code we extracted from the Radboud website, we need to go to one of the nodes first. If you move your cursor over body in the html code on the right-hand side of your screen, the entire body of the page should become some shade of blue. This means that the elements encapsulated in the body node captures everything that turned blue. This is essentially the nested data structure we mentioned above. Figure 11.2: Website body node Next, we need to look at the specific elements on the page that we need to extract. Somewhat by informed trial and error, looking for the correct code, we can select the elements we want. In the screenshot below, you see that the td elements actually are the ones we need. So we need code that looks for the node body and the td elements in the xml object and then extract those elements in it. Note that you can click on the arrows once you are in the Inspect mode in the web browser to trial-and-error to get at the correct elements. Figure 11.3: Element td on website Something like the code below should do just that: # so we need to find WHERE the table is located in the html &#39;inspect element&#39; in mozilla firefox or # &#39;view page source&#39; and you see that everything AFTER /td in the &#39;body&#39; of the page seems to be # the table we do need soc_staff &lt;- soc_staff %&gt;% rvest::html_nodes(&quot;body&quot;) %&gt;% xml2::xml_find_all(&quot;//td&quot;) %&gt;% rvest::html_text() Question: What happens in the code above? Why do we specify search for body and //td? Let us check out what happened to the soc_staff object now: soc_staff # looks much better!! Table 11.1: Sociology staff x Staff: Expertise: Batenburg, prof. dr. R. (Ronald) Healthcare, labour market and healthcare professions and training Begall, dr. K.H. (Katia) Family, life course, labour market participation, division of household tasks and gender norms Bekhuis, dr. H. (Hidde) Welfare state, nationalism and sports Berg, dr. L. van den (Lonneke) Family, life course and transition to adulthood Blommaert, dr. L. (Lieselotte) Discrimination and inequality on the labour market Damman, dr. M. (Marleen) Labour market, life course, older workers, retirement and solo self-employed Eisinga, prof. dr. R.N. (Rob) Methods of research and statistics Gesthuizen, dr. M.J.W. (Maurice) Poverty en social cohesion Glas, dr. S. (Saskia) Islam, gender attitudes and sexuality Hek, dr. M. van (Margriet) Educational inequality, gender inequality, organizational sociology and culture Hoekman, dr. R. H. A.(Remco) Sports and policy sociology Hofstra, dr. B. (Bas) Diversity, inequality and innovation Kraaykamp, prof. dr. G.L.M. (Gerbert) Educational inequality, culture and health Meuleman, dr. (Roza) Culture and nationalism Savelkoul, dr. M.J. (Michael) Ethnic diversity, prejudice and social cohesion Scheepers, prof. dr. P.L.H. (Peer) Comparative research, social cohesion and diversity Spierings, dr. C.H.B.M. (Niels) Islam, gender, populism, social media, Middle East and migration Tolsma, dr. J. (Jochem) Inequality, criminology and ethnic diversity Verbakel, prof. dr. C.M.C. (Ellen) Head of the department Health, family and work Visser, dr. M. (Mark) Older workers, radicalism and social cohesion Wolbers, prof. dr. M.H.J. (Maarten) Educational inequality and labour market inequality PhD: Expertise: Bussemakers, C. (Carlijn) MSc Adverse youth experiences and social inequality Franken, R. (Rob) MSc Sport networks and motivation for sustainable sports participation Firat, M. (Mustafa) MSc Social inequality, older workers, life course and retirement Geurts, P.G. (Nella) MSc Integration and migration Hendriks, I.P. (Inge) MSc Resistance to refugees and social cohesion Jeroense, T.M.G. (Thijmen) MSc Political participation, segregation, opinion polarization and voting behaviour Linders, N. (Nik) MSc Populism, gender, masculinity and sexuality Loh, S.M. (Renae) MSc Educational sociology, social stratification, gender inequality and information communication technology (ICT) Meijeren, M. (Maikel) MSc Social capital, volunteer work and diversity Mensvoort, C.A. van (Carly) MSc Gender, leadership and social norms Müller, K. (Katrin) MSc Opinions about discrimination, migration and inequality Raiber, K. (Klara) MSc Informal care, employment, social inequality and gender Ramaekers, M.J.M. (Marlou) MSc Prosocial behaviour and family Wiertsema, S. (Sara) MSc Inequality in sports and physical activity, school-to-work transition and employment External PhD: Expertise: Betkó, drs. J.G. (János) Social assistance benefit, poverty, reintegration, RCT and social experiment Houten, J. (Jasper) van MSc Sports Middendorp J. (Jansje) van MSc Home administration Vis, E. (Elize) MSc Healthcare, labour market, healthcare professions and training, health and social capital Weber, T. (Tijmen) MSc International student mobility and the internationalization of higher education Guest researchers: Expertise: Sterkens, dr. C.J.A. (Carl) Religious conflicts, cohesion, religion and the philosophy of life Vermeer, dr. P.A.D.M. (Paul) Socialization processes, secularisation, religion and the philosophy of life So it looks much nicer but does not seem to be in the entirely correct order. We have odd rows and even rows: odd rows are names, even rows have the expertise of staff. We need to get a bit creative to put the data in a nicer format. The %% operator gives a remainder of integers (whole numbers). So 10/2=5 with no remainder, but 11/2=5 with a remainder of 1. This means that we can derive odd or even with a function with that operator. Remember functions from A? fodd &lt;- function(x) x%%2 != 0 feven &lt;- function(x) x%%2 == 0 Question: Do you understand what this function does? How long are the data? nstaf &lt;- length(soc_staff) nstaf #&gt; [1] 92 Alright, can we get the odd rows out of there? # Do you understand why we need the nstaf? What it does? soc_names &lt;- soc_staff[fodd(1:nstaf)] # in the 1 until 94st number, get the odd elements head(soc_names) #&gt; [1] &quot;Staff:&quot; &quot;Batenburg, prof. dr. R. (Ronald)&quot; #&gt; [3] &quot;Begall, dr. K.H. (Katia)&quot; &quot;Bekhuis, dr. H. (Hidde)&quot; #&gt; [5] &quot;Berg, dr. L. van den (Lonneke)&quot; &quot;Blommaert, dr. L. (Lieselotte)&quot; And how about peoples expertise? soc_experts &lt;- soc_staff[feven(1:nstaf)] # in the 1 until 94st number, get the even elements head(soc_experts) #&gt; [1] &quot;Expertise:&quot; #&gt; [2] &quot;Healthcare, labour market and healthcare professions and training&quot; #&gt; [3] &quot;Family, life course, labour market participation, division of household tasks and gender norms&quot; #&gt; [4] &quot;Welfare state, nationalism and sports&quot; #&gt; [5] &quot;Family, life course and transition to adulthood&quot; #&gt; [6] &quot;Discrimination and inequality on the labour market&quot; Finally, can we merge those two vectors? soc_df &lt;- data.frame(cbind(soc_names, soc_experts)) # columnbind those and we have a DF for soc staff! How does that look? soc_df # pretty nice! Table 11.2: Sociology staff cleaner soc_names soc_experts Staff: Expertise: Batenburg, prof. dr. R. (Ronald) Healthcare, labour market and healthcare professions and training Begall, dr. K.H. (Katia) Family, life course, labour market participation, division of household tasks and gender norms Bekhuis, dr. H. (Hidde) Welfare state, nationalism and sports Berg, dr. L. van den (Lonneke) Family, life course and transition to adulthood Blommaert, dr. L. (Lieselotte) Discrimination and inequality on the labour market Damman, dr. M. (Marleen) Labour market, life course, older workers, retirement and solo self-employed Eisinga, prof. dr. R.N. (Rob) Methods of research and statistics Gesthuizen, dr. M.J.W. (Maurice) Poverty en social cohesion Glas, dr. S. (Saskia) Islam, gender attitudes and sexuality Hek, dr. M. van (Margriet) Educational inequality, gender inequality, organizational sociology and culture Hoekman, dr. R. H. A.(Remco) Sports and policy sociology Hofstra, dr. B. (Bas) Diversity, inequality and innovation Kraaykamp, prof. dr. G.L.M. (Gerbert) Educational inequality, culture and health Meuleman, dr. (Roza) Culture and nationalism Savelkoul, dr. M.J. (Michael) Ethnic diversity, prejudice and social cohesion Scheepers, prof. dr. P.L.H. (Peer) Comparative research, social cohesion and diversity Spierings, dr. C.H.B.M. (Niels) Islam, gender, populism, social media, Middle East and migration Tolsma, dr. J. (Jochem) Inequality, criminology and ethnic diversity Verbakel, prof. dr. C.M.C. (Ellen) Head of the department Health, family and work Visser, dr. M. (Mark) Older workers, radicalism and social cohesion Wolbers, prof. dr. M.H.J. (Maarten) Educational inequality and labour market inequality PhD: Expertise: Bussemakers, C. (Carlijn) MSc Adverse youth experiences and social inequality Franken, R. (Rob) MSc Sport networks and motivation for sustainable sports participation Firat, M. (Mustafa) MSc Social inequality, older workers, life course and retirement Geurts, P.G. (Nella) MSc Integration and migration Hendriks, I.P. (Inge) MSc Resistance to refugees and social cohesion Jeroense, T.M.G. (Thijmen) MSc Political participation, segregation, opinion polarization and voting behaviour Linders, N. (Nik) MSc Populism, gender, masculinity and sexuality Loh, S.M. (Renae) MSc Educational sociology, social stratification, gender inequality and information communication technology (ICT) Meijeren, M. (Maikel) MSc Social capital, volunteer work and diversity Mensvoort, C.A. van (Carly) MSc Gender, leadership and social norms Müller, K. (Katrin) MSc Opinions about discrimination, migration and inequality Raiber, K. (Klara) MSc Informal care, employment, social inequality and gender Ramaekers, M.J.M. (Marlou) MSc Prosocial behaviour and family Wiertsema, S. (Sara) MSc Inequality in sports and physical activity, school-to-work transition and employment External PhD: Expertise: Betkó, drs. J.G. (János) Social assistance benefit, poverty, reintegration, RCT and social experiment Houten, J. (Jasper) van MSc Sports Middendorp J. (Jansje) van MSc Home administration Vis, E. (Elize) MSc Healthcare, labour market, healthcare professions and training, health and social capital Weber, T. (Tijmen) MSc International student mobility and the internationalization of higher education Guest researchers: Expertise: Sterkens, dr. C.J.A. (Carl) Religious conflicts, cohesion, religion and the philosophy of life Vermeer, dr. P.A.D.M. (Paul) Socialization processes, secularisation, religion and the philosophy of life That looks much better! Now we only need to remove the redundant rows that state expertise, staff, and so forth. # inspect again, and remove the rows we don&#39;t need (check for yourself to be certain!) delrows &lt;- which(soc_df$soc_names == &quot;Staff:&quot; | soc_df$soc_names == &quot;PhD:&quot; | soc_df$soc_names == &quot;External PhD:&quot; | soc_df$soc_names == &quot;Guest researchers:&quot; | soc_df$soc_names == &quot;Other researchers:&quot;) soc_df &lt;- soc_df[-delrows, ] Lets check it out soc_df # even better Table 11.3: Sociology staff even cleaner soc_names soc_experts 2 Batenburg, prof. dr. R. (Ronald) Healthcare, labour market and healthcare professions and training 3 Begall, dr. K.H. (Katia) Family, life course, labour market participation, division of household tasks and gender norms 4 Bekhuis, dr. H. (Hidde) Welfare state, nationalism and sports 5 Berg, dr. L. van den (Lonneke) Family, life course and transition to adulthood 6 Blommaert, dr. L. (Lieselotte) Discrimination and inequality on the labour market 7 Damman, dr. M. (Marleen) Labour market, life course, older workers, retirement and solo self-employed 8 Eisinga, prof. dr. R.N. (Rob) Methods of research and statistics 9 Gesthuizen, dr. M.J.W. (Maurice) Poverty en social cohesion 10 Glas, dr. S. (Saskia) Islam, gender attitudes and sexuality 11 Hek, dr. M. van (Margriet) Educational inequality, gender inequality, organizational sociology and culture 12 Hoekman, dr. R. H. A.(Remco) Sports and policy sociology 13 Hofstra, dr. B. (Bas) Diversity, inequality and innovation 14 Kraaykamp, prof. dr. G.L.M. (Gerbert) Educational inequality, culture and health 15 Meuleman, dr. (Roza) Culture and nationalism 16 Savelkoul, dr. M.J. (Michael) Ethnic diversity, prejudice and social cohesion 17 Scheepers, prof. dr. P.L.H. (Peer) Comparative research, social cohesion and diversity 18 Spierings, dr. C.H.B.M. (Niels) Islam, gender, populism, social media, Middle East and migration 19 Tolsma, dr. J. (Jochem) Inequality, criminology and ethnic diversity 20 Verbakel, prof. dr. C.M.C. (Ellen) Head of the department Health, family and work 21 Visser, dr. M. (Mark) Older workers, radicalism and social cohesion 22 Wolbers, prof. dr. M.H.J. (Maarten) Educational inequality and labour market inequality 24 Bussemakers, C. (Carlijn) MSc Adverse youth experiences and social inequality 25 Franken, R. (Rob) MSc Sport networks and motivation for sustainable sports participation 26 Firat, M. (Mustafa) MSc Social inequality, older workers, life course and retirement 27 Geurts, P.G. (Nella) MSc Integration and migration 28 Hendriks, I.P. (Inge) MSc Resistance to refugees and social cohesion 29 Jeroense, T.M.G. (Thijmen) MSc Political participation, segregation, opinion polarization and voting behaviour 30 Linders, N. (Nik) MSc Populism, gender, masculinity and sexuality 31 Loh, S.M. (Renae) MSc Educational sociology, social stratification, gender inequality and information communication technology (ICT) 32 Meijeren, M. (Maikel) MSc Social capital, volunteer work and diversity 33 Mensvoort, C.A. van (Carly) MSc Gender, leadership and social norms 34 Müller, K. (Katrin) MSc Opinions about discrimination, migration and inequality 35 Raiber, K. (Klara) MSc Informal care, employment, social inequality and gender 36 Ramaekers, M.J.M. (Marlou) MSc Prosocial behaviour and family 37 Wiertsema, S. (Sara) MSc Inequality in sports and physical activity, school-to-work transition and employment 39 Betkó, drs. J.G. (János) Social assistance benefit, poverty, reintegration, RCT and social experiment 40 Houten, J. (Jasper) van MSc Sports 41 Middendorp J. (Jansje) van MSc Home administration 42 Vis, E. (Elize) MSc Healthcare, labour market, healthcare professions and training, health and social capital 43 Weber, T. (Tijmen) MSc International student mobility and the internationalization of higher education 45 Sterkens, dr. C.J.A. (Carl) Religious conflicts, cohesion, religion and the philosophy of life 46 Vermeer, dr. P.A.D.M. (Paul) Socialization processes, secularisation, religion and the philosophy of life Now we have a nice relatively clean dataset with all sociology staff and their expterise. But there is yet some work to do before we can move on. We need to do some data cleaning. Ideally, we have staff their first and last names in clean columns. So the last name seems easy, everything before the comma. Do you understand the code below? gsub is a function that remove something and replaces it with something else. In the code below it replaces everything thats behind a comma with nothing in the column soc_names in the data frame soc_df. The first name is trickier, we need some more difficult expressions to extract first names from this string. Its not necessary for now to exactly know how the expressions below work, but if you want to get into it, heres a nice resource. The important part of the code below is that it extracts everything thats in between the brackets. # Last name seems to be everything before the comma soc_df$last_name &lt;- gsub(&quot;,.*$&quot;, &quot;&quot;, soc_df$soc_names) # first name is everything between brackets soc_df$first_name &lt;- str_extract_all(soc_df$soc_names, &quot;(?&lt;=\\\\().+?(?=\\\\))&quot;, simplify = TRUE) Table 11.4: Sociology staff cleaner yet soc_names soc_experts last_name first_name 2 Batenburg, prof. dr. R. (Ronald) Healthcare, labour market and healthcare professions and training Batenburg Ronald 3 Begall, dr. K.H. (Katia) Family, life course, labour market participation, division of household tasks and gender norms Begall Katia 4 Bekhuis, dr. H. (Hidde) Welfare state, nationalism and sports Bekhuis Hidde 5 Berg, dr. L. van den (Lonneke) Family, life course and transition to adulthood Berg Lonneke 6 Blommaert, dr. L. (Lieselotte) Discrimination and inequality on the labour market Blommaert Lieselotte 7 Damman, dr. M. (Marleen) Labour market, life course, older workers, retirement and solo self-employed Damman Marleen 8 Eisinga, prof. dr. R.N. (Rob) Methods of research and statistics Eisinga Rob 9 Gesthuizen, dr. M.J.W. (Maurice) Poverty en social cohesion Gesthuizen Maurice 10 Glas, dr. S. (Saskia) Islam, gender attitudes and sexuality Glas Saskia 11 Hek, dr. M. van (Margriet) Educational inequality, gender inequality, organizational sociology and culture Hek Margriet 12 Hoekman, dr. R. H. A.(Remco) Sports and policy sociology Hoekman Remco 13 Hofstra, dr. B. (Bas) Diversity, inequality and innovation Hofstra Bas 14 Kraaykamp, prof. dr. G.L.M. (Gerbert) Educational inequality, culture and health Kraaykamp Gerbert 15 Meuleman, dr. (Roza) Culture and nationalism Meuleman Roza 16 Savelkoul, dr. M.J. (Michael) Ethnic diversity, prejudice and social cohesion Savelkoul Michael 17 Scheepers, prof. dr. P.L.H. (Peer) Comparative research, social cohesion and diversity Scheepers Peer 18 Spierings, dr. C.H.B.M. (Niels) Islam, gender, populism, social media, Middle East and migration Spierings Niels 19 Tolsma, dr. J. (Jochem) Inequality, criminology and ethnic diversity Tolsma Jochem 20 Verbakel, prof. dr. C.M.C. (Ellen) Head of the department Health, family and work Verbakel Ellen 21 Visser, dr. M. (Mark) Older workers, radicalism and social cohesion Visser Mark 22 Wolbers, prof. dr. M.H.J. (Maarten) Educational inequality and labour market inequality Wolbers Maarten 24 Bussemakers, C. (Carlijn) MSc Adverse youth experiences and social inequality Bussemakers Carlijn 25 Franken, R. (Rob) MSc Sport networks and motivation for sustainable sports participation Franken Rob 26 Firat, M. (Mustafa) MSc Social inequality, older workers, life course and retirement Firat Mustafa 27 Geurts, P.G. (Nella) MSc Integration and migration Geurts Nella 28 Hendriks, I.P. (Inge) MSc Resistance to refugees and social cohesion Hendriks Inge 29 Jeroense, T.M.G. (Thijmen) MSc Political participation, segregation, opinion polarization and voting behaviour Jeroense Thijmen 30 Linders, N. (Nik) MSc Populism, gender, masculinity and sexuality Linders Nik 31 Loh, S.M. (Renae) MSc Educational sociology, social stratification, gender inequality and information communication technology (ICT) Loh Renae 32 Meijeren, M. (Maikel) MSc Social capital, volunteer work and diversity Meijeren Maikel 33 Mensvoort, C.A. van (Carly) MSc Gender, leadership and social norms Mensvoort Carly 34 Müller, K. (Katrin) MSc Opinions about discrimination, migration and inequality Müller Katrin 35 Raiber, K. (Klara) MSc Informal care, employment, social inequality and gender Raiber Klara 36 Ramaekers, M.J.M. (Marlou) MSc Prosocial behaviour and family Ramaekers Marlou 37 Wiertsema, S. (Sara) MSc Inequality in sports and physical activity, school-to-work transition and employment Wiertsema Sara 39 Betkó, drs. J.G. (János) Social assistance benefit, poverty, reintegration, RCT and social experiment Betkó János 40 Houten, J. (Jasper) van MSc Sports Houten Jasper 41 Middendorp J. (Jansje) van MSc Home administration Middendorp J. (Jansje) van MSc Jansje 42 Vis, E. (Elize) MSc Healthcare, labour market, healthcare professions and training, health and social capital Vis Elize 43 Weber, T. (Tijmen) MSc International student mobility and the internationalization of higher education Weber Tijmen 45 Sterkens, dr. C.J.A. (Carl) Religious conflicts, cohesion, religion and the philosophy of life Sterkens Carl 46 Vermeer, dr. P.A.D.M. (Paul) Socialization processes, secularisation, religion and the philosophy of life Vermeer Paul So we need yet to do some manual cleaning, one name seemed to be inconsistent with how the other names were listed on the webpage. As data get bigger, this becomes impossible to do manually and we simply have to accept this as noise. soc_df$last_name &lt;- gsub(&quot; J. \\\\(Jansje\\\\) van MSc&quot;, &quot;&quot;, soc_df$last_name) soc_df$first_name &lt;- tolower(soc_df$first_name) # everything to lower! soc_df$last_name &lt;- tolower(soc_df$last_name) Not quite there yet. To be sure, well trim some white space in the variables we know created. This means we remove spaces before and after strings. Usually, with a much larger dataset which you cannot immediately observe, you can further clean the data by removing weird characters. # trimws looses all spacing before and after (if you specify &#39;both&#39;) a character string soc_df$last_name &lt;- trimws(soc_df$last_name, which = c(&quot;both&quot;), whitespace = &quot;[ \\t\\r\\n]&quot;) soc_df$first_name &lt;- trimws(soc_df$first_name, which = c(&quot;both&quot;), whitespace = &quot;[ \\t\\r\\n]&quot;) soc_df$soc_experts &lt;- trimws(soc_df$soc_experts, which = c(&quot;both&quot;), whitespace = &quot;[ \\t\\r\\n]&quot;) soc_df$soc_names &lt;- trimws(soc_df$soc_names, which = c(&quot;both&quot;), whitespace = &quot;[ \\t\\r\\n]&quot;) Finally, because were quite sure that all these staff are in some way affiliated with Radboud University (why would they otherwise be on the Radboud website?), we simply create a variable that contains a character string radboud university for all. # set affiliation to radboud, comes in handy for querying google scholar soc_df$affiliation &lt;- &quot;radboud university&quot; How do the data look? soc_df Table 11.5: Sociology staff cleanest? soc_names soc_experts last_name first_name affiliation 2 Batenburg, prof. dr. R. (Ronald) Healthcare, labour market and healthcare professions and training batenburg ronald radboud university 3 Begall, dr. K.H. (Katia) Family, life course, labour market participation, division of household tasks and gender norms begall katia radboud university 4 Bekhuis, dr. H. (Hidde) Welfare state, nationalism and sports bekhuis hidde radboud university 5 Berg, dr. L. van den (Lonneke) Family, life course and transition to adulthood berg lonneke radboud university 6 Blommaert, dr. L. (Lieselotte) Discrimination and inequality on the labour market blommaert lieselotte radboud university 7 Damman, dr. M. (Marleen) Labour market, life course, older workers, retirement and solo self-employed damman marleen radboud university 8 Eisinga, prof. dr. R.N. (Rob) Methods of research and statistics eisinga rob radboud university 9 Gesthuizen, dr. M.J.W. (Maurice) Poverty en social cohesion gesthuizen maurice radboud university 10 Glas, dr. S. (Saskia) Islam, gender attitudes and sexuality glas saskia radboud university 11 Hek, dr. M. van (Margriet) Educational inequality, gender inequality, organizational sociology and culture hek margriet radboud university 12 Hoekman, dr. R. H. A.(Remco) Sports and policy sociology hoekman remco radboud university 13 Hofstra, dr. B. (Bas) Diversity, inequality and innovation hofstra bas radboud university 14 Kraaykamp, prof. dr. G.L.M. (Gerbert) Educational inequality, culture and health kraaykamp gerbert radboud university 15 Meuleman, dr. (Roza) Culture and nationalism meuleman roza radboud university 16 Savelkoul, dr. M.J. (Michael) Ethnic diversity, prejudice and social cohesion savelkoul michael radboud university 17 Scheepers, prof. dr. P.L.H. (Peer) Comparative research, social cohesion and diversity scheepers peer radboud university 18 Spierings, dr. C.H.B.M. (Niels) Islam, gender, populism, social media, Middle East and migration spierings niels radboud university 19 Tolsma, dr. J. (Jochem) Inequality, criminology and ethnic diversity tolsma jochem radboud university 20 Verbakel, prof. dr. C.M.C. (Ellen) Head of the department Health, family and work verbakel ellen radboud university 21 Visser, dr. M. (Mark) Older workers, radicalism and social cohesion visser mark radboud university 22 Wolbers, prof. dr. M.H.J. (Maarten) Educational inequality and labour market inequality wolbers maarten radboud university 24 Bussemakers, C. (Carlijn) MSc Adverse youth experiences and social inequality bussemakers carlijn radboud university 25 Franken, R. (Rob) MSc Sport networks and motivation for sustainable sports participation franken rob radboud university 26 Firat, M. (Mustafa) MSc Social inequality, older workers, life course and retirement firat mustafa radboud university 27 Geurts, P.G. (Nella) MSc Integration and migration geurts nella radboud university 28 Hendriks, I.P. (Inge) MSc Resistance to refugees and social cohesion hendriks inge radboud university 29 Jeroense, T.M.G. (Thijmen) MSc Political participation, segregation, opinion polarization and voting behaviour jeroense thijmen radboud university 30 Linders, N. (Nik) MSc Populism, gender, masculinity and sexuality linders nik radboud university 31 Loh, S.M. (Renae) MSc Educational sociology, social stratification, gender inequality and information communication technology (ICT) loh renae radboud university 32 Meijeren, M. (Maikel) MSc Social capital, volunteer work and diversity meijeren maikel radboud university 33 Mensvoort, C.A. van (Carly) MSc Gender, leadership and social norms mensvoort carly radboud university 34 Müller, K. (Katrin) MSc Opinions about discrimination, migration and inequality müller katrin radboud university 35 Raiber, K. (Klara) MSc Informal care, employment, social inequality and gender raiber klara radboud university 36 Ramaekers, M.J.M. (Marlou) MSc Prosocial behaviour and family ramaekers marlou radboud university 37 Wiertsema, S. (Sara) MSc Inequality in sports and physical activity, school-to-work transition and employment wiertsema sara radboud university 39 Betkó, drs. J.G. (János) Social assistance benefit, poverty, reintegration, RCT and social experiment betkó jános radboud university 40 Houten, J. (Jasper) van MSc Sports houten jasper radboud university 41 Middendorp J. (Jansje) van MSc Home administration middendorp jansje radboud university 42 Vis, E. (Elize) MSc Healthcare, labour market, healthcare professions and training, health and social capital vis elize radboud university 43 Weber, T. (Tijmen) MSc International student mobility and the internationalization of higher education weber tijmen radboud university 45 Sterkens, dr. C.J.A. (Carl) Religious conflicts, cohesion, religion and the philosophy of life sterkens carl radboud university 46 Vermeer, dr. P.A.D.M. (Paul) Socialization processes, secularisation, religion and the philosophy of life vermeer paul radboud university Pretty good, so I think we can move on to the next section. 11.6.3 Google Scholar Profiles and Publications What we now have is a data frame of sociology staff members. So we successfully gathered the anchor data set we can move on with. Next, we need to find out whether these staff have a Google Scholar profile. I imagine you have accessed Google Scholar many times during your studies for finding scientists or publications. The nice thing about Google Scholar is that it lists collaborators, publications, and citations on profiles. So what we first need to do is look for Google Scholar profiles among sociology staff. Luckily, we cleaned first and last names and have their affiliation. That makes looking them up much easier. So we need to do this for every person in our data frame. Before we query Google Scholar, we first need to go back to the neat trick of for loops (remember them from the A?). Can you follow the code below? We can thus do all kinds of things automatically in a for loop. # The &#39;for loop&#39;: for every i in a vector (can be numbers, strings, etc.), say 1 to 10, you can do # &#39;something&#39; for (i in 1:10) { print(i) # So for every i from 1 to 10, we print i, see what happens! } #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 #&gt; [1] 4 #&gt; [1] 5 #&gt; [1] 6 #&gt; [1] 7 #&gt; [1] 8 #&gt; [1] 9 #&gt; [1] 10 # # or do something more complicated p &lt;- rnorm(10, 0, 1) # draw 10 normally distributed numbers # with mean 0 and SD 1 (so z-scores, essentially) plot(density(p)) # relatively, normal, right? u # &lt;- 0 # make an element we can fill up in the loop below for (i in 1:10) { u[i] &lt;- p[i]*p[i] # get # p-squared for every i-th element in vector p print(u[i]) # and print that squared element } Now that we know how to implement for loops in our workflow, we can utilize them to do slightly more complicated stuff. We want to know the identifying link on Google Scholar for each sociology staff member. We first set an empty identifier in our data frame so that we can fill up that data column later. soc_df$gs_id &lt;- &quot;&quot; # we set an empty identifier So lets move on with attempting to find Google Scholar profiles. The package scholar has a range of very nice functions! What type of functions does scholar have? Take a look by ?scholar or ??scholar. It includes all kinds of interesting functions like comparing scholars careers, getting scholar citations, getting profiles, and so forth. Using get_scholar_id seems appropriate to find out what the profile ID is of Jochems Google Scholar page. If you write get_scholar_id and then click ctrl or cmd together with your right mouse button, you can actually see precisely what the function does (its quite complicated!). Note that this package does not use an API, but simple wrote code to extract Google Scholar pages from the internet. They then wrapped the code in much simpler functions for you to use. The function get_scholar_id needs a last name, first name, and affiliation. Luckily, we already found those on the Radboud University website! So we can fill in those. Lets try it for one staff member first. source(&quot;addfiles/function_fix.R&quot;) # Put the function_fix.R in your working directory, we need this first line. get_scholar_id_fix(last_name = &quot;tolsma&quot;, first_name = &quot;jochem&quot;, affiliation = &quot;radboud university&quot;) #&gt; [1] &quot;Iu23-90AAAAJ&quot; We now know that Jochems Scholar ID is Iu23-90AAAAJ. Thats very convenient, because now we can use the package scholar again to extract a range of useful other information from his Google Scholar profile. Lets try it out on his profile first. Notice the nice function get_profiles. We simply have to input his Google Scholar ID and it shows everything on the profile. get_profile(&quot;Iu23-90AAAAJ&quot;) # Jochem&#39;s profile #&gt; $id #&gt; [1] &quot;Iu23-90AAAAJ&quot; #&gt; #&gt; $name #&gt; [1] &quot;Jochem Tolsma&quot; #&gt; #&gt; $affiliation #&gt; [1] &quot;Professor, Radboud University Nijmegen / University of Groningen&quot; #&gt; #&gt; $total_cites #&gt; [1] 2286 #&gt; #&gt; $h_index #&gt; [1] 22 #&gt; #&gt; $i10_index #&gt; [1] 33 #&gt; #&gt; $fields #&gt; [1] &quot;verified email at ru.nl - homepage&quot; #&gt; #&gt; $homepage #&gt; [1] &quot;http://www.jochemtolsma.nl/&quot; #&gt; #&gt; $interests #&gt; [1] &quot;social divisions between groups&quot; &quot;segregation&quot; #&gt; [3] &quot;inequality&quot; #&gt; #&gt; $coauthors #&gt; [1] &quot;Tom van der Meer&quot; &quot;Maarten HJ Wolbers&quot; &quot;Gerbert Kraaykamp&quot; &quot;peer scheepers&quot; #&gt; [5] &quot;Michael Savelkoul&quot; &quot;Stijn Ruiter&quot; &quot;Marcel Lubbers&quot; &quot;Maurice Gesthuizen&quot; #&gt; [9] &quot;Marcel Coenders&quot; &quot;Nan Dirk de Graaf&quot; &quot;Tobias H. Stark&quot; &quot;Sara Kinsbergen&quot; #&gt; [13] &quot;Christiaan Monden&quot; &quot;Matthijs Kalmijn&quot; &quot;Lincoln Quillian&quot; &quot;Marloes de Lange&quot; #&gt; [17] &quot;Thomas Feliciani&quot; &quot;Andreas Flache&quot; &quot;Ariana Need&quot; &quot;René Veenstra&quot; A lot of useful information! Next up, Jochems publications. Notice how not everything is in a nice data frame format yet, well get to that later. get_publications(&quot;Iu23-90AAAAJ&quot;) # Jochem&#39;s pubs #&gt; title #&gt; 1 Ethnic diversity and its effects on social cohesion #&gt; 2 Anti-Muslim attitudes in the Netherlands: Tests of contradictory hypotheses derived from ethnic competition theory and intergroup contact theory #&gt; 3 The impact of neighbourhood and municipality characteristics on social cohesion in the Netherlands #&gt; 4 The effects of parental reading socialization and early school involvement on childrens academic performance: A panel study of primary school pupils in the Netherlands #&gt; 5 Ethnic competition and opposition to ethnic intermarriage in the Netherlands: A multi-level approach #&gt; 6 Who is bullying whom in ethnically diverse primary schools? Exploring links between bullying, ethnicity, and ethnic diversity in Dutch primary schools #&gt; 7 When do people report crime to the police? Results from a factorial survey design in the Netherlands, 2010 #&gt; 8 Education and cultural integration among ethnic minorities and natives in the Netherlands: A test of the integration paradox #&gt; 9 Trends in ethnic educational inequalities in the Netherlands: a cohort design #&gt; 10 Does intergenerational social mobility affect antagonistic attitudes towards ethnic minorities? #&gt; 11 Explaining participation differentials in Dutch higher education: the impact of subjective success probabilities on level choice and field choice #&gt; 12 The impact of adolescents&#39; classroom and neighborhood ethnic diversity on same-and cross-ethnic friendships within classrooms #&gt; 13 Neighbourhood ethnic composition and voting for the radical right in The Netherlands. The role of perceived neighbourhood threat and interethnic neighbourhood contact #&gt; 14 Losing Wallets, Retaining Trust? The Relationship Between Ethnic Heterogeneity and Trusting Coethnic and Non-coethnic Neighbours and Non-neighbours to Return a Lost Wallet #&gt; 15 At which geographic scale does ethnic diversity affect intra-neighborhood social capital? #&gt; 16 Educational expansion and field of study: trends in the intergenerational transmission of educational inequality in the Netherlands #&gt; 17 The NEtherlands Longitudinal Lifecourse Study (NELLS, Panel): Codebook #&gt; 18 Bringing the beneficiary closer: Explanations for volunteering time in Dutch private development initiatives #&gt; 19 Onderwijs als nieuwe sociale scheidslijn? De gevolgen van onderwijsexpansie voor sociale mobiliteit, de waarde van diplomas en het relatieve belang van opleiding in Nederland #&gt; 20 Naar een open samenleving? #&gt; 21 Explaining monetary donations to international development organisations: A factorial survey approach #&gt; 22 Social origin and occupational success at labour market entry in The Netherlands, 193180 #&gt; 23 How friends involvement in crime affects the risk of offending and victimization #&gt; 24 How, when and where can spatial segregation induce opinion polarization? Two competing models #&gt; 25 Perceptions as the crucial link? The mediating role of neighborhood perceptions in the relationship between the neighborhood context and neighborhood cohesion #&gt; 26 VU Research Portal #&gt; 27 Ethnic hostility among ethnic majority and minority groups in the Netherlands: An investigation into the impact of social mobility experiences, the local living environment and  #&gt; 28 Explaining natives&#39; interethnic friendship and contact with colleagues in European regions #&gt; 29 Netherlands Longitudinal Lifecourse Study-NELLS Panel Wave 1 2009 and Wave 2 2013-version 1.1 #&gt; 30 Trust and contact in diverse neighbourhoods: An interplay of four ethnicity effects #&gt; 31 Combating hooliganism in the Netherlands: An evaluation of measures to combat hooliganism with longitudinal registration data #&gt; 32 De onderwijskansen van allochtone en autochtone Nederlanders vergeleken: Een cohort-design [Ethnic inequality of educational opportunities in the Netherlands: A cohort design] #&gt; 33 Size is in the eye of the beholder: How differences between neighbourhoods and individuals explain variation in estimations of the ethnic out-group size in the neighbourhood #&gt; 34 Like two peas in a pod? Explaining friendship selection processes related to victimization and offending #&gt; 35 Aangiftebereidheid: Welke overwegingen spelen een rol bij de beslissing om wel of niet aangifte te doen? #&gt; 36 De aangifte van delicten bij de multichannelstrategie van de politie #&gt; 37 Parents, television and children&#39;s weight status: On lasting effects of parental television socialization in the Netherlands #&gt; 38 Taakstraffen langs de lat: strafopvattingen van Nederlanders #&gt; 39 Co-occurrence of adverse childhood experiences and its association with family characteristics. A latent class analysis with Dutch population data #&gt; 40 Opleiding als sociale scheidslijn. Een nieuw perspectief op een oude kloof. #&gt; 41 Archeologische verwachtings-en beleidskaart buitengebied gemeente Midden-Drenthe #&gt; 42 Under what conditions do ethnic minority candidates attract the ethnic minority vote? How neighbourhood and candidate characteristics affected ethnic affinity voting in the  #&gt; 43 Where does ethnic concentration matter for populist radical right support? An analysis of geographical scale and the halo effect #&gt; 44 Vrijheid versus veiligheid: Wie steunt vrijheidsbeperking omwille van veiligheid in Nederland? #&gt; 45 Opleiding als sociale scheidslijn #&gt; 46 Dader, slachtoffer, of beiden? #&gt; 47 Education&#39;s impact on explanations of radical right-wing voting #&gt; 48 Social origin and inequality in educational returns in the Dutch labour market #&gt; 49 Opleiding als sociale scheidslijn: Een nieuw perspectief op een oude kloof #&gt; 50 De burger als rechter #&gt; 51 Exposure to asylum seekers and changing support for the radical right: A natural experiment in the Netherlands #&gt; 52 Ontwikkelingen in de maatschappelijke positie van middelbaar opgeleiden in Nederland #&gt; 53 Integratie en depressie: De relatie tussen sociaal-culturele integratie en depressieklachten bij Turkse en Marokkaanse Nederlanders #&gt; 54 Integratie en depressie-De relatie tussen sociaal-culturele integratie en depressieklachten bij Turkse en Marokkaanse Nederlander #&gt; 55 Integratie en depressie #&gt; 56 Preferences for work arrangements: A discrete choice experiment #&gt; 57 Fairly paid but dissatisfied? Determinants of pay fairness and pay satisfaction: Evidence from Germany and the Netherlands #&gt; 58 In hoeverre verklaart de etnische samenstelling van de buurt de kans om te stemmen op de PVV? #&gt; 59 Soort zoekt soort: vriendschapselectieprocessen met betrekking tot slachtofferschap en daderschap #&gt; 60 Joran Laméris Radboud University Nijmegen #&gt; 61 Modeling opinion dynamics in a real city: How realistic spatial patterns of demographic attributes affect the emergence of opinion polarization #&gt; 62 Summary of How, When and Where Can Spatial Segregation Induce Opinion Polarization? Two Competing Models: Paper Under Review as JASSS Fast Track Submission #&gt; 63 ICS Alumni revisited [Brochure tbv Lustrum ICS] #&gt; 64 ICS Alumni Survey 2016 [Databestand] #&gt; 65 Modeling opinion dynamics in a simulated city. Realistic spatial patterns of demographic attributes and the emergence of opinion polarization #&gt; 66 De invloed van grootouders op het opleidingsniveau van kleinkinderen #&gt; 67 Opleiding als sociale scheidslijn: Een tegengeluid #&gt; 68 Opleiding als sociale scheidslijn: aanleiding, probleemstelling, bestaande inzichten en werkwijze #&gt; 69 De samenhang in het opleidingsniveau tussen (huwelijks) partners #&gt; 70 Sociale herkomst en ongelijkheid in de opbrengsten van diploma&#39;s op de arbeidsmarkt #&gt; 71 De rol van het opleidingsniveau voor het starten en stoppen met vrijwilligerswerk #&gt; 72 Trends in de opleidingskloof op verschillende maatschappelijke domeinen #&gt; 73 Was, is of wordt opleiding de sociale scheidslijn? Een terugblik, stand van zaken en toekomstvisie #&gt; 74 DANS EASY #&gt; 75 Klein en vrijwillig of groot en ervaren? Een analyse van de voorkeuren van donateurs #&gt; 76 De burger als rechter, onderzoek naar geprefereerde sancties voor misdrijven in Nederland (projectnummer 1933B) #&gt; 77 Aangiftebereidheid: Welke overwegingen spelen een rol bij de beslissing om wel of niet aangifte te doen?| #&gt; 78 Onderwijsexpansie veroorzaakt nieuwe ongelijkheid #&gt; 79 Sociale daling schaadt vertrouwen: Effecten van sociale mobiliteit #&gt; 80 Toenemende gelijkheid is nog geen verheffing #&gt; 81 Over ouders, televisiekijken en (over) gewicht: Een studie naar de langetermijneffecten van ouderlijke televisiesocialisatie op het lichaamsgewicht van kinderen #&gt; 82 Onderwijsexpansie en opleidingsrichting: Trends in de intergenerationele overdracht van onderwijsongelijkheid #&gt; 83 Does Intergenerational Social Mobility affect Antagonistic Attitudes towards Ethnic Minorities #&gt; 84 De invloed van lands-, gemeente-en buurtkenmerken op sociaal kapitaal: Putnam&#39;s hypothese getest in Europa en Nederland #&gt; 85 Causes of dyads (theory) #&gt; 86 Egocentric Networks #&gt; 87 Onbeperkte mogelijkheden 122 #&gt; 88 De invloed van lands-, gemeente-en buurtkenmerken op sociaal kapitaal Gesthuizen, M.; Scheepers, P.; Tolsma, J.; Meer, TWG van der #&gt; 89 The Role of Recent Migrants Country of Origin Engagement in Dutch Language Proficiency #&gt; 90 Notes on Contributors Female Education and Marriage Dissolution: Is it a Selection Effect? Fabrizio Bernardi and Juan-Ignacio Martinez-Pastor 693 Age, Inequality, and Reactions  #&gt; 91 Notes on Contributors The Effects of Parental Reading Socialization and Early School Involvement on Childrens Academic Performance: A Panel Study of Primary School Pupils in  #&gt; 92 M1-102: Social capital and networks #&gt; 93 Social origin and occupational success at labour market entry in the Netherlands #&gt; author #&gt; 1 T Van der Meer, J Tolsma #&gt; 2 M Savelkoul, P Scheepers, J Tolsma, L Hagendoorn #&gt; 3 J Tolsma, T van der Meer, M Gesthuizen #&gt; 4 R Kloosterman, N Notten, J Tolsma, G Kraaykamp #&gt; 5 J Tolsma, M Lubbers, M Coenders #&gt; 6 J Tolsma, I van Deurzen, TH Stark, R Veenstra #&gt; 7 J Tolsma, J Blaauw, M Te Grotenhuis #&gt; 8 J Tolsma, M Lubbers, M Gijsberts #&gt; 9 J Tolsma, M Coenders, M Lubbers #&gt; 10 J Tolsma, ND De Graaf, L Quillian #&gt; 11 J Tolsma, A Need, U De Jong #&gt; 12 A Munniksma, P Scheepers, TH Stark, J Tolsma #&gt; 13 M Savelkoul, J Laméris, J Tolsma #&gt; 14 J Tolsma, TWG van der Meer #&gt; 15 R Sluiter, J Tolsma, P Scheepers #&gt; 16 G Kraaykamp, J Tolsma, MHJ Wolbers #&gt; 17 J Tolsma, GLM Kraaykamp, PM de Graaf, M Kalmijn, CWS Monden #&gt; 18 S Kinsbergen, J Tolsma, S Ruiter #&gt; 19 J Tolsma, MHJ Wolbers #&gt; 20 J Tolsma, MHJ Wolbers #&gt; 21 S Kinsbergen, J Tolsma #&gt; 22 J Tolsma, MHJ Wolbers #&gt; 23 JJ Rokven, G de Boer, J Tolsma, S Ruiter #&gt; 24 T Feliciani, A Flache, J Tolsma #&gt; 25 J Laméris, JR Hipp, J Tolsma #&gt; 26 S Ruiter, J Tolsma, M De Hoon, H Elffers, P Van der Laan #&gt; 27 J Tolsma #&gt; 28 M Savelkoul, J Tolsma, P Scheepers #&gt; 29 J Tolsma, GLM Kraaykamp, PM De Graaf, M Kalmijn, CM Monden #&gt; 30 J Tolsma, TWG Van der Meer #&gt; 31 D Schaap, M Postma, L Jansen, J Tolsma #&gt; 32 J Tolsma, MTA Coenders, M Lubbers #&gt; 33 J Laméris, G Kraaykamp, S Ruiter, J Tolsma #&gt; 34 JJ Rokven, J Tolsma, S Ruiter, G Kraaykamp #&gt; 35 J Tolsma #&gt; 36 PFM Boekhoorn, J Tolsma #&gt; 37 N Notten, G Kraaykamp, J Tolsma #&gt; 38 S Ruiter, J Tolsma #&gt; 39 C Bussemakers, G Kraaykamp, J Tolsma #&gt; 40 M de Lange, J Tolsma, MHJ Wolbers #&gt; 41 MG Marinelli, J Tolsma #&gt; 42 R van der Zwan, J Tolsma, M Lubbers #&gt; 43 D van Wijk, G Bolt, J Tolsma #&gt; 44 G Jansen, J Tolsma, ND de Graaf #&gt; 45 M Lange, J Tolsma, MHJ Wolbers #&gt; 46 J Rokven, S Ruiter, J Tolsma #&gt; 47 M Lubbers, J Tolsma #&gt; 48 J Tolsma, MHJ Wolbers #&gt; 49 M Lange, J Tolsma, MHJ Wolbers #&gt; 50 S Ruiter, J Tolsma, M Hoon, H Elffers, P Laan #&gt; 51 J Tolsma, J Laméris, M Savelkoul #&gt; 52 J Tolsma, MHJ Wolbers #&gt; 53 R Zwan, J Tolsma #&gt; 54 R van der Zwan, J Tolsma #&gt; 55 R van der Zwan, J Tolsma #&gt; 56 P Valet, C Sauer, J Tolsma #&gt; 57 J Adriaans, CG Sauer, J Tolsma #&gt; 58 M Savelkoul, J Laméris, J Tolsma #&gt; 59 J Rokven, J Tolsma, S Ruiter, G Kraaykamp #&gt; 60 JR Hipp, J Tolsma #&gt; 61 T Feliciani, A Flache, J Tolsma #&gt; 62 T Feliciani, A Flache, J Tolsma #&gt; 63 PE Thijs, GLM Kraaykamp, M Scholte, J Tolsma #&gt; 64 GLM Kraaykamp, M Scholte, PE Thijs, J Tolsma #&gt; 65 T Feliciani, A Flache, J Tolsma, M Maes #&gt; 66 MHJ Wolbers, WC Ultee #&gt; 67 J Tolsma #&gt; 68 M de Lange, J Tolsma, MHJ Wolbers #&gt; 69 J Tolsma, ND de Graaf #&gt; 70 J Tolsma, MHJ Wolbers #&gt; 71 D Wiertz, J Tolsma, ND de Graaf #&gt; 72 M Lange, J Tolsma #&gt; 73 J Tolsma, MHJ Wolbers #&gt; 74 J Tolsma, GLM Kraaykamp, DM de Graaf, M Kalmijn, C Monden #&gt; 75 S Kinsbergen, J Tolsma #&gt; 76 S Ruiter, J Tolsma, M Hoon, H Elffers, D Laan #&gt; 77 J Tolsma #&gt; 78 GLM Kraaykamp, MHJ Wolbers, J Tolsma #&gt; 79 J Tolsma, MHJ Wolbers #&gt; 80 MHJ Wolbers, J Tolsma #&gt; 81 N Notten, GLM Kraaykamp, J Tolsma #&gt; 82 GLM Kraaykamp, J Tolsma, MHJ Wolbers #&gt; 83 NDG de Graaf, J Tolsma, L Quillian #&gt; 84 MJW Gesthuizen, PLH Scheepers, J Tolsma, TWG van der Meer #&gt; 85 J Tolsma #&gt; 86 J Tolsma #&gt; 87 T Fischer, J Winkels, M Visser, M Gesthuizen, P Scheepers, A ten Cate, ... #&gt; 88 M Gesthuizen #&gt; 89 N Geurts, J Tolsma #&gt; 90 P Horvat, G Evans, G Rohwer, M Savelkoul, P Scheepers, J Tolsma, ... #&gt; 91 R Kloosterman, N Notten, J Tolsma, G Kraaykamp, J Hansson, ... #&gt; 92 J Laméris, J Tolsma, J Hipp #&gt; 93 J Tolsma, MHJ Wolbers #&gt; journal #&gt; 1 Annual Review of Sociology #&gt; 2 European sociological review #&gt; 3 Acta Politica #&gt; 4 European Sociological Review #&gt; 5 European Sociological Review #&gt; 6 Social Networks #&gt; 7 Journal of experimental criminology #&gt; 8 Journal of Ethnic and Migration Studies #&gt; 9 European Sociological Review #&gt; 10 The British Journal of Sociology #&gt; 11 European Sociological Review #&gt; 12 Journal of Research on Adolescence #&gt; 13 European Sociological Review #&gt; 14 Social Indicators Research #&gt; 15 Social science research #&gt; 16 British Journal of Sociology of Education #&gt; 17 Nijmegen; Tilburg; Amsterdam: Radboud University Nijmegen; Tilburg  #&gt; 18 Nonprofit and Voluntary Sector Quarterly #&gt; 19 #&gt; 20 #&gt; 21 Social science research #&gt; 22 Acta Sociologica #&gt; 23 European journal of criminology #&gt; 24 #&gt; 25 Social science research #&gt; 26 #&gt; 27 [Sl]: sn [ICS dissertation series #&gt; 28 Journal of Ethnic and Migration Studies #&gt; 29 DANS. DOI: https://doi. org/ #&gt; 30 Social science research #&gt; 31 European Journal on Criminal Policy and Research #&gt; 32 #&gt; 33 International Journal of Intercultural Relations #&gt; 34 European Journal of Criminology #&gt; 35 Proces-verbaal, aangifte en forensisch onderzoek #&gt; 36 Apeldoorn; Nijmegen: Politie &amp; Wetenschap; BBSO en Radboud Universiteit #&gt; 37 Journal of Children and Media #&gt; 38 #&gt; 39 Child abuse &amp; neglect #&gt; 40 Maklu #&gt; 41 Oranjewoud, Heerenveen #&gt; 42 Political Geography #&gt; 43 Political Geography #&gt; 44 Mens en maatschappij #&gt; 45 #&gt; 46 Tijdschrift voor Criminologie #&gt; 47 London: University College London #&gt; 48 Education, Occupation and Social Origin #&gt; 49 Antwerpen/Apeldoorn: Garant #&gt; 50 NSCR #&gt; 51 PLoS One #&gt; 52 Den Haag: WRR #&gt; 53 #&gt; 54 Mens en maatschappij #&gt; 55 De relatie tussen sociaal #&gt; 56 PloS one #&gt; 57 New York, NY: Society for the Advancement of Socio-Economics #&gt; 58 Mens en Maatschappij #&gt; 59 Mens en maatschappij #&gt; 60 #&gt; 61 #&gt; 62 International Conference on Principles and Practice of Multi-Agent Systems  #&gt; 63 Nijmegen: Interuniversity Center for Social Science Theory and Methodology (ICS) #&gt; 64 Nijmegen: Interuniversity Center for Social Science Theory and Methodology (ICS) #&gt; 65 Social Simulation Conference #&gt; 66 Lange, M. de; Tolsma, J.; Wolbers, MHJ (ed.), Opleiding als sociale  #&gt; 67 Sociologos: Tijdschrift voor Sociologie #&gt; 68 Opleiding als sociale scheidslijn. Een nieuw perspectief op een oude kloof #&gt; 69 Antwerpen/Apeldoorn: Garant #&gt; 70 Antwerpen/Apeldoorn: Garant #&gt; 71 Lange, M. de; Tolsma, J.; Wolbers, MHJ (ed.), Opleiding als sociale  #&gt; 72 Antwerpen/Apeldoorn: Garant #&gt; 73 Lange, M. de; Tolsma, J.; Wolbers, MHJ (ed.), Opleiding als sociale  #&gt; 74 DANS EASY #&gt; 75 Amsterdam: NCDO #&gt; 76 DANS EASY #&gt; 77 Cahiers Politiestudies #&gt; 78 [Sl]: Sociale Vraagstukken #&gt; 79 #&gt; 80 #&gt; 81 Assen: Van Gorcum #&gt; 82 Amsterdam: AUP #&gt; 83 Wiley Blackwell #&gt; 84 Den Haag/Nijmegen: SCP-NSV #&gt; 85 #&gt; 86 #&gt; 87 #&gt; 88 #&gt; 89 #&gt; 90 #&gt; 91 #&gt; 92 Book of Abstracts #&gt; 93 #&gt; number cites year #&gt; 1 40 (1), 459-478 434 2014 #&gt; 2 27 (6), 741-758 289 2011 #&gt; 3 44 (3) 268 2009 #&gt; 4 27 (3), 291-306 120 2011 #&gt; 5 24 (2), 215-230 120 2008 #&gt; 6 35 (1), 51-61 107 2013 #&gt; 7 8 (2), 117-134 76 2012 #&gt; 8 38 (5), 793-813 70 2012 #&gt; 9 23 (3), 325-339 68 2007 #&gt; 10 60 (2), 257-277 63 2009 #&gt; 11 26 (2), 235-252 62 2010 #&gt; 12 27 (1), 20-33 43 2017 #&gt; 13 33 (2), 209-224 37 2017 #&gt; 14 35 2016 #&gt; 15 54, 80-95 33 2015 #&gt; 16 34 (5-6), 888-906 32 2013 #&gt; 17 31 2014 #&gt; 18 42 (1), 59-83 29 2013 #&gt; 19 28 2010 #&gt; 20 26 2010 #&gt; 21 42 (6), 1571-1586 24 2013 #&gt; 22 57 (3), 253-269 22 2014 #&gt; 23 14 (6), 697-719 19 2017 #&gt; 24 19 2017 #&gt; 25 72, 53-68 17 2018 #&gt; 26 17 2011 #&gt; 27 155] 17 2009 #&gt; 28 41 (5), 683-709 15 2015 #&gt; 29 /10.17026/dans-25n-2xjv 14 2014 #&gt; 30 73, 92-106 13 2018 #&gt; 31 21 (1), 83-97 13 2015 #&gt; 32 13 2007 #&gt; 33 63, 80-94 11 2018 #&gt; 34 13 (2), 231-256 9 2016 #&gt; 35 11 9 2011 #&gt; 36 7 2016 #&gt; 37 7 (2), 235-252 7 2013 #&gt; 38 7 2010 #&gt; 39 98, 104185 6 2019 #&gt; 40 6 2015 #&gt; 41 6 2009 #&gt; 42 77, 102098 5 2020 #&gt; 43 77, 102097 5 2020 #&gt; 44 83 (1), 47-69 5 2008 #&gt; 45 4 2016 #&gt; 46 55 (3), 278 4 2013 #&gt; 47 4 2011 #&gt; 48 3 2016 #&gt; 49 3 2015 #&gt; 50 3 2011 #&gt; 51 16 (2), e0245644 2 2021 #&gt; 52 2 2017 #&gt; 53 2 2013 #&gt; 54 88 (2), 177-205 1 2013 #&gt; 55 1 2013 #&gt; 56 16 (7), e0254483 0 2021 #&gt; 57 0 2019 #&gt; 58 93 (1), 82-85 0 2018 #&gt; 59 92 (3), 327-329 0 2017 #&gt; 60 0 2017 #&gt; 61 9th Conference of the International Network of Analytical Sociology  0 2016 #&gt; 62 0 2016 #&gt; 63 0 2016 #&gt; 64 0 2016 #&gt; 65 2016 0 2016 #&gt; 66 0 2015 #&gt; 67 36, 276-285 0 2015 #&gt; 68 9-32 0 2015 #&gt; 69 0 2015 #&gt; 70 0 2015 #&gt; 71 0 2015 #&gt; 72 0 2015 #&gt; 73 0 2015 #&gt; 74 0 2014 #&gt; 75 0 2014 #&gt; 76 0 2013 #&gt; 77 2 (4), 11 0 2012 #&gt; 78 0 2011 #&gt; 79 0 2011 #&gt; 80 0 2011 #&gt; 81 0 2011 #&gt; 82 0 2011 #&gt; 83 0 2009 #&gt; 84 0 2009 #&gt; 85 0 NA #&gt; 86 0 NA #&gt; 87 0 NA #&gt; 88 0 NA #&gt; 89 0 NA #&gt; 90 0 NA #&gt; 91 0 NA #&gt; 92 1 0 NA #&gt; 93 0 NA #&gt; cid pubid #&gt; 1 17240473400423700490,461159763596233481,1315542974843119305 UxriW0iASnsC #&gt; 2 9140218593636983243 9yKSN-GCB0IC #&gt; 3 203105297399726489 UeHWp8X0CEIC #&gt; 4 9327830809512404486 qjMakFHDy7sC #&gt; 5 17191703704621608544 u5HHmVD_uO8C #&gt; 6 15442728615805262127 kNdYIx-mwKoC #&gt; 7 3147100585201897138 UebtZRa9Y70C #&gt; 8 16121967639591190378 eQOLeE2rZwMC #&gt; 9 5904489841843560927 d1gkVwhDpl0C #&gt; 10 10446633547221929964 2osOgNQ5qMEC #&gt; 11 18143881066769803140,18233438384904663264,12975380653095517868 Tyk-4Ss8FVUC #&gt; 12 18309594979069207516 maZDTaKrznsC #&gt; 13 4894344398065441656,17805961515959316077 ldfaerwXgEUC #&gt; 14 2251620908592189324 BqipwSGYUEgC #&gt; 15 7670225499012303854 e5wmG9Sq2KIC #&gt; 16 2401615506068930127 7PzlFSSx8tAC #&gt; 17 8792123396141403739 xtRiw3GOFMkC #&gt; 18 2112276567018030922 _FxGoFyzp5QC #&gt; 19 16059273116934807949 YsMSGLbcyi4C #&gt; 20 2539524527836644253 Y0pCki6q_DkC #&gt; 21 10149692484122806616 aqlVkmm33-oC #&gt; 22 8248470043986462984 M3ejUd6NZC8C #&gt; 23 13322468554278639475 vV6vV6tmYwMC #&gt; 24 6880814424039971499 g5m5HwL7SMYC #&gt; 25 16357054384393453824 D03iK_w7-QYC #&gt; 26 5199682358769198644 KlAtU1dfN6UC #&gt; 27 10378332126833599949 IjCSPb-OGe4C #&gt; 28 18182577779862774305 -f6ydRqryjwC #&gt; 29 9375222806902931665,3745616990512837825 mB3voiENLucC #&gt; 30 8349908030823257502 pyW8ca7W8N0C #&gt; 31 9528443224826780083 ZeXyd9-uunAC #&gt; 32 41511425553822262 zYLM7Y9cAGgC #&gt; 33 1627288244325129498 a0OBvERweLwC #&gt; 34 16075774780598089063 k_IJM867U9cC #&gt; 35 10745397192148013810 LkGwnXOMwfcC #&gt; 36 14256154602665082067 JV2RwH3_ST0C #&gt; 37 818925813101569366 Se3iqnhoufwC #&gt; 38 15258532569899652859 W7OEmFMy1HYC #&gt; 39 163003866819331000 CHSYGLWDkRkC #&gt; 40 6027896113597554400 isC4tDSrTZIC #&gt; 41 641362829363743487 kzcrU_BdoSEC #&gt; 42 7114430646392466648 uWQEDVKXjbEC #&gt; 43 4092382021694339447 SP6oXDckpogC #&gt; 44 6273244451878075724 ufrVoPGSRksC #&gt; 45 4589290607551316207 O3NaXMp0MMsC #&gt; 46 10200636729873805270 QIV2ME_5wuYC #&gt; 47 10658172101302530460 4TOpqqG69KYC #&gt; 48 10107474183324844052 YFjsv_pBGBYC #&gt; 49 8633532378010504541 p2g8aNsByqUC #&gt; 50 16411127097378483929 ns9cj8rnVeAC #&gt; 51 10820360089296230361 Fu2w8maKXqMC #&gt; 52 12961270881488694754 u_35RYKgDlwC #&gt; 53 10798588282304546316 35N4QoGY0k4C #&gt; 54 53571672086000399 dhFuZR0502QC #&gt; 55 12114359551598956835 f2IySw72cVMC #&gt; 56 &lt;NA&gt; tKAzc9rXhukC #&gt; 57 &lt;NA&gt; OU6Ihb5iCvQC #&gt; 58 &lt;NA&gt; b0M2c_1WBrUC #&gt; 59 &lt;NA&gt; dfsIfKJdRG4C #&gt; 60 &lt;NA&gt; u9iWguZQMMsC #&gt; 61 &lt;NA&gt; 7T2F9Uy0os0C #&gt; 62 &lt;NA&gt; NJ774b8OgUMC #&gt; 63 &lt;NA&gt; lSLTfruPkqcC #&gt; 64 &lt;NA&gt; RYcK_YlVTxYC #&gt; 65 &lt;NA&gt; NaGl4SEjCO4C #&gt; 66 &lt;NA&gt; nb7KW1ujOQ8C #&gt; 67 &lt;NA&gt; NMxIlDl6LWMC #&gt; 68 &lt;NA&gt; TFP_iSt0sucC #&gt; 69 &lt;NA&gt; bEWYMUwI8FkC #&gt; 70 &lt;NA&gt; iH-uZ7U-co4C #&gt; 71 &lt;NA&gt; r0BpntZqJG4C #&gt; 72 &lt;NA&gt; j3f4tGmQtD8C #&gt; 73 &lt;NA&gt; 4JMBOYKVnBMC #&gt; 74 &lt;NA&gt; XiSMed-E-HIC #&gt; 75 &lt;NA&gt; yD5IFk8b50cC #&gt; 76 &lt;NA&gt; 738O_yMBCRsC #&gt; 77 &lt;NA&gt; P5F9QuxV20EC #&gt; 78 &lt;NA&gt; _kc_bZDykSQC #&gt; 79 &lt;NA&gt; ULOm3_A8WrAC #&gt; 80 &lt;NA&gt; Zph67rFs4hoC #&gt; 81 &lt;NA&gt; 3fE2CSJIrl8C #&gt; 82 &lt;NA&gt; 5nxA0vEk-isC #&gt; 83 &lt;NA&gt; dshw04ExmUIC #&gt; 84 &lt;NA&gt; YOwf2qJgpHMC #&gt; 85 &lt;NA&gt; WbkHhVStYXYC #&gt; 86 &lt;NA&gt; Tiz5es2fbqcC #&gt; 87 &lt;NA&gt; 1sJd4Hv_s6UC #&gt; 88 &lt;NA&gt; cFHS6HbyZ2cC #&gt; 89 &lt;NA&gt; 4OULZ7Gr8RgC #&gt; 90 &lt;NA&gt; rO6llkc54NcC #&gt; 91 &lt;NA&gt; 3s1wT3WcHBgC #&gt; 92 &lt;NA&gt; M05iB0D1s5AC #&gt; 93 &lt;NA&gt; 70eg2SAEIzsC When and how often was Jochem cited? Seems like an increasing trend line! get_citation_history(&quot;Iu23-90AAAAJ&quot;) # Jochem&#39;s citation history #&gt; year cites #&gt; 1 2008 12 #&gt; 2 2009 21 #&gt; 3 2010 26 #&gt; 4 2011 79 #&gt; 5 2012 79 #&gt; 6 2013 116 #&gt; 7 2014 151 #&gt; 8 2015 204 #&gt; 9 2016 228 #&gt; 10 2017 223 #&gt; 11 2018 268 #&gt; 12 2019 297 #&gt; 13 2020 303 #&gt; 14 2021 254 And now most importantly, Jochems collaborators, and the collaborators of those collaborators (note the n_deep = 1, can you find out what that does?). So essentially a one-step-further-than-Jochem network. jochem_coauthors &lt;- get_coauthors(&quot;Iu23-90AAAAJ&quot;, n_coauthors = 50, n_deep = 1) # Jochem&#39;s collaborators and their co-authors! jochem_coauthors Table 11.6: Sociology staff cleanest? author coauthors 1 Jochem Tolsma Tom Van Der Meer 2 Jochem Tolsma Maarten Hj Wolbers 3 Jochem Tolsma Gerbert Kraaykamp 4 Jochem Tolsma Peer Scheepers 5 Jochem Tolsma Michael Savelkoul 6 Jochem Tolsma Stijn Ruiter 7 Jochem Tolsma Marcel Lubbers 8 Jochem Tolsma Maurice Gesthuizen 9 Jochem Tolsma Marcel Coenders 10 Jochem Tolsma Nan Dirk De Graaf 11 Jochem Tolsma Tobias H. Stark 12 Jochem Tolsma Sara Kinsbergen 13 Jochem Tolsma Christiaan Monden 14 Jochem Tolsma Matthijs Kalmijn 15 Jochem Tolsma Lincoln Quillian 16 Jochem Tolsma Marloes De Lange 17 Jochem Tolsma Thomas Feliciani 18 Jochem Tolsma Andreas Flache 19 Jochem Tolsma Ariana Need 20 Jochem Tolsma René Veenstra 21 Tom Van Der Meer Paul Dekker 22 Tom Van Der Meer Peer Scheepers 23 Tom Van Der Meer Wouter Van Der Brug 24 Tom Van Der Meer Jochem Tolsma 25 Tom Van Der Meer Manfred Te Grotenhuis 26 Tom Van Der Meer Erika Van Elsas 27 Tom Van Der Meer Maurice Gesthuizen 28 Tom Van Der Meer Sarah L. De Lange 29 Tom Van Der Meer Eefje Steenvoorden 30 Tom Van Der Meer Armen Hakhverdian 31 Tom Van Der Meer Eelco Harteveld 32 Tom Van Der Meer Erik Van Ingen 33 Tom Van Der Meer Huib Pellikaan 34 Tom Van Der Meer Mérove Gijsberts 35 Tom Van Der Meer Ben Pelzer 36 Tom Van Der Meer Jan W. Van Deth 37 Tom Van Der Meer Jaco Dagevos 38 Tom Van Der Meer Sonja Zmerli 39 Tom Van Der Meer Loes Aaldering 40 Tom Van Der Meer Tim Reeskens 44 Maarten Hj Wolbers Maurice Gesthuizen 45 Maarten Hj Wolbers Marloes De Lange 46 Maarten Hj Wolbers Gerbert Kraaykamp 47 Maarten Hj Wolbers Wout Ultee 48 Maarten Hj Wolbers Jochem Tolsma 49 Maarten Hj Wolbers Paul M. De Graaf 50 Maarten Hj Wolbers Mark Visser 51 Maarten Hj Wolbers Jaap Dronkers 52 Maarten Hj Wolbers Emer Smyth 53 Maarten Hj Wolbers Ruud Luijkx 54 Maarten Hj Wolbers Walter Müller 55 Maarten Hj Wolbers Renze Kolster 56 Maarten Hj Wolbers Tanja Traag 57 Maarten Hj Wolbers Don Westerheijden 58 Maarten Hj Wolbers Muja Ardita 59 Maarten Hj Wolbers Nicole Tieben 60 Maarten Hj Wolbers Andries De Grip 61 Maarten Hj Wolbers Lieselotte Blommaert 62 Maarten Hj Wolbers Richard Layte 63 Maarten Hj Wolbers Selina Mccoy 67 Gerbert Kraaykamp Nan Dirk De Graaf 68 Gerbert Kraaykamp Paul M. De Graaf 69 Gerbert Kraaykamp Matthijs Kalmijn 70 Gerbert Kraaykamp Tim Huijts 71 Gerbert Kraaykamp Maarten Hj Wolbers 72 Gerbert Kraaykamp Christiaan Monden 73 Gerbert Kraaykamp Maurice Gesthuizen 74 Gerbert Kraaykamp Mark Levels 75 Gerbert Kraaykamp Jochem Tolsma 76 Gerbert Kraaykamp Wout Ultee 77 Gerbert Kraaykamp Herman G. Van De Werfhorst 78 Gerbert Kraaykamp Roza Meuleman 79 Gerbert Kraaykamp Mark Visser 80 Gerbert Kraaykamp Koen Van Eijck 81 Gerbert Kraaykamp Margriet Van Hek 82 Gerbert Kraaykamp Ellen Verbakel 83 Gerbert Kraaykamp Stéfanie André 84 Gerbert Kraaykamp Jesper Jelle Rözer 85 Gerbert Kraaykamp Niels Blom 86 Gerbert Kraaykamp Marcel Lubbers 90 Peer Scheepers Marcel Coenders 91 Peer Scheepers Marcel Lubbers 92 Peer Scheepers Rob Eisinga 93 Peer Scheepers Manfred Te Grotenhuis 94 Peer Scheepers Mérove Gijsberts 95 Peer Scheepers Maurice Gesthuizen 96 Peer Scheepers Tom Van Der Meer 97 Peer Scheepers Michael Savelkoul 98 Peer Scheepers Jaak Billiet 99 Peer Scheepers Hans De Witte 100 Peer Scheepers Maurice Vergeer 101 Peer Scheepers Karin M Van Der Pal-De Bruin 102 Peer Scheepers Agnieszka Kanas 103 Peer Scheepers Jochem Tolsma 104 Peer Scheepers Ruben Konig 105 Peer Scheepers Pytrik Schafraad 106 Peer Scheepers Frans Van Der Slik 107 Peer Scheepers Mark Visser 108 Peer Scheepers Paula Thijs 109 Peer Scheepers Ben Pelzer 113 Michael Savelkoul Peer Scheepers 114 Michael Savelkoul Maurice Gesthuizen 115 Michael Savelkoul Jochem Tolsma 116 Michael Savelkoul William M. Van Der Veld 117 Michael Savelkoul Dietlind Stolle 118 Michael Savelkoul Miles Hewstone 122 Stijn Ruiter Wim Bernasco 123 Stijn Ruiter Nan Dirk De Graaf 124 Stijn Ruiter Jochem Tolsma 125 Stijn Ruiter Gerbert Kraaykamp 126 Stijn Ruiter Frank Van Tubergen 127 Stijn Ruiter Shane D Johnson 128 Stijn Ruiter Daniel Birks 129 Stijn Ruiter Michael Townsley 130 Stijn Ruiter Marieke Van De Rakt 131 Stijn Ruiter Paul Nieuwbeerta 132 Stijn Ruiter Jean-Louis Van Gelder 133 Stijn Ruiter Gentry White 134 Stijn Ruiter Frank Weerman 135 Stijn Ruiter Paul M. De Graaf 136 Stijn Ruiter Hidde Bekhuis 137 Stijn Ruiter Marcel Coenders 138 Stijn Ruiter Scott Baum 139 Stijn Ruiter Lieven J.r. Pauwels 140 Stijn Ruiter Marleen Weulen Kranenbarg 141 Stijn Ruiter René Bekkers 145 Marcel Lubbers Peer Scheepers 146 Marcel Lubbers Marcel Coenders 147 Marcel Lubbers Mérove Gijsberts 148 Marcel Lubbers Eva Jaspers 149 Marcel Lubbers Roza Meuleman 150 Marcel Lubbers Jochem Tolsma 151 Marcel Lubbers Rob Eisinga 152 Marcel Lubbers Maykel Verkuyten 153 Marcel Lubbers Hidde Bekhuis 154 Marcel Lubbers Nan Dirk De Graaf 155 Marcel Lubbers Tim Immerzeel 156 Marcel Lubbers Niels Spierings 157 Marcel Lubbers Jaak Billiet 158 Marcel Lubbers Mark Visser 159 Marcel Lubbers Maurice Vergeer 160 Marcel Lubbers Roos Van Der Zwan 161 Marcel Lubbers Claudia Diehl 162 Marcel Lubbers Nella Geurts 163 Marcel Lubbers Jeanette A.j. Renema 164 Marcel Lubbers Hilde Coffe 168 Maurice Gesthuizen Peer Scheepers 169 Maurice Gesthuizen Gerbert Kraaykamp 170 Maurice Gesthuizen Marloes De Lange 171 Maurice Gesthuizen Tom Van Der Meer 172 Maurice Gesthuizen Mark Visser 173 Maurice Gesthuizen Michael Savelkoul 174 Maurice Gesthuizen Heike Solga Or H. Solga 175 Maurice Gesthuizen Jochem Tolsma 176 Maurice Gesthuizen Bram Steijn 177 Maurice Gesthuizen Ariana Need 178 Maurice Gesthuizen Paul M. De Graaf 179 Maurice Gesthuizen Ellen Verbakel 180 Maurice Gesthuizen Tim Huijts 181 Maurice Gesthuizen Stéfanie André 182 Maurice Gesthuizen Jasper Van Houten 183 Maurice Gesthuizen Geert Driessen 184 Maurice Gesthuizen Beate Volker 185 Maurice Gesthuizen William M. Van Der Veld 186 Maurice Gesthuizen Jan Paul Heisig 187 Maurice Gesthuizen J.c. Vrooman 194 Nan Dirk De Graaf Gerbert Kraaykamp 195 Nan Dirk De Graaf Paul M. De Graaf 196 Nan Dirk De Graaf Paul Nieuwbeerta 197 Nan Dirk De Graaf Ariana Need 198 Nan Dirk De Graaf Stijn Ruiter 199 Nan Dirk De Graaf Geoffrey Evans 200 Nan Dirk De Graaf Anthony F Heath 201 Nan Dirk De Graaf Manfred Te Grotenhuis 202 Nan Dirk De Graaf Giedo Jansen 203 Nan Dirk De Graaf Herman G. Van De Werfhorst 204 Nan Dirk De Graaf Jonathan Kelley 205 Nan Dirk De Graaf Christiaan Monden 206 Nan Dirk De Graaf Marcel Lubbers 207 Nan Dirk De Graaf René Bekkers 208 Nan Dirk De Graaf Harry Bg Ganzeboom 209 Nan Dirk De Graaf Jochem Tolsma 210 Nan Dirk De Graaf Eva Jaspers 211 Nan Dirk De Graaf Ruud Luijkx 212 Nan Dirk De Graaf Lincoln Quillian 213 Nan Dirk De Graaf Jacques A. Hagenaars 217 Tobias H. Stark Anke Munniksma 218 Tobias H. Stark René Veenstra 219 Tobias H. Stark Maykel Verkuyten 220 Tobias H. Stark Josh Pasek 221 Tobias H. Stark Trevor Tompson 222 Tobias H. Stark Jochem Tolsma 223 Tobias H. Stark J. Ashwin Rambaran, Ph.d. 224 Tobias H. Stark Ioana Van Deurzen 234 Matthijs Kalmijn Paul M. De Graaf 235 Matthijs Kalmijn Kène Henkens 236 Matthijs Kalmijn Frank Van Tubergen 237 Matthijs Kalmijn Gerbert Kraaykamp 238 Matthijs Kalmijn Aart C. Liefbroer 239 Matthijs Kalmijn Wilfred Uunk 240 Matthijs Kalmijn Christiaan Monden 241 Matthijs Kalmijn Marleen Damman 242 Matthijs Kalmijn Katya Ivanova 243 Matthijs Kalmijn Anne-Rigt Poortman 244 Matthijs Kalmijn Harry Bg Ganzeboom 245 Matthijs Kalmijn Jornt J. Mandemakers 246 Matthijs Kalmijn Ellen Verbakel 247 Matthijs Kalmijn Ruud Luijkx 248 Matthijs Kalmijn Jaap Dronkers 249 Matthijs Kalmijn Marjolein Broese Van Groenou 250 Matthijs Kalmijn Tanja Van Der Lippe 251 Matthijs Kalmijn Erik Van Ingen 252 Matthijs Kalmijn Nells - Netherlands Longitudinal Lif 256 Lincoln Quillian Jochem Tolsma 266 Andreas Flache Michael Macy 267 Andreas Flache Michael Mäs 268 Andreas Flache Tobias H. Stark 269 Andreas Flache Rainer Hegselmann 270 Andreas Flache René Veenstra 271 Andreas Flache Rafael Wittek 272 Andreas Flache Guillaume Deffuant 273 Andreas Flache Dirk Helbing 274 Andreas Flache Anke Munniksma 275 Andreas Flache Károly Takács 276 Andreas Flache James A Kitts 277 Andreas Flache Maykel Verkuyten 278 Andreas Flache Nigel Gilbert 279 Andreas Flache Maxi San Miguel 280 Andreas Flache Rosaria Conte 281 Andreas Flache Andrzej Nowak 282 Andreas Flache Josep M. Pujol 283 Andreas Flache André Grow 284 Andreas Flache Werner Raub 285 Andreas Flache Rene Torenvlied 292 René Veenstra Ormel, Johan (Hans) 293 René Veenstra Frank Verhulst 294 René Veenstra Siegwart Lindenberg 295 René Veenstra Jan Kornelis Dijkstra 296 René Veenstra Tineke Oldehinkel 297 René Veenstra Gijs Huitsing 298 René Veenstra Christina Salmivalli 299 René Veenstra Wilma Vollebergh 300 René Veenstra Christian Steglich 301 René Veenstra Tina Kretschmer 302 René Veenstra Jelle Sijtsema 303 René Veenstra Miranda Sentse 304 René Veenstra Rozemarijn Van Der Ploeg 305 René Veenstra Sijmen A Reijneveld 306 René Veenstra Catharina A. Hartman 307 René Veenstra Miia Sainio 308 René Veenstra Beau Oldenburg 309 René Veenstra Marijtje A.j. Van Duijn 310 René Veenstra Henning Tiemeier 311 René Veenstra Antonius Cillessen Notice, however, that we could easily plot Jochems collaboration network already! This is thus a one-deeper network where we plot the co-authors of Jochem and who they collaborate with, there is some overlap, but not always. plot_coauthors(get_coauthors(&quot;Iu23-90AAAAJ&quot;, n_coauthors = 20, n_deep = 1), size_labels = 2) # Doesn&#39;t look like much yet, but we&#39;ll make it prettier later. So lets gather these data, but now for all sociology staff simultaneously! For this, we use the for loop again. The for loop I make below is a bit more complicated, but follows the same logic as before. For each row (i) in soc_df, we attempt to query Google Scholar on the basis of the first name, last name, and affiliation listed in that row in the data frame. We use some handy subsetting, e.g., soc_df[i, 3] means we input last_name= with the last name (which is the third column) found in the i-th row in the data frame. The same goes for first name and affiliation. We fill up gs_id in the data frame with the Google Scholar IDs well hopefully find. The for (i in nrow(soc_df)) simply means we let i run for however many rows the data frame has. Finally, the tryCatch({}) function makes that we can continue the loop even though we may encounter errors for a given row. Here, that probably means that not every row (i.e., sociology staff member) can be found on Google Scholar. We print the error, but continue the for loop with the tryCatch({}) function. In the final rows of the code below. We simply drop those rows that we cannot identify on Google Scholar. # because we don&#39;t wanna &#39;Rate limit&#39; google scholar, they throw you out if you make to many # requests, we randomize request time do you understand the code below? for (i in 1:10) { time &lt;- runif(1, 0, 5) Sys.sleep(time) print(paste(i, &quot;: R slept for&quot;, round(time, 1), &quot;seconds&quot;)) } # for every number from 1 to 10 we draw one number from 0 to 5 from a uniform distribution we put # the wrapper sys.sleep around it that we put R to sleep for the drawn number # Look throught get_scholar_id_fix(last_name, first_name, affiliation) # if we can find google scholar profiles of sociology staff! for (i in 1:nrow(soc_df)) { time &lt;- runif(1, 0, 5) Sys.sleep(time) tryCatch({ soc_df[i,c(&quot;gs_id&quot;)] &lt;- get_scholar_id_fix(last_name = soc_df[i, 3], # so search on last_name of staff (third column) first_name = soc_df[i,4], # search on first_name of staff (fourth column) affiliation = soc_df[i,5]) # search on affiliation of each staff (fifth column) }, error=function(e){cat(&quot;ERROR :&quot;, conditionMessage(e), &quot;\\n&quot;)}) # continue on error, but print the error } # remove those without pubs from the df # seems we&#39;re left with about 34 sociology staff members! soc_df &lt;- soc_df[!soc_df$gs_id == &quot;&quot;, ] soc_df Table 11.7: Sociology staff with Scholar IDs soc_names soc_experts last_name first_name affiliation gs_id 2 Batenburg, prof. dr. R. (Ronald) Healthcare, labour market and healthcare professions and training Batenburg Ronald radboud university UK7nVSEAAAAJ 3 Begall, dr. K.H. (Katia) Family, life course, labour market participation, division of household tasks and gender norms Begall Katia radboud university e7zfTqMAAAAJ 4 Bekhuis, dr. H. (Hidde) Welfare state, nationalism and sports Bekhuis Hidde radboud university Q4saWX8AAAAJ 5 Berg, dr. L. van den (Lonneke) Family, life course and transition to adulthood Berg Lonneke radboud university vzBNQ1kAAAAJ 6 Blommaert, dr. L. (Lieselotte) Discrimination and inequality on the labour market Blommaert Lieselotte radboud university RG54uasAAAAJ 7 Damman, dr. M. (Marleen) Labour market, life course, older workers, retirement and solo self-employed Damman Marleen radboud university MEv-V_YAAAAJ 8 Eisinga, prof. dr. R.N. (Rob) Methods of research and statistics Eisinga Rob radboud university GDHdsXAAAAAJ 9 Gesthuizen, dr. M.J.W. (Maurice) Poverty en social cohesion Gesthuizen Maurice radboud university n6hiblQAAAAJ 10 Glas, dr. S. (Saskia) Islam, gender attitudes and sexuality Glas Saskia radboud university ZMc0j2YAAAAJ 11 Hek, dr. M. van (Margriet) Educational inequality, gender inequality, organizational sociology and culture Hek Margriet radboud university ZvLlx2EAAAAJ 12 Hoekman, dr. R. H. A.(Remco) Sports and policy sociology Hoekman Remco radboud university LsMimOEAAAAJ 13 Hofstra, dr. B. (Bas) Diversity, inequality and innovation Hofstra Bas radboud university Nx7pDywAAAAJ 14 Kraaykamp, prof. dr. G.L.M. (Gerbert) Educational inequality, culture and health Kraaykamp Gerbert radboud university l8aM4jAAAAAJ 15 Meuleman, dr. (Roza) Culture and nationalism Meuleman Roza radboud university iKs_5WkAAAAJ 16 Savelkoul, dr. M.J. (Michael) Ethnic diversity, prejudice and social cohesion Savelkoul Michael radboud university _f3krXUAAAAJ 17 Scheepers, prof. dr. P.L.H. (Peer) Comparative research, social cohesion and diversity Scheepers Peer radboud university hPeXxvEAAAAJ 18 Spierings, dr. C.H.B.M. (Niels) Islam, gender, populism, social media, Middle East and migration Spierings Niels radboud university cy3Ye6sAAAAJ 19 Tolsma, dr. J. (Jochem) Inequality, criminology and ethnic diversity Tolsma Jochem radboud university Iu23-90AAAAJ 20 Verbakel, prof. dr. C.M.C. (Ellen) Head of the department Health, family and work Verbakel Ellen radboud university w2McVJAAAAAJ 21 Visser, dr. M. (Mark) Older workers, radicalism and social cohesion Visser Mark radboud university ItITloQAAAAJ 22 Wolbers, prof. dr. M.H.J. (Maarten) Educational inequality and labour market inequality Wolbers Maarten radboud university TqKrXnMAAAAJ 24 Bussemakers, C. (Carlijn) MSc Adverse youth experiences and social inequality Bussemakers Carlijn radboud university bDPtkIoAAAAJ 25 Franken, R. (Rob) MSc Sport networks and motivation for sustainable sports participation Franken Rob radboud university p3IwtT4AAAAJ 26 Firat, M. (Mustafa) MSc Social inequality, older workers, life course and retirement Firat Mustafa radboud university _ukytQYAAAAJ 27 Geurts, P.G. (Nella) MSc Integration and migration Geurts Nella radboud university VCTvbTkAAAAJ 29 Jeroense, T.M.G. (Thijmen) MSc Political participation, segregation, opinion polarization and voting behaviour Jeroense Thijmen radboud university izq-KNUAAAAJ 31 Loh, S.M. (Renae) MSc Educational sociology, social stratification, gender inequality and information communication technology (ICT) Loh Renae radboud university tFaMPOQAAAAJ 33 Mensvoort, C.A. van (Carly) MSc Gender, leadership and social norms Mensvoort Carly radboud university z6iMs-UAAAAJ 34 Müller, K. (Katrin) MSc Opinions about discrimination, migration and inequality Müller Katrin radboud university lkVq32sAAAAJ 35 Raiber, K. (Klara) MSc Informal care, employment, social inequality and gender Raiber Klara radboud university xE65HUcAAAAJ 36 Ramaekers, M.J.M. (Marlou) MSc Prosocial behaviour and family Ramaekers Marlou radboud university fp99JAQAAAAJ 40 Houten, J. (Jasper) van MSc Sports Houten Jasper radboud university iR4UIwwAAAAJ 41 Middendorp J. (Jansje) van MSc Home administration Middendorp Jansje radboud university gs0li6MAAAAJ 43 Weber, T. (Tijmen) MSc International student mobility and the internationalization of higher education Weber Tijmen radboud university KfLALRIAAAAJ It works! So what is left to do is to get the data we already extracted for Jochem (citation, publications, and so forth), but now for all sociology staff. For that, we need a bunch of for loops. Lets first gather the profiles and publications. We store those in a list() which is an object in which you can store multiple data frames, vectors, matrices, and so forth. This is particularly good for for loops because you can store information that is  at first sight  not necessarily compatible. For instance, matrices of different length. Note that we bind a Google Scholar ID to the publications too which is important to be able match to the soc_df later on. soc_list_profiles &lt;- list() # first we create an empty list that we then fill up with the for loop soc_list_publications &lt;- list() for (i in 1:nrow(soc_df)) { time &lt;- runif(1, 0, 5) Sys.sleep(time) # note how you call different elements in a list &#39;[[]]&#39;, fill in the i-th element soc_list_profiles[[i]] &lt;- get_profile(soc_df[i, c(&quot;gs_id&quot;)]) # Note how we call row i (remember how to call rows in a DF/Matrix) and then the associated scholar id soc_list_publications[[i]] &lt;- get_publications(soc_df[i, c(&quot;gs_id&quot;)]) soc_list_publications[[i]][, c(&quot;gs_id&quot;)] &lt;- soc_df[i, c(&quot;gs_id&quot;)] # note that we again attach an id # so both functions here call the entire profile and pubs for an author, based on google # scholar ids } # Notice how fast the data blow up! The 34 RU sociology scholars publish ~3000 papers soc_df_publications &lt;- bind_rows(soc_list_publications) Note how soc_list_profiles contains all Google Scholar profile information in a list of 34 elements. We need to do some relatively involved data handling to attach the Google Scholar profiles in soc_list_profiles to the soc_df. That is, we want some information from soc_list_profiles to be attached to soc_df. Specifically, we want several things we want from the profiles: id, name (as reported on Scholar), affiliation, tot_cites, h_index, i10index, fields, and homepage. Some profile elements can contain more than one row. For instance, co-authors are stored in long format per profile that do not easily merge to a data frame where each staff member is a row. For instance, say Bas has two co-authors; one needs to first concatonate those co-authors before you can merge them in one row for Bas. So we need to get only the profile elements that have a single element per profile element (e.g., total_cites). Seems these are the first 8 elements in a list element. So we need to get those out of the lists and store those in a new dataframe. This involves several steps: We first `unlist` the 1 tot 8-th elements in a list element We then make it a data frame Then *transpose* the data such that each row contains 8 columns. We then use the function `bind_rows()` to simply make a data frame from the list elements. We then merge it to `soc_df`. So what we end up with is a sociology staff data frame with much more information than before: citations, indices, expertise listed on Google Scholar, and so forth. Note how we can do this at once, or all separately (I commented out the code that does the same as the next three rows). Which has your preference? soc_profiles_df &lt;- list() for (i in 1:length(soc_list_profiles)) { # soc_profiles_df[[i]] &lt;- data.frame(t(unlist(soc_list_profiles[[i]][1:8]))) #some annyoing # data handling soc_profiles_df[[i]] &lt;- unlist(soc_list_profiles[[i]][1:8]) soc_profiles_df[[i]] &lt;- data.frame(soc_profiles_df[[i]]) soc_profiles_df[[i]] &lt;- t(soc_profiles_df[[i]]) } soc_profiles_df &lt;- bind_rows(soc_profiles_df) soc_df &lt;- left_join(soc_df, soc_profiles_df, by = c(gs_id = &quot;id&quot;)) # merge data with soc_df soc_df # notice all the new information we were able to get from the scholar profiles! Lets see whether we Table 11.8: Sociology staff with Scholar info soc_names soc_experts last_name first_name affiliation.x gs_id name affiliation.y total_cites h_index i10_index fields homepage Batenburg, prof. dr. R. (Ronald) Healthcare, labour market and healthcare professions and training Batenburg Ronald radboud university UK7nVSEAAAAJ Ronald Batenburg Programmaleider NIVEL en bijzonder hoogleraar Radboud Universiteit Nijmegen 3608 30 87 verified email at nivel.nl - homepage https://www.nivel.nl/nl/ronald-batenburg Begall, dr. K.H. (Katia) Family, life course, labour market participation, division of household tasks and gender norms Begall Katia radboud university e7zfTqMAAAAJ Katia Begall Radboud University Nijmegen 936 9 9 verified email at maw.ru.nl NA Bekhuis, dr. H. (Hidde) Welfare state, nationalism and sports Bekhuis Hidde radboud university Q4saWX8AAAAJ Hidde Bekhuis Post Doc Sociology, Radboud University Nijmegen 348 8 7 verified email at ru.nl NA Berg, dr. L. van den (Lonneke) Family, life course and transition to adulthood Berg Lonneke radboud university vzBNQ1kAAAAJ Lonneke van den Berg Radboud University 34 3 2 verified email at maw.ru.nl - homepage https://www.ru.nl/personen/berg-l-van-den-lonneke/ Blommaert, dr. L. (Lieselotte) Discrimination and inequality on the labour market Blommaert Lieselotte radboud university RG54uasAAAAJ Lieselotte Blommaert Sociology/Social Cultural Research, Radboud University, Nijmegen, the 317 7 7 verified email at ru.nl - homepage http://www.ru.nl/english/people/blommaert-e/ Damman, dr. M. (Marleen) Labour market, life course, older workers, retirement and solo self-employed Damman Marleen radboud university MEv-V_YAAAAJ Marleen Damman Assistant Professor, Utrecht University 515 10 12 verified email at uu.nl - homepage https://www.uu.nl/staff/MDamman Eisinga, prof. dr. R.N. (Rob) Methods of research and statistics Eisinga Rob radboud university GDHdsXAAAAAJ Rob Eisinga Professor social science research methods, Radboud University Nijmegen 4994 33 77 verified email at ru.nl - homepage http://robeisinga.ruhosting.nl/ Gesthuizen, dr. M.J.W. (Maurice) Poverty en social cohesion Gesthuizen Maurice radboud university n6hiblQAAAAJ Maurice Gesthuizen Sociology, Radboud University Nijmegen, the Netherland - Assistant Professor 2425 24 41 verified email at maw.ru.nl - homepage http://www.ru.nl/methodenentechnieken/methoden-technieken/medewerkers/vm_medewerkers/maurice_gesthuizen/ Glas, dr. S. (Saskia) Islam, gender attitudes and sexuality Glas Saskia radboud university ZMc0j2YAAAAJ Saskia Glas PhD student, Radboud University 70 4 2 verified email at ru.nl NA Hek, dr. M. van (Margriet) Educational inequality, gender inequality, organizational sociology and culture Hek Margriet radboud university ZvLlx2EAAAAJ Margriet van Hek Radboud University 262 8 7 verified email at maw.ru.nl NA Hoekman, dr. R. H. A.(Remco) Sports and policy sociology Hoekman Remco radboud university LsMimOEAAAAJ Remco Hoekman Director, Mulier Institute / Senior researcher, Radboud University 610 12 15 verified email at mulierinstituut.nl - homepage https://www.mulierinstituut.nl/over-mulier/medewerkers/remco-hoekman/ Hofstra, dr. B. (Bas) Diversity, inequality and innovation Hofstra Bas radboud university Nx7pDywAAAAJ Bas Hofstra Assistant Professor, Radboud University 384 7 7 verified email at ru.nl - homepage http://www.bashofstra.com/ Kraaykamp, prof. dr. G.L.M. (Gerbert) Educational inequality, culture and health Kraaykamp Gerbert radboud university l8aM4jAAAAAJ Gerbert Kraaykamp Professor of Sociology, Radboud Universiteit Nijmegen 7724 46 98 verified email at maw.ru.nl - homepage https://www.ru.nl/english/people/kraaykamp-g/ Meuleman, dr. (Roza) Culture and nationalism Meuleman Roza radboud university iKs_5WkAAAAJ Roza Meuleman Assistant Professor - Sociology - Radboud University Nijmegen 214 8 6 verified email at ru.nl NA Savelkoul, dr. M.J. (Michael) Ethnic diversity, prejudice and social cohesion Savelkoul Michael radboud university _f3krXUAAAAJ Michael Savelkoul Assistant Professor - Sociology, Radboud University Nijmegen, the Netherlands 580 8 7 verified email at maw.ru.nl NA Scheepers, prof. dr. P.L.H. (Peer) Comparative research, social cohesion and diversity Scheepers Peer radboud university hPeXxvEAAAAJ peer scheepers hoogleraar methodologie, faculteit der sociale wetenschappen radboud universiteit 14399 61 180 verified email at maw.ru.nl NA Spierings, dr. C.H.B.M. (Niels) Islam, gender, populism, social media, Middle East and migration Spierings Niels radboud university cy3Ye6sAAAAJ Niels Spierings Associate Professor of Sociology, Radboud University 1662 22 33 verified email at ru.nl - homepage https://www.ru.nl/english/people/spierings-c/ Tolsma, dr. J. (Jochem) Inequality, criminology and ethnic diversity Tolsma Jochem radboud university Iu23-90AAAAJ Jochem Tolsma Professor, Radboud University Nijmegen / University of Groningen 2260 22 33 verified email at ru.nl - homepage http://www.jochemtolsma.nl/ Verbakel, prof. dr. C.M.C. (Ellen) Head of the department Health, family and work Verbakel Ellen radboud university w2McVJAAAAAJ Ellen Verbakel Professor of Sociology, Department of Sociology, Radboud University Nijmegen 1474 24 32 verified email at maw.ru.nl - homepage http://www.ellenverbakel.nl/ Visser, dr. M. (Mark) Older workers, radicalism and social cohesion Visser Mark radboud university ItITloQAAAAJ Mark Visser Assistant Professor, Radboud University 381 9 8 verified email at ru.nl - homepage https://www.researchgate.net/profile/Mark_Visser Wolbers, prof. dr. M.H.J. (Maarten) Educational inequality and labour market inequality Wolbers Maarten radboud university TqKrXnMAAAAJ Maarten HJ Wolbers Professor of Sociology, Radboud University, Nijmegen 3624 29 58 verified email at ru.nl - homepage http://www.socsci.ru.nl/maartenw/ Bussemakers, C. (Carlijn) MSc Adverse youth experiences and social inequality Bussemakers Carlijn radboud university bDPtkIoAAAAJ Carlijn Bussemakers Department of Sociology, Radboud University 37 3 1 verified email at maw.ru.nl NA Franken, R. (Rob) MSc Sport networks and motivation for sustainable sports participation Franken Rob radboud university p3IwtT4AAAAJ Rob JM Franken Unknown affiliation 1219 11 12 no verified email NA Firat, M. (Mustafa) MSc Social inequality, older workers, life course and retirement Firat Mustafa radboud university _ukytQYAAAAJ mustafa Inc firat university 5298 34 173 verified email at firat.edu.tr NA Geurts, P.G. (Nella) MSc Integration and migration Geurts Nella radboud university VCTvbTkAAAAJ Nella Geurts Department of Sociology, Radboud University 32 3 1 verified email at ru.nl NA Jeroense, T.M.G. (Thijmen) MSc Political participation, segregation, opinion polarization and voting behaviour Jeroense Thijmen radboud university izq-KNUAAAAJ Thijmen Jeroense PhD candidate, Radboud University Nijmegen 1 1 0 verified email at ru.nl - homepage https://www.ru.nl/personen/jeroense-t/ Loh, S.M. (Renae) MSc Educational sociology, social stratification, gender inequality and information communication technology (ICT) Loh Renae radboud university tFaMPOQAAAAJ Renae Sze Ming Loh PhD candidate, Radboud University 70 2 2 verified email at ru.nl - homepage http://renaeloh.com/ Mensvoort, C.A. van (Carly) MSc Gender, leadership and social norms Mensvoort Carly radboud university z6iMs-UAAAAJ Carly van Mensvoort Radboud University 35 2 2 verified email at ru.nl - homepage https://www.ru.nl/english/people/mensvoort-c-van/ Müller, K. (Katrin) MSc Opinions about discrimination, migration and inequality Müller Katrin radboud university lkVq32sAAAAJ Kathrin Friederike Müller Post-Doc, Universtität Rostock/CAIS NRW 201 9 9 verified email at uni-rostock.de - homepage https://www.imf.uni-rostock.de/institut/mitarbeiterinnen/lehrende/dr-kathrin-friederike-mueller/ Raiber, K. (Klara) MSc Informal care, employment, social inequality and gender Raiber Klara radboud university xE65HUcAAAAJ Klara Raiber PhD candidate, Radboud University Nijmegen 4 1 0 verified email at maw.ru.nl - homepage https://www.ru.nl/english/people/raiber-k/ Ramaekers, M.J.M. (Marlou) MSc Prosocial behaviour and family Ramaekers Marlou radboud university fp99JAQAAAAJ Marlou Ramaekers PhD Candidate, Radboud University NA NA NA verified email at ru.nl NA Houten, J. (Jasper) van MSc Sports Houten Jasper radboud university iR4UIwwAAAAJ Jasper van Houten PhD Candidate, HAN Institute of Sport and Exercise Studies (Hogeschool van Arnhem en Nijmegen 31 4 1 verified email at ru.nl - homepage https://www.researchgate.net/profile/Jasper_Houten Middendorp J. (Jansje) van MSc Home administration Middendorp Jansje radboud university gs0li6MAAAAJ Jansje van Middendorp Buitenpromovendus Radboud Universiteit 3 1 0 verified email at ru.nl NA Weber, T. (Tijmen) MSc International student mobility and the internationalization of higher education Weber Tijmen radboud university KfLALRIAAAAJ Tijmen Weber Lecturer Statistics and Research, HAN University of Applied Sciences 42 2 2 verified email at han.nl NA So we have papers and profiles. Remember how we got Jochems citation history? We want that for each staff member too. Yet again, we use a for loop. We first store the citation history in a list. But notice the if statement! We only continue the for loop for that i-th element if some statement is TRUE. Here, we attempt to find out if the i-th element, the citation history of the staff member, has a length than is larger than 0. Some staff members are never cited (which happens all the time if papers are only just published), and so for these staff members that is no list element that contains information. We only attach a Google Scholar ID for those staff members that are cited at least once. We bind the rows again and end up with a data frame in long format: three columns with years, cites, and Google Scholar ID. Therefore, there is more than one row per staff member. # get citation history of a scholar soc_staff_cit &lt;- list() for (i in 1:nrow(soc_df)) { soc_staff_cit[[i]] &lt;- get_citation_history(soc_df[i, c(&quot;gs_id&quot;)]) if (nrow(soc_staff_cit[[i]]) &gt; 0) { soc_staff_cit[[i]][, c(&quot;gs_id&quot;)] &lt;- soc_df[i, c(&quot;gs_id&quot;)] # again attach the gs_id as third column } } soc_staff_cit &lt;- bind_rows(soc_staff_cit) colnames(soc_staff_cit)[3] &lt;- &quot;gs_id&quot; # Note how we use some complicated code to make a nice table you can scroll through in this book, # you could just write &#39;soc_staff_cit&#39; in your own code Also, no names yet! You could merge the # names of scholars to gs_id here. scroll_box(knitr::kable(soc_staff_cit, booktabs = TRUE, caption = &quot;Sociology staff citation by time&quot;), height = &quot;300px&quot;) Table 11.9: Sociology staff citation by time year cites gs_id 1999 14 UK7nVSEAAAAJ 2000 25 UK7nVSEAAAAJ 2001 21 UK7nVSEAAAAJ 2002 18 UK7nVSEAAAAJ 2003 35 UK7nVSEAAAAJ 2004 24 UK7nVSEAAAAJ 2005 53 UK7nVSEAAAAJ 2006 64 UK7nVSEAAAAJ 2007 52 UK7nVSEAAAAJ 2008 80 UK7nVSEAAAAJ 2009 115 UK7nVSEAAAAJ 2010 134 UK7nVSEAAAAJ 2011 158 UK7nVSEAAAAJ 2012 186 UK7nVSEAAAAJ 2013 219 UK7nVSEAAAAJ 2014 207 UK7nVSEAAAAJ 2015 257 UK7nVSEAAAAJ 2016 336 UK7nVSEAAAAJ 2017 307 UK7nVSEAAAAJ 2018 358 UK7nVSEAAAAJ 2019 314 UK7nVSEAAAAJ 2020 335 UK7nVSEAAAAJ 2021 183 UK7nVSEAAAAJ 2008 5 e7zfTqMAAAAJ 2009 18 e7zfTqMAAAAJ 2010 28 e7zfTqMAAAAJ 2011 29 e7zfTqMAAAAJ 2012 32 e7zfTqMAAAAJ 2013 48 e7zfTqMAAAAJ 2014 51 e7zfTqMAAAAJ 2015 61 e7zfTqMAAAAJ 2016 77 e7zfTqMAAAAJ 2017 120 e7zfTqMAAAAJ 2018 99 e7zfTqMAAAAJ 2019 119 e7zfTqMAAAAJ 2020 137 e7zfTqMAAAAJ 2021 102 e7zfTqMAAAAJ 2008 1 Q4saWX8AAAAJ 2009 4 Q4saWX8AAAAJ 2010 7 Q4saWX8AAAAJ 2011 7 Q4saWX8AAAAJ 2012 17 Q4saWX8AAAAJ 2013 22 Q4saWX8AAAAJ 2014 36 Q4saWX8AAAAJ 2015 29 Q4saWX8AAAAJ 2016 37 Q4saWX8AAAAJ 2017 25 Q4saWX8AAAAJ 2018 33 Q4saWX8AAAAJ 2019 50 Q4saWX8AAAAJ 2020 40 Q4saWX8AAAAJ 2021 32 Q4saWX8AAAAJ 2018 1 vzBNQ1kAAAAJ 2019 6 vzBNQ1kAAAAJ 2020 9 vzBNQ1kAAAAJ 2021 15 vzBNQ1kAAAAJ 2012 3 RG54uasAAAAJ 2013 3 RG54uasAAAAJ 2014 8 RG54uasAAAAJ 2015 24 RG54uasAAAAJ 2016 19 RG54uasAAAAJ 2017 34 RG54uasAAAAJ 2018 41 RG54uasAAAAJ 2019 58 RG54uasAAAAJ 2020 72 RG54uasAAAAJ 2021 51 RG54uasAAAAJ 2011 2 MEv-V_YAAAAJ 2012 7 MEv-V_YAAAAJ 2013 15 MEv-V_YAAAAJ 2014 19 MEv-V_YAAAAJ 2015 30 MEv-V_YAAAAJ 2016 60 MEv-V_YAAAAJ 2017 65 MEv-V_YAAAAJ 2018 78 MEv-V_YAAAAJ 2019 88 MEv-V_YAAAAJ 2020 85 MEv-V_YAAAAJ 2021 60 MEv-V_YAAAAJ 1991 18 GDHdsXAAAAAJ 1992 13 GDHdsXAAAAAJ 1993 14 GDHdsXAAAAAJ 1994 41 GDHdsXAAAAAJ 1995 38 GDHdsXAAAAAJ 1996 37 GDHdsXAAAAAJ 1997 26 GDHdsXAAAAAJ 1998 24 GDHdsXAAAAAJ 1999 36 GDHdsXAAAAAJ 2000 47 GDHdsXAAAAAJ 2001 50 GDHdsXAAAAAJ 2002 44 GDHdsXAAAAAJ 2003 42 GDHdsXAAAAAJ 2004 35 GDHdsXAAAAAJ 2005 48 GDHdsXAAAAAJ 2006 63 GDHdsXAAAAAJ 2007 58 GDHdsXAAAAAJ 2008 83 GDHdsXAAAAAJ 2009 120 GDHdsXAAAAAJ 2010 98 GDHdsXAAAAAJ 2011 104 GDHdsXAAAAAJ 2012 143 GDHdsXAAAAAJ 2013 184 GDHdsXAAAAAJ 2014 277 GDHdsXAAAAAJ 2015 345 GDHdsXAAAAAJ 2016 411 GDHdsXAAAAAJ 2017 441 GDHdsXAAAAAJ 2018 527 GDHdsXAAAAAJ 2019 495 GDHdsXAAAAAJ 2020 591 GDHdsXAAAAAJ 2021 447 GDHdsXAAAAAJ 2005 10 n6hiblQAAAAJ 2006 19 n6hiblQAAAAJ 2007 28 n6hiblQAAAAJ 2008 47 n6hiblQAAAAJ 2009 54 n6hiblQAAAAJ 2010 67 n6hiblQAAAAJ 2011 128 n6hiblQAAAAJ 2012 121 n6hiblQAAAAJ 2013 175 n6hiblQAAAAJ 2014 200 n6hiblQAAAAJ 2015 231 n6hiblQAAAAJ 2016 252 n6hiblQAAAAJ 2017 237 n6hiblQAAAAJ 2018 206 n6hiblQAAAAJ 2019 208 n6hiblQAAAAJ 2020 210 n6hiblQAAAAJ 2021 186 n6hiblQAAAAJ 2018 2 ZMc0j2YAAAAJ 2019 13 ZMc0j2YAAAAJ 2020 29 ZMc0j2YAAAAJ 2021 26 ZMc0j2YAAAAJ 2014 3 ZvLlx2EAAAAJ 2015 4 ZvLlx2EAAAAJ 2016 19 ZvLlx2EAAAAJ 2017 15 ZvLlx2EAAAAJ 2018 31 ZvLlx2EAAAAJ 2019 55 ZvLlx2EAAAAJ 2020 78 ZvLlx2EAAAAJ 2021 55 ZvLlx2EAAAAJ 2010 4 LsMimOEAAAAJ 2011 7 LsMimOEAAAAJ 2012 12 LsMimOEAAAAJ 2013 24 LsMimOEAAAAJ 2014 40 LsMimOEAAAAJ 2015 36 LsMimOEAAAAJ 2016 53 LsMimOEAAAAJ 2017 76 LsMimOEAAAAJ 2018 81 LsMimOEAAAAJ 2019 49 LsMimOEAAAAJ 2020 122 LsMimOEAAAAJ 2021 98 LsMimOEAAAAJ 2014 2 Nx7pDywAAAAJ 2015 2 Nx7pDywAAAAJ 2016 15 Nx7pDywAAAAJ 2017 25 Nx7pDywAAAAJ 2018 33 Nx7pDywAAAAJ 2019 28 Nx7pDywAAAAJ 2020 105 Nx7pDywAAAAJ 2021 162 Nx7pDywAAAAJ 1997 22 l8aM4jAAAAAJ 1998 19 l8aM4jAAAAAJ 1999 31 l8aM4jAAAAAJ 2000 49 l8aM4jAAAAAJ 2001 77 l8aM4jAAAAAJ 2002 87 l8aM4jAAAAAJ 2003 98 l8aM4jAAAAAJ 2004 116 l8aM4jAAAAAJ 2005 126 l8aM4jAAAAAJ 2006 176 l8aM4jAAAAAJ 2007 205 l8aM4jAAAAAJ 2008 256 l8aM4jAAAAAJ 2009 246 l8aM4jAAAAAJ 2010 303 l8aM4jAAAAAJ 2011 360 l8aM4jAAAAAJ 2012 363 l8aM4jAAAAAJ 2013 460 l8aM4jAAAAAJ 2014 474 l8aM4jAAAAAJ 2015 512 l8aM4jAAAAAJ 2016 614 l8aM4jAAAAAJ 2017 581 l8aM4jAAAAAJ 2018 655 l8aM4jAAAAAJ 2019 621 l8aM4jAAAAAJ 2020 662 l8aM4jAAAAAJ 2021 440 l8aM4jAAAAAJ 2012 1 iKs_5WkAAAAJ 2013 5 iKs_5WkAAAAJ 2014 14 iKs_5WkAAAAJ 2015 19 iKs_5WkAAAAJ 2016 23 iKs_5WkAAAAJ 2017 30 iKs_5WkAAAAJ 2018 31 iKs_5WkAAAAJ 2019 39 iKs_5WkAAAAJ 2020 23 iKs_5WkAAAAJ 2021 20 iKs_5WkAAAAJ 2011 10 _f3krXUAAAAJ 2012 24 _f3krXUAAAAJ 2013 32 _f3krXUAAAAJ 2014 51 _f3krXUAAAAJ 2015 67 _f3krXUAAAAJ 2016 54 _f3krXUAAAAJ 2017 63 _f3krXUAAAAJ 2018 80 _f3krXUAAAAJ 2019 64 _f3krXUAAAAJ 2020 70 _f3krXUAAAAJ 2021 54 _f3krXUAAAAJ 1994 60 hPeXxvEAAAAJ 1995 35 hPeXxvEAAAAJ 1996 39 hPeXxvEAAAAJ 1997 33 hPeXxvEAAAAJ 1998 35 hPeXxvEAAAAJ 1999 47 hPeXxvEAAAAJ 2000 74 hPeXxvEAAAAJ 2001 122 hPeXxvEAAAAJ 2002 107 hPeXxvEAAAAJ 2003 153 hPeXxvEAAAAJ 2004 170 hPeXxvEAAAAJ 2005 180 hPeXxvEAAAAJ 2006 253 hPeXxvEAAAAJ 2007 336 hPeXxvEAAAAJ 2008 439 hPeXxvEAAAAJ 2009 515 hPeXxvEAAAAJ 2010 511 hPeXxvEAAAAJ 2011 622 hPeXxvEAAAAJ 2012 767 hPeXxvEAAAAJ 2013 782 hPeXxvEAAAAJ 2014 935 hPeXxvEAAAAJ 2015 1129 hPeXxvEAAAAJ 2016 1076 hPeXxvEAAAAJ 2017 1182 hPeXxvEAAAAJ 2018 1206 hPeXxvEAAAAJ 2019 1163 hPeXxvEAAAAJ 2020 1235 hPeXxvEAAAAJ 2021 863 hPeXxvEAAAAJ 2011 12 cy3Ye6sAAAAJ 2012 21 cy3Ye6sAAAAJ 2013 42 cy3Ye6sAAAAJ 2014 55 cy3Ye6sAAAAJ 2015 74 cy3Ye6sAAAAJ 2016 141 cy3Ye6sAAAAJ 2017 140 cy3Ye6sAAAAJ 2018 223 cy3Ye6sAAAAJ 2019 285 cy3Ye6sAAAAJ 2020 346 cy3Ye6sAAAAJ 2021 287 cy3Ye6sAAAAJ 2008 12 Iu23-90AAAAJ 2009 21 Iu23-90AAAAJ 2010 26 Iu23-90AAAAJ 2011 79 Iu23-90AAAAJ 2012 79 Iu23-90AAAAJ 2013 116 Iu23-90AAAAJ 2014 151 Iu23-90AAAAJ 2015 204 Iu23-90AAAAJ 2016 228 Iu23-90AAAAJ 2017 223 Iu23-90AAAAJ 2018 267 Iu23-90AAAAJ 2019 297 Iu23-90AAAAJ 2020 305 Iu23-90AAAAJ 2021 228 Iu23-90AAAAJ 2007 7 w2McVJAAAAAJ 2008 3 w2McVJAAAAAJ 2009 14 w2McVJAAAAAJ 2010 19 w2McVJAAAAAJ 2011 19 w2McVJAAAAAJ 2012 19 w2McVJAAAAAJ 2013 51 w2McVJAAAAAJ 2014 50 w2McVJAAAAAJ 2015 76 w2McVJAAAAAJ 2016 113 w2McVJAAAAAJ 2017 138 w2McVJAAAAAJ 2018 175 w2McVJAAAAAJ 2019 229 w2McVJAAAAAJ 2020 312 w2McVJAAAAAJ 2021 220 w2McVJAAAAAJ 2012 1 ItITloQAAAAJ 2013 5 ItITloQAAAAJ 2014 12 ItITloQAAAAJ 2015 15 ItITloQAAAAJ 2016 38 ItITloQAAAAJ 2017 38 ItITloQAAAAJ 2018 57 ItITloQAAAAJ 2019 71 ItITloQAAAAJ 2020 74 ItITloQAAAAJ 2021 60 ItITloQAAAAJ 1999 11 TqKrXnMAAAAJ 2000 17 TqKrXnMAAAAJ 2001 28 TqKrXnMAAAAJ 2002 33 TqKrXnMAAAAJ 2003 44 TqKrXnMAAAAJ 2004 41 TqKrXnMAAAAJ 2005 61 TqKrXnMAAAAJ 2006 64 TqKrXnMAAAAJ 2007 83 TqKrXnMAAAAJ 2008 109 TqKrXnMAAAAJ 2009 102 TqKrXnMAAAAJ 2010 148 TqKrXnMAAAAJ 2011 196 TqKrXnMAAAAJ 2012 129 TqKrXnMAAAAJ 2013 222 TqKrXnMAAAAJ 2014 236 TqKrXnMAAAAJ 2015 251 TqKrXnMAAAAJ 2016 305 TqKrXnMAAAAJ 2017 301 TqKrXnMAAAAJ 2018 295 TqKrXnMAAAAJ 2019 308 TqKrXnMAAAAJ 2020 299 TqKrXnMAAAAJ 2021 259 TqKrXnMAAAAJ 2017 1 bDPtkIoAAAAJ 2018 4 bDPtkIoAAAAJ 2019 8 bDPtkIoAAAAJ 2020 13 bDPtkIoAAAAJ 2021 10 bDPtkIoAAAAJ 2003 12 p3IwtT4AAAAJ 2004 26 p3IwtT4AAAAJ 2005 35 p3IwtT4AAAAJ 2006 39 p3IwtT4AAAAJ 2007 76 p3IwtT4AAAAJ 2008 54 p3IwtT4AAAAJ 2009 78 p3IwtT4AAAAJ 2010 75 p3IwtT4AAAAJ 2011 81 p3IwtT4AAAAJ 2012 92 p3IwtT4AAAAJ 2013 74 p3IwtT4AAAAJ 2014 87 p3IwtT4AAAAJ 2015 75 p3IwtT4AAAAJ 2016 85 p3IwtT4AAAAJ 2017 62 p3IwtT4AAAAJ 2018 73 p3IwtT4AAAAJ 2019 65 p3IwtT4AAAAJ 2020 70 p3IwtT4AAAAJ 2021 49 p3IwtT4AAAAJ 2005 17 _ukytQYAAAAJ 2006 19 _ukytQYAAAAJ 2007 34 _ukytQYAAAAJ 2008 73 _ukytQYAAAAJ 2009 56 _ukytQYAAAAJ 2010 82 _ukytQYAAAAJ 2011 40 _ukytQYAAAAJ 2012 59 _ukytQYAAAAJ 2013 60 _ukytQYAAAAJ 2014 87 _ukytQYAAAAJ 2015 73 _ukytQYAAAAJ 2016 89 _ukytQYAAAAJ 2017 314 _ukytQYAAAAJ 2018 750 _ukytQYAAAAJ 2019 922 _ukytQYAAAAJ 2020 1100 _ukytQYAAAAJ 2021 1461 _ukytQYAAAAJ 2017 4 VCTvbTkAAAAJ 2018 4 VCTvbTkAAAAJ 2019 7 VCTvbTkAAAAJ 2020 5 VCTvbTkAAAAJ 2021 12 VCTvbTkAAAAJ 2019 10 tFaMPOQAAAAJ 2020 29 tFaMPOQAAAAJ 2021 31 tFaMPOQAAAAJ 2016 4 z6iMs-UAAAAJ 2017 9 z6iMs-UAAAAJ 2018 6 z6iMs-UAAAAJ 2019 6 z6iMs-UAAAAJ 2020 6 z6iMs-UAAAAJ 2021 3 z6iMs-UAAAAJ 2010 6 lkVq32sAAAAJ 2011 3 lkVq32sAAAAJ 2012 12 lkVq32sAAAAJ 2013 8 lkVq32sAAAAJ 2014 27 lkVq32sAAAAJ 2015 10 lkVq32sAAAAJ 2016 15 lkVq32sAAAAJ 2017 11 lkVq32sAAAAJ 2018 13 lkVq32sAAAAJ 2019 33 lkVq32sAAAAJ 2020 39 lkVq32sAAAAJ 2021 17 lkVq32sAAAAJ 2013 1 iR4UIwwAAAAJ 2014 1 iR4UIwwAAAAJ 2015 2 iR4UIwwAAAAJ 2016 4 iR4UIwwAAAAJ 2017 4 iR4UIwwAAAAJ 2018 3 iR4UIwwAAAAJ 2019 3 iR4UIwwAAAAJ 2020 6 iR4UIwwAAAAJ 2021 7 iR4UIwwAAAAJ 2019 1 gs0li6MAAAAJ 2020 0 gs0li6MAAAAJ 2021 2 gs0li6MAAAAJ 2017 3 KfLALRIAAAAJ 2018 4 KfLALRIAAAAJ 2019 10 KfLALRIAAAAJ 2020 11 KfLALRIAAAAJ 2021 11 KfLALRIAAAAJ Next, we get the collaborators. Note that on Google Scholar, people add co-authors manually. Unfortunately, we can only scrape the first twenty with the scholar package. Some scholars have more co-authors listed, but for that you would need to click on VIEW ALL on Google Scholar. The scholar package does not do that for us. However, you could take a look at the underlying code of get_coathors and attempts to do that yourself. Because scholars add their own co-authors, it is a directed tie. For instance, I could list Albert Einstein as my co-author, but Albert probably doesnt reciprocate that tie. The for loop should be clear by now. We get collaborators for a given Google Scholar ID, 20 of them, with a depth of at most 1. We then bind_rows again, and remove those staff members that did not list any collaborator. source(&quot;addfiles/fcollabs.R&quot;) # Put the function_fix.R in your working directory, we need this first line. # first the soc collaborators note how we already build a function (fcollabs()) for you you need to # input a google scholar id and a 1 (if you want to find collabs) or 0 (only extracting names) # fcollabs --&gt; you can check it out if you&#39;re interested soc_collabs &lt;- list() for (i in 1:nrow(soc_df)) { time &lt;- runif(1, 0, 5) Sys.sleep(time) soc_collabs[[i]] &lt;- fcollabs(soc_df[i, c(&quot;gs_id&quot;)], 1) } soc_collabs &lt;- bind_rows(soc_collabs) # bind rows, get the unique ones! soc_collabs_unique &lt;- unique(soc_collabs[, 3]) # so 149 unique collaborators for RU staff? soc_collabs_unique &lt;- soc_collabs_unique[!is.na(soc_collabs_unique)] #------------------------------------------------------------------------ # then the names of those collaborators plus THEIR collaborators understand that we don&#39;t have # names of them yet from the code above? collabs_1deep &lt;- list() for (i in 1:length(soc_collabs_unique)) { time &lt;- runif(1, 0, 3) Sys.sleep(time) if (!soc_collabs_unique[i] %in% soc_df$gs_id) { collabs_1deep[[i]] &lt;- fcollabs(soc_collabs_unique[i], 1) } } collabs_1deep &lt;- bind_rows(collabs_1deep) collabs_1deep_unique &lt;- unique(collabs_1deep[, 1]) collabs_1deep_unique &lt;- collabs_1deep_unique[!is.na(collabs_1deep_unique)] #------------------------------------------------------------------------ # names of the collaborators of collaborators collabs_1deep_names &lt;- list() for (i in 1:length(collabs_1deep_unique)) { if (!collabs_1deep_unique[i] %in% unique(collabs_1deep[, 2]) &amp; !collabs_1deep_unique[i] %in% soc_df$gs_id) { # note the if not in...and in... time &lt;- runif(1, 0, 3) Sys.sleep(time) collabs_1deep_names[[i]] &lt;- fcollabs(collabs_1deep_unique[i], 0) } } collabs_1deep_names &lt;- bind_rows(collabs_1deep_names) # scroll_box(knitr::kable(soc_df_collabs, booktabs = TRUE) , height=&#39;300px&#39;) Lets save the data we may use in for the next tutorial. # save the DFs thus far save(soc_df_publications, &quot;addfiles/soc_df_publications.rda&quot;) save(soc_df, &quot;addfiles/soc_df.rda&quot;) save(soc_df_collabs, &quot;addfiles/soc_df_collabs.rda&quot;) # save(soc_art_cit, &#39;addfiles/soc_art_cit.rda&#39;)) Notice how we did this one for you. save(soc_staff_cit, &quot;addfiles/soc_staff_cit.rda&quot;) By just eyeballing the data there are two suspicous outliers: Muller and Firat and Franken have suspiciously many citations for PhD candidates. By looking up their Google Scholar profiles, we find that both have the wrong Google Scholar ID attached to them (common names error). So we deleted them from the data. The rest seems to have worked fine: less than 5% data corruption in this webscraping effort as there were 45 records in sociology staff directory. Nicely done, this was the webscraping tutorial for bibliometric data. We gathered useful information about sociology staff: - 1.1 who actually is the staff on the RU website? - 1.2 staff google scholar profiles (we merged 1.1 and 1.2) - 2 publications and total cites per publication - 3 collaborators plus their collaborators (friends-of-friends) - 4 publication citation history (cites per year) - 5 citation history of scholars themselves (cites per year) With this, you can move on to some potentially very cool network visualization! References "],["network-visualization.html", "Chapter 12 Network Visualization 12.1 Zacharys karate club 12.2 Smallworld 12.3 Twittersphere in the Dutch HoP", " Chapter 12 Network Visualization .button1 { background-color: #f44336; /* Red */ border: none; color: white; padding: 15px 32px; text-align: center; text-decoration: none; display: inline-block; font-size: 16px; margin: 4px 2px; cursor: pointer; } .button1:hover { box-shadow: 0 12px 16px 0 rgba(0,0,0,0.24), 0 17px 50px 0 rgba(0,0,0,0.19); } .button1 {border-radius: 12px;} .button1 {width: 100%;} Visualizations of social networks are cool. I am often impressed if I see them in papers. They seem to signal that the authors are technically savvy. If you know me a little bit you will also know that I think that the importance of a good description of your data cannot be downplayed: describing before (and over) explanation; simplicity (e.g. univariate or bivatiate statistics) trumps complexity. What is a better description of your network data than a graphical summary of your network? What could I possibly have against network visualizations? Unfortunately, more often than not the story behind the (cool, dynamic, interactive) network picture is not clear. I do not see selection and influence processes. We have have to guess what the node and tie attributes are. And, importantly, typical network structures remain elusive. A good example of a bad network visualization is Smallworld, introduced in section 1.4.1.1. Paradoxically, one of the reasons why oftentimes it is not worth the time to look at network visualisations is because scholars did not think it was worth the time to look at the data. The take home message of this chapter is that before you can make a successful network visualization you need to answer the following question: What story do I want my network visualization to tell? The answer to this question depends on: - your research question - descriptive statistics of your data - how preliminary network plots are able to convey your message. In this chapter we will take on three case studies: Smallworld: Should tell the story that a typical smallworld network has a relatively low density, high level of clustering and low average path length. Twittersphere in the Dutch House of Parliament: should tell the story that twitter networks are segregated along party affiliation. Co-author network of sociology staff of Radboud University Nijmegen: But before we take on these case studies, we will start with a classical example: Zacharys karate club (Zachary 1977). Where for our analyses we heavily relay on the R package lavaan (Rosseel 2012) and RSiena (T. A. B. Snijders et al. 2021), for network description and visualisation we will mainly use igraph (file. 2020). 12.1 Zacharys karate club As stated above, we will mainly rely on the igraph package. This means we have to make an igraph graph object before we can start doing anything. Luckily igraph comes with Zacharys network build-in. But dont worry, we will start from scratch later on. Let us have a first look at the data. require(igraph) g &lt;- make_graph(&quot;Zachary&quot;) plot(g) gmat &lt;- as_adjacency_matrix(g, type = &quot;both&quot;, sparse = FALSE) gmat #&gt; [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] #&gt; [1,] 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 #&gt; [2,] 1 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 #&gt; [3,] 1 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 0 #&gt; [4,] 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 #&gt; [5,] 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 #&gt; [6,] 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 #&gt; [7,] 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 #&gt; [8,] 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [9,] 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [10,] 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [11,] 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 #&gt; [12,] 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [13,] 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [14,] 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [15,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [16,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [17,] 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 #&gt; [18,] 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [19,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [20,] 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [21,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [22,] 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [23,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [24,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [25,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [26,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [27,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [28,] 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [29,] 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [30,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [31,] 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 #&gt; [32,] 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [33,] 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 #&gt; [34,] 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 #&gt; [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] #&gt; [1,] 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 #&gt; [2,] 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 #&gt; [3,] 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 #&gt; [4,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [5,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [6,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [7,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [8,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [9,] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 #&gt; [10,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [11,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [12,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [13,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [14,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [15,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [16,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [17,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [18,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [19,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [20,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [21,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [22,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [23,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [24,] 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 #&gt; [25,] 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 #&gt; [26,] 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 #&gt; [27,] 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 #&gt; [28,] 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 #&gt; [29,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 #&gt; [30,] 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 #&gt; [31,] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 #&gt; [32,] 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 #&gt; [33,] 0 1 0 1 0 1 1 0 0 0 0 0 1 1 1 #&gt; [34,] 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 #&gt; [,33] [,34] #&gt; [1,] 0 0 #&gt; [2,] 0 0 #&gt; [3,] 1 0 #&gt; [4,] 0 0 #&gt; [5,] 0 0 #&gt; [6,] 0 0 #&gt; [7,] 0 0 #&gt; [8,] 0 0 #&gt; [9,] 1 1 #&gt; [10,] 0 1 #&gt; [11,] 0 0 #&gt; [12,] 0 0 #&gt; [13,] 0 0 #&gt; [14,] 0 1 #&gt; [15,] 1 1 #&gt; [16,] 1 1 #&gt; [17,] 0 0 #&gt; [18,] 0 0 #&gt; [19,] 1 1 #&gt; [20,] 0 1 #&gt; [21,] 1 1 #&gt; [22,] 0 0 #&gt; [23,] 1 1 #&gt; [24,] 1 1 #&gt; [25,] 0 0 #&gt; [26,] 0 0 #&gt; [27,] 0 1 #&gt; [28,] 0 1 #&gt; [29,] 0 1 #&gt; [30,] 1 1 #&gt; [31,] 1 1 #&gt; [32,] 1 1 #&gt; [33,] 0 1 #&gt; [34,] 1 0 12.1.1 Descriptive statistics 12.1.1.1 Size # number of nodes vcount(g) #&gt; [1] 34 # number of edges ecount(g) #&gt; [1] 78 12.1.1.2 node-level We discussed several network structure in section 5.3.3. Decide for yourself at which structures you want to look at. I give three examples below. degree degree(g) #&gt; [1] 16 9 10 6 3 4 4 4 5 2 3 1 2 5 2 2 2 2 2 3 2 2 2 5 3 3 2 4 3 4 4 6 #&gt; [33] 12 17 # hist(table(degree(g)), xlab=&#39;indegree&#39;, main= &#39;Histogram of indegree&#39;) transitivity # be aware that directed graphs are considered as undirected. but g is undirected. transitivity(g, type = c(&quot;localundirected&quot;), isolates = c(&quot;NaN&quot;, &quot;zero&quot;)) #&gt; [1] 0.1500000 0.3333333 0.2444444 0.6666667 0.6666667 0.5000000 0.5000000 1.0000000 0.5000000 #&gt; [10] 0.0000000 0.6666667 NaN 1.0000000 0.6000000 1.0000000 1.0000000 1.0000000 1.0000000 #&gt; [19] 1.0000000 0.3333333 1.0000000 1.0000000 1.0000000 0.4000000 0.3333333 0.3333333 1.0000000 #&gt; [28] 0.1666667 0.3333333 0.6666667 0.5000000 0.2000000 0.1969697 0.1102941 betweenness igraph::betweenness(g, directed = FALSE) #&gt; [1] 231.0714286 28.4785714 75.8507937 6.2880952 0.3333333 15.8333333 15.8333333 0.0000000 #&gt; [9] 29.5293651 0.4476190 0.3333333 0.0000000 0.0000000 24.2158730 0.0000000 0.0000000 #&gt; [17] 0.0000000 0.0000000 0.0000000 17.1468254 0.0000000 0.0000000 0.0000000 9.3000000 #&gt; [25] 1.1666667 2.0277778 0.0000000 11.7920635 0.9476190 1.5428571 7.6095238 73.0095238 #&gt; [33] 76.6904762 160.5515873 12.1.2 dyad-census dyad.census(g) #&gt; $mut #&gt; [1] 78 #&gt; #&gt; $asym #&gt; [1] 0 #&gt; #&gt; $null #&gt; [1] 483 12.1.3 triad-census igraph::triad.census(g) #&gt; [1] 3971 0 1575 0 0 0 0 0 0 0 393 0 0 0 0 45 sna::triad.census(gmat) #&gt; 003 012 102 021D 021U 021C 111D 111U 030T 030C 201 120D 120U 120C 210 300 #&gt; [1,] 3971 0 1575 0 0 0 0 0 0 0 393 0 0 0 0 45 Based on the above triad.census, please calculate the global transitivity of the network and check your answer with igraph or sna. Only click button after 5 minutes! igraph::transitivity(g, type = &quot;global&quot;) #&gt; [1] 0.2556818 sna::gtrans(gmat) #&gt; [1] 0.2556818 triad_g &lt;- data.frame(sna::triad.census(gmat)) transitivity_g &lt;- (3 * triad_g$X300)/(triad_g$X201 + 3 * triad_g$X300) transitivity_g #&gt; [1] 0.2556818   Even if you did not read-up on Zacharys Karate club (Zachary 1977; Girvan and Newman 2002; Kadushin 2012) our initial plot and the above descriptive network statistics start to tell a story. There are two very central nodes (based on degree and betweenness) but their local transitivity index is relatively low (compared to other nodes). Are these two nodes connected? Well gmat[1,34] returns: 0. Thus NO! 12.1.4 Network visualisation Lets make size proportional to betweenness score: # changing V V(g)$size = betweenness(g, normalized = T, directed = FALSE) * 60 + 10 #after some trial and error plot(g, mode = &quot;undirected&quot;) It would be nice if we could place the nodes 1 and 34 further apart. Preferably based on some nice algoritm. After some browsing in the igraph manual, I came up with this. set.seed(2345) l &lt;- layout_with_mds(g) #https://igraph.org/r/doc/layout_with_mds.html plot(g, layout = l) It does more or less what we want, but it would be great to place nodes 1 and 34 even further apart. l l[1, 1] &lt;- 4 l[34, 1] &lt;- -3.5 plot(g, layout = l) #&gt; [,1] [,2] #&gt; [1,] 1.070931935 -0.172458113 #&gt; [2,] 0.732844464 0.754023309 #&gt; [3,] 0.100582299 0.397693607 #&gt; [4,] 0.708246655 0.570205545 #&gt; [5,] 1.816293170 -0.120778206 #&gt; [6,] 1.881329566 -0.135518854 #&gt; [7,] 1.881329566 -0.135518854 #&gt; [8,] 0.812606714 0.472619437 #&gt; [9,] -0.003769996 0.615513628 #&gt; [10,] -0.685680315 0.621065149 #&gt; [11,] 1.816293170 -0.120778206 #&gt; [12,] 1.621247830 -0.065820692 #&gt; [13,] 1.637845123 0.001789972 #&gt; [14,] 0.067317230 0.681421148 #&gt; [15,] -1.796316404 0.351417630 #&gt; [16,] -1.796316404 0.351417630 #&gt; [17,] 2.775260452 -0.124317652 #&gt; [18,] 1.616210024 0.182510197 #&gt; [19,] -1.796316404 0.351417630 #&gt; [20,] 0.048362858 0.566654982 #&gt; [21,] -1.796316404 0.351417630 #&gt; [22,] 1.616210024 0.182510197 #&gt; [23,] -1.796316404 0.351417630 #&gt; [24,] -1.891240567 -0.799574907 #&gt; [25,] -0.258345165 -2.006346563 #&gt; [26,] -0.360530857 -2.131642875 #&gt; [27,] -1.865177401 0.128596564 #&gt; [28,] -0.760226022 -0.529392331 #&gt; [29,] -0.710979936 -0.299960128 #&gt; [30,] -1.898426916 -0.149398746 #&gt; [31,] -0.568691923 0.804189411 #&gt; [32,] -0.048136037 -0.870967614 #&gt; [33,] -1.023681000 -0.035802363 #&gt; [34,] -1.146442924 -0.037605192 It now more or less looks like nodes 1 and 34 are ripping the network in two. Hey, that is a funny coincidence, let that now be exactly what the original paper was all about. 12.2 Smallworld 12.2.1 Descriptive statistics 12.2.2 Network visualisation 12.3 Twittersphere in the Dutch HoP have a look here References "],["tutorial.html", "A Introduction to R for SNA A.1 Preliminary notes A.2 Getting up and running A.3 Working with RScript A.4 Installing additional packages A.5 I dont understand the code!! A.6 Reading in data files A.7 Define missings A.8 Recoding variables A.9 Means and counting specific values A.10 Merging data files A.11 Aggregate data A.12 Missing values A.13 Very important stuff!! A.14 Assignment", " A Introduction to R for SNA A.1 Preliminary notes This very short R tutorial is for students who already have some experience with R who want to make the switch from stata/spss to R. - In this tutorial I assume you will work with RScripts (.R files) not with Rmarkdown (.Rmd files) - I will show you how to do things in base R and in a Tidyverse way. A.2 Getting up and running install the latest version of R: R install the latest version of RStudio: RStudio open RStudio and follow a brief tour/tutorial brief tour of Gulzar Do you Want more information, or are you looking for a different (tidyverse) tutorial? R-bloggers RStudio cheatsheets R for Data Science Statistical Inference via Data Science: A Modern Dive into R and the tidyverse Are you a Research Master Social and Cultural Science student? Or, a social science student/scientist with some statistical background in descriptive and explanatory statistics (e.g. regression analysis) who wants to make a switch from SPSS to R? Please read on. Open RStudio. Your screen will look something like this: Figure A.1: Screenshot Rstudio During the workgroup I will show you around the major subwindows and taps in RStudio. A.3 Working with RScript Open a new R-script (via file &gt; new &gt; RScript (see Figure A.1 Arrow 1), or simply hit Ctrl+Shift+N) Start your script with your name and date. Start with a clean workspace. Start with the latest versions of R, RStudio and your packages. Load the additional packages you will need later. Define your workdirectory. Thus your RScript will look something like this: ########################### Title: Introducation to R for SNA Author: J Tolsma version: 30-10-2019 # start with clean workspace rm(list = ls()) # install.packages I will need later here install.packages(&quot;installr&quot;) #you first install packages require(installr) #then you will need to load them. This package is used to simply update R install.packages(&quot;foreign&quot;) require(foreign) #used to read in spss data files require(tidyverse) # update if necessarry. Best to run this command in RGui, not in RStudio. updateR() # define workdirectory, note the double backslashes setwd(&quot;C:\\\\SNA-4-Social-Scientists\\\\&quot;) #change to your own workdirectory Do you see I start some lines with a # these lines are comments and not code/commands. This is similar as the * sign in SPSS. To run some code, you place your cursor in the line and hit Ctrl+Enter. You may also select the code you want to run, or copy and paste it directly in the console window (A.1 Arrow 2). To see which commands you have executed, you may want to have a look at the history tab (A.1 Arrow 3). Hint 1: In the upper right corner of the code blocks you see a copy-and-paste sign. You may use this to copy and paste the code of this tutorial in your own script. Hint 2: You really want to learn R? Never ever copy and paste code. Type the code yourself. A.4 Installing additional packages You will probably always need to load and/or install additional packages. You may want to use RStudios functionality (A.1 Arrow 4). I normally prefer to put everything in my script. See for example in the code block above, line 9 to 12. A.5 I dont understand the code!! When you see functionname()12 it means we use a build-in function of R If you want to see how lines/commands/functions work, try to decipher them from the inside out. Thus if you want to dechiper rm(list=ls()): ls() list=ls() list Lets give it a go: tesvariable &lt;- 4 ls() #&gt; [1] &quot;colorize&quot; &quot;tesvariable&quot; list = ls() list #&gt; [1] &quot;colorize&quot; &quot;tesvariable&quot; ls() #&gt; [1] &quot;colorize&quot; &quot;list&quot; &quot;tesvariable&quot; rm(list) ls() #&gt; [1] &quot;colorize&quot; &quot;tesvariable&quot; # ? :-) rm(list = ls()) ls() #&gt; character(0) If you want to know more about specific functions, try to use the help function. For example try the following: ?ls ?rm Any idea what &lt;- does? At first it will be difficult to read the R Documentation pages. Dont worry, you will get the hang of it. How am I to remember all that code/syntax??!! By using them. You dont need to, you just need to remember in which script you used them before. By using existing cheat sheets By making your own cheat sheets. You being the ideal student, you started your own cheat sheet. What should be on it by know? Functions: install.packages() # to install additional packages. Only do this once or to update package. require() # activate installed package setwd() # set your working directory ls() # list the objects in your environment rm() # to remove objects packages: installr # a package to easily update R (needs to be run in Rgui directly instead of RStudio ) foreign # to read in spss data files tidyverse # a bunch of packages which allows for a completely different way of programming/scripting in R. operators / symbols: ? # if placed in front of a function opens up the help pages. &lt;- # used to assign values/objects to a different object. = # used to assign values/objects to arguments within a function. A.6 Reading in data files We are going to work with two datasets: Culturele Veranderingen. For more information on these datasets, see here. Please download the files to your working directory. Cultural_Changes_2008.sav Cultural_Changes_2010.sav   There are different packages to read in data. Generally, I would recommend to use the haven package. In the past I used the foreign package. The advantage of using haven::read_spss is that more information is stored in the dataset and in the variables (variable/value labels!!). A disadvantage is that not all other functions/packages of R are capable of dealing with the data object that haven produces. below I will use package_name::function_name notation, to make explicit from which package the function belongs. # ignore the warnings ?read.spss note that I have saved the data files in a folder called # &#39;addfiles&#39;. cv08 &lt;- foreign::read.spss(&quot;addfiles\\\\Cultural_Changes_2008.sav&quot;, use.value.labels = T, to.data.frame = T) cv10 &lt;- foreign::read.spss(&quot;addfiles\\\\Cultural_Changes_2010.sav&quot;, use.value.labels = T, to.data.frame = T) # normally I think setting use.value.labels=F is more convenient. Thus lets load the data again but # now without labels cv08_nolab &lt;- foreign::read.spss(&quot;addfiles\\\\Cultural_Changes_2008.sav&quot;, use.value.labels = F, to.data.frame = T) cv10_nolab &lt;- foreign::read.spss(&quot;addfiles\\\\Cultural_Changes_2010.sav&quot;, use.value.labels = F, to.data.frame = T) # finally, import the data using haven cv08_haven &lt;- haven::read_spss(&quot;addfiles\\\\Cultural_Changes_2008.sav&quot;) cv10_haven &lt;- haven::read_spss(&quot;addfiles\\\\Cultural_Changes_2010.sav&quot;) So you see I read in the data by using the function read.spss() of the package foreign. Within this function I set some arguments/parameters (e.g. use.value.labels). Now we can inspect our datasets and look for some differences: Find the Environment tab in the upper right window (A.1 Arrow 5). Find the little arrow and decollapse. What do we see? Double click on the three versions of the cv08 datasets. What happens? Go to the new windows and have a look at the data. What are the differences? Close this window when finished. Lets use some build-in functions to get more information of our dataset. str(cv08) #&gt; &#39;data.frame&#39;: 1963 obs. of 278 variables: #&gt; $ we_id : Factor w/ 1963 levels &quot;36775330&quot;,&quot;36775340&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ veilignr: num 8.11e+08 8.11e+08 8.11e+08 8.11e+08 8.11e+08 ... #&gt; $ lft1 : Factor w/ 78 levels &quot;0&quot;,&quot;15&quot;,&quot;16&quot;,..: 38 26 3 17 44 36 49 21 46 28 ... #&gt; $ geslacht: Factor w/ 3 levels &quot;Onbekend&quot;,&quot;Man&quot;,..: 2 3 3 2 2 3 2 3 2 2 ... #&gt; $ allochtn: Factor w/ 4 levels &quot;geen allochtoon&quot;,..: 1 1 2 1 1 2 1 2 1 1 ... #&gt; $ lft01 : Factor w/ 82 levels &quot;&lt; één jaar&quot;,&quot;één jaar&quot;,..: 40 28 4 19 46 38 51 23 48 30 ... #&gt; $ lftop : Factor w/ 81 levels &quot;&lt; één jaar&quot;,&quot;één jaar&quot;,..: 40 28 4 18 46 38 51 23 48 30 ... #&gt; $ gewicht : num 8423 6244 13434 8997 8423 ... #&gt; $ var006n : Factor w/ 11 levels &quot;onbekend&quot;,&quot;OP &lt; 12 jr of volgt actueel bas.ondw.&quot;,..: 8 10 5 10 8 4 4 7 7 3 ... #&gt; $ v040 : Factor w/ 6 levels &quot;Geen opgave&quot;,..: 6 5 6 5 6 6 5 5 5 5 ... #&gt; $ var723 : Factor w/ 62 levels &quot;Weigert&quot;,&quot;Weet niet&quot;,..: 3 43 3 17 3 3 39 30 28 17 ... #&gt; $ var723a : Factor w/ 8 levels &quot;Geen opgave&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... #&gt; $ v202n : Factor w/ 10 levels &quot;-3&quot;,&quot;werkt &gt;12 uur&quot;,..: 6 2 9 2 5 4 2 2 2 2 ... #&gt; $ var1061a: Factor w/ 6 levels &quot;Geen opgave&quot;,..: 5 6 6 6 6 6 6 6 6 6 ... #&gt; $ var1061b: Factor w/ 31 levels &quot;Weigert&quot;,&quot;Weet niet&quot;,..: 17 3 3 3 3 3 3 3 3 3 ... #&gt; $ var1062a: Factor w/ 6 levels &quot;Geen opgave&quot;,..: 6 6 6 6 6 6 6 6 5 6 ... #&gt; $ var1062b: Factor w/ 31 levels &quot;Weigert&quot;,&quot;Weet niet&quot;,..: 3 3 3 3 3 3 3 3 21 3 ... #&gt; $ int137n : Factor w/ 8 levels &quot;Geen opgave&quot;,..: 2 7 2 5 2 5 6 7 7 6 ... #&gt; $ int138n : Factor w/ 8 levels &quot;Geen opgave&quot;,..: 7 7 7 7 6 7 7 6 7 7 ... #&gt; $ int139n : Factor w/ 8 levels &quot;Geen opgave&quot;,..: 2 7 2 2 2 7 5 5 7 2 ... #&gt; $ int140n : Factor w/ 8 levels &quot;Geen opgave&quot;,..: 2 7 7 7 5 7 5 5 7 7 ... #&gt; $ int141n : Factor w/ 8 levels &quot;Geen opgave&quot;,..: 5 7 5 7 5 7 5 5 7 5 ... #&gt; $ v401 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 6 6 6 6 7 6 7 6 6 6 ... #&gt; $ var1343 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 7 7 7 7 6 7 7 7 7 7 ... #&gt; $ var648 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 7 5 7 8 7 7 7 8 7 7 ... #&gt; $ var149 : Factor w/ 8 levels &quot;Geen opgave&quot;,..: 6 5 3 7 6 6 6 6 5 6 ... #&gt; $ var058 : Factor w/ 6 levels &quot;Geen opgave&quot;,..: 5 5 5 5 5 6 6 5 5 5 ... #&gt; $ var059 : Factor w/ 6 levels &quot;Geen opgave&quot;,..: 5 5 5 6 5 5 5 5 5 6 ... #&gt; $ var064 : Factor w/ 6 levels &quot;Geen opgave&quot;,..: 5 5 6 5 6 6 6 5 6 6 ... #&gt; $ var365 : Factor w/ 6 levels &quot;Geen opgave&quot;,..: 5 5 6 5 5 5 5 6 6 6 ... #&gt; $ var065 : Factor w/ 6 levels &quot;Geen opgave&quot;,..: 6 5 5 6 3 5 6 6 5 5 ... #&gt; $ var092 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 6 3 6 8 8 8 8 7 8 7 ... #&gt; $ var096 : Factor w/ 10 levels &quot;Geen opgave&quot;,..: 7 7 6 7 7 5 6 6 5 5 ... #&gt; $ int054 : Factor w/ 8 levels &quot;Geen opgave&quot;,..: 8 3 7 7 5 5 6 7 6 5 ... #&gt; $ int055 : Factor w/ 8 levels &quot;Geen opgave&quot;,..: 7 7 7 6 3 5 6 7 7 6 ... #&gt; $ int056 : Factor w/ 8 levels &quot;Geen opgave&quot;,..: 6 6 7 7 6 5 5 7 6 7 ... #&gt; $ int057 : Factor w/ 8 levels &quot;Geen opgave&quot;,..: 6 7 6 6 5 6 7 7 6 5 ... #&gt; $ int058 : Factor w/ 8 levels &quot;Geen opgave&quot;,..: 7 7 6 6 3 5 6 7 8 6 ... #&gt; $ int059 : Factor w/ 8 levels &quot;Geen opgave&quot;,..: 6 7 3 7 7 5 5 6 8 6 ... #&gt; $ int059a : Factor w/ 8 levels &quot;Geen opgave&quot;,..: 7 6 6 6 7 5 6 6 7 6 ... #&gt; $ var571 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 6 7 7 6 6 6 6 7 6 7 ... #&gt; $ var572 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 2 5 7 2 2 2 2 7 2 5 ... #&gt; $ var573 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 5 6 7 6 5 5 5 5 6 6 ... #&gt; $ var574 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 7 2 7 2 6 7 7 6 2 2 ... #&gt; $ var576 : Factor w/ 10 levels &quot;Geen opgave&quot;,..: 9 6 7 8 9 6 9 8 8 9 ... #&gt; $ var153 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 7 5 5 5 5 5 5 5 5 5 ... #&gt; $ var154 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 7 6 6 6 7 6 7 6 7 7 ... #&gt; $ var155 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 6 6 6 7 7 7 7 3 6 6 ... #&gt; $ var156 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 7 6 6 6 6 3 7 6 6 6 ... #&gt; $ var157 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 6 3 6 6 6 7 7 3 6 3 ... #&gt; $ var157a : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 7 3 6 6 7 7 7 6 6 6 ... #&gt; $ var154a : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 3 3 6 6 6 6 6 6 6 6 ... #&gt; $ var164 : Factor w/ 8 levels &quot;Geen opgave&quot;,..: 6 6 6 5 5 5 5 6 5 5 ... #&gt; $ var165 : Factor w/ 10 levels &quot;Geen opgave&quot;,..: 7 7 7 8 9 9 8 8 9 8 ... #&gt; $ var166 : Factor w/ 10 levels &quot;Geen opgave&quot;,..: 7 7 8 7 9 9 8 7 8 8 ... #&gt; $ var179 : Factor w/ 6 levels &quot;Geen opgave&quot;,..: 5 5 6 5 5 5 5 5 5 5 ... #&gt; $ var180 : Factor w/ 6 levels &quot;Geen opgave&quot;,..: 5 5 5 6 5 6 6 5 5 5 ... #&gt; $ var184 : Factor w/ 6 levels &quot;Geen opgave&quot;,..: 5 5 6 5 6 5 5 5 5 5 ... #&gt; $ var185 : Factor w/ 6 levels &quot;Geen opgave&quot;,..: 6 5 5 5 6 5 5 5 5 5 ... #&gt; $ var198a : Factor w/ 6 levels &quot;Geen opgave&quot;,..: 6 5 5 5 5 5 6 6 6 5 ... #&gt; $ var198 : Factor w/ 11 levels &quot;Geen opgave&quot;,..: 2 7 9 7 5 5 2 2 2 7 ... #&gt; $ var201a : Factor w/ 6 levels &quot;Geen opgave&quot;,..: 6 5 5 6 6 5 6 6 6 6 ... #&gt; $ var201b : Factor w/ 10 levels &quot;Geen opgave&quot;,..: 2 6 8 2 2 5 2 2 2 2 ... #&gt; $ var204 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 9 8 9 8 9 8 9 9 7 9 ... #&gt; $ int257 : Factor w/ 11 levels &quot;Geen opgave&quot;,..: 11 7 7 8 10 6 7 9 7 11 ... #&gt; $ var211 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 7 6 7 7 7 5 7 6 6 7 ... #&gt; $ var223 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 5 6 5 7 5 5 5 7 5 5 ... #&gt; $ var1320 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 5 6 6 6 6 6 6 7 5 5 ... #&gt; $ var1321 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 5 8 6 6 8 6 8 8 6 6 ... #&gt; $ var1322 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 7 7 6 6 7 5 5 6 5 5 ... #&gt; $ var1323 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 7 7 6 7 7 6 7 7 7 7 ... #&gt; $ var1324 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 7 7 6 7 7 7 7 7 7 7 ... #&gt; $ var1325 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 7 7 7 7 7 6 7 7 7 6 ... #&gt; $ var1326 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 7 7 7 7 7 7 7 7 7 7 ... #&gt; $ var1327 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 7 7 6 7 7 6 7 7 6 7 ... #&gt; $ var1328 : Factor w/ 7 levels &quot;Geen opgave&quot;,..: 7 7 7 7 7 7 7 7 7 7 ... #&gt; $ var229 : Factor w/ 12 levels &quot;Geen opgave&quot;,..: 7 7 7 12 7 7 7 6 7 7 ... #&gt; $ int218 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 6 6 7 7 6 8 7 7 8 7 ... #&gt; $ int219 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 6 6 7 7 7 6 6 6 7 7 ... #&gt; $ int221 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 9 6 9 7 7 6 7 7 6 7 ... #&gt; $ int222 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 6 6 7 6 8 7 7 6 8 7 ... #&gt; $ int223 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 7 7 5 6 7 6 7 6 5 8 ... #&gt; $ int710 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 8 6 5 7 7 6 7 7 6 7 ... #&gt; $ int711 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 8 6 7 6 7 9 7 6 5 8 ... #&gt; $ int712 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 6 7 7 7 7 6 8 6 8 8 ... #&gt; $ int713 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 7 7 3 7 6 8 8 6 8 7 ... #&gt; $ int714 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 7 6 3 7 7 7 8 7 8 8 ... #&gt; $ int715 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 7 6 9 6 6 7 9 7 7 8 ... #&gt; $ int716 : Factor w/ 9 levels &quot;Geen opgave&quot;,..: 9 7 7 7 6 6 7 7 7 7 ... #&gt; $ var433 : Factor w/ 10 levels &quot;Geen opgave&quot;,..: 6 6 7 7 7 5 9 6 7 6 ... #&gt; $ var439 : Factor w/ 10 levels &quot;Geen opgave&quot;,..: 6 5 9 8 3 5 5 8 5 9 ... #&gt; $ var1329 : Factor w/ 10 levels &quot;Geen opgave&quot;,..: 6 8 6 8 6 5 6 5 5 6 ... #&gt; $ var1330 : Factor w/ 10 levels &quot;Geen opgave&quot;,..: 9 9 8 7 9 5 5 8 5 6 ... #&gt; $ var445 : Factor w/ 10 levels &quot;Geen opgave&quot;,..: 8 9 6 6 8 8 9 6 7 6 ... #&gt; $ var446 : Factor w/ 10 levels &quot;Geen opgave&quot;,..: 7 9 5 8 6 5 6 7 5 5 ... #&gt; $ var447 : Factor w/ 10 levels &quot;Geen opgave&quot;,..: 7 9 5 8 6 5 8 7 5 5 ... #&gt; $ var451 : Factor w/ 10 levels &quot;Geen opgave&quot;,..: 9 9 8 8 8 5 8 9 9 8 ... #&gt; $ var452 : Factor w/ 10 levels &quot;Geen opgave&quot;,..: 6 7 6 6 6 5 8 7 5 5 ... #&gt; $ var1316 : Factor w/ 10 levels &quot;Geen opgave&quot;,..: 9 9 8 8 9 5 6 6 5 9 ... #&gt; [list output truncated] #&gt; - attr(*, &quot;variable.labels&quot;)= Named chr [1:278] &quot;WE_ID&quot; &quot;veilignummer&quot; &quot;Leeftijd op 1-jan-2009&quot; &quot;Geslacht hhpersoon (GBA)&quot; ... #&gt; ..- attr(*, &quot;names&quot;)= chr [1:278] &quot;we_id&quot; &quot;veilignr&quot; &quot;lft1&quot; &quot;geslacht&quot; ... #&gt; - attr(*, &quot;codepage&quot;)= int 1252 str(cv08_nolab) #&gt; &#39;data.frame&#39;: 1963 obs. of 278 variables: #&gt; $ we_id : num 36775330 36775340 36775420 36775440 36775450 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named num(0) #&gt; .. ..- attr(*, &quot;names&quot;)= chr(0) #&gt; $ veilignr: num 8.11e+08 8.11e+08 8.11e+08 8.11e+08 8.11e+08 ... #&gt; $ lft1 : num 51 39 16 30 57 49 62 34 59 41 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr &quot;99&quot; #&gt; .. ..- attr(*, &quot;names&quot;)= chr &quot;Onbekend&quot; #&gt; $ geslacht: chr &quot;M&quot; &quot;V&quot; &quot;V&quot; &quot;M&quot; ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:3] &quot;V &quot; &quot;M &quot; &quot;9 &quot; #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Vrouw&quot; &quot;Man&quot; &quot;Onbekend&quot; #&gt; $ allochtn: num 0 0 1 0 0 1 0 1 0 0 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:4] &quot;9&quot; &quot;2&quot; &quot;1&quot; &quot;0&quot; #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Onbekend&quot; &quot;onbekend&quot; &quot;allochtoon&quot; &quot;geen allochtoon&quot; #&gt; $ lft01 : num 50 38 15 29 56 48 61 33 58 40 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:5] &quot;125&quot; &quot;99&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;125 jaar&quot; &quot;Onbekend&quot; &quot;twee jaar&quot; &quot;één jaar&quot; ... #&gt; $ lftop : num 51 39 16 29 57 49 62 34 59 41 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:5] &quot;125&quot; &quot;99&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;125 jaar&quot; &quot;Onbekend&quot; &quot;twee jaar&quot; &quot;één jaar&quot; ... #&gt; $ gewicht : num 8423 6244 13434 8997 8423 ... #&gt; $ var006n : num 6 8 3 8 6 2 2 5 5 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:11] &quot;9999999999&quot; &quot;8&quot; &quot;7&quot; &quot;6&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;Onbekend&quot; &quot;wo&quot; &quot;wo&quot; &quot;hbo&quot; ... #&gt; $ v040 : num 2 1 2 1 2 2 1 1 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:6] &quot;2&quot; &quot;1&quot; &quot;-2&quot; &quot;-3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Nee&quot; &quot;Ja&quot; &quot;Weigert&quot; &quot;Weet niet&quot; ... #&gt; $ var723 : num -5 45 -5 20 -5 -5 40 32 30 20 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:4] &quot;-2&quot; &quot;-3&quot; &quot;-5&quot; &quot;-6&quot; #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Weigert&quot; &quot;Weet niet&quot; &quot;N.v.t.&quot; &quot;Geen opgave&quot; #&gt; $ var723a : num -5 -5 -5 -5 -5 -5 -5 -5 -5 -5 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:8] &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;of om 30 uur of meer per week ?&quot; &quot;minder dan 30 uur,&quot; &quot;minder dan 12 uur,&quot; &quot;4 uur of minder per week,&quot; ... #&gt; $ v202n : num 4 1 7 1 3 2 1 1 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;10&quot; &quot;8&quot; &quot;7&quot; &quot;6&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;vrijwilliger&quot; &quot;anders&quot; &quot;scholier, student&quot; &quot;werkt &lt;12 uur&quot; ... #&gt; $ var1061a: num 1 2 2 2 2 2 2 2 2 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:6] &quot;2&quot; &quot;1&quot; &quot;-2&quot; &quot;-3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Nee&quot; &quot;Ja&quot; &quot;Weigert&quot; &quot;Weet niet&quot; ... #&gt; $ var1061b: num 23 -5 -5 -5 -5 -5 -5 -5 -5 -5 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:4] &quot;-2&quot; &quot;-3&quot; &quot;-5&quot; &quot;-6&quot; #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Weigert&quot; &quot;Weet niet&quot; &quot;N.v.t.&quot; &quot;Geen opgave&quot; #&gt; $ var1062a: num 2 2 2 2 2 2 2 2 1 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:6] &quot;2&quot; &quot;1&quot; &quot;-2&quot; &quot;-3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Nee&quot; &quot;Ja&quot; &quot;Weigert&quot; &quot;Weet niet&quot; ... #&gt; $ var1062b: num -5 -5 -5 -5 -5 -5 -5 -5 3 -5 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:4] &quot;-2&quot; &quot;-3&quot; &quot;-5&quot; &quot;-6&quot; #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Weigert&quot; &quot;Weet niet&quot; &quot;N.v.t.&quot; &quot;Geen opgave&quot; #&gt; $ int137n : num -5 3 -5 1 -5 1 2 3 3 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:8] &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Niet van toepassing (niet noemen)&quot; &quot;of net zoveel tijd als nu?&quot; &quot;minder tijd,&quot; &quot;meer tijd,&quot; ... #&gt; $ int138n : num 3 3 3 3 2 3 3 2 3 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:8] &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Niet van toepassing (niet noemen)&quot; &quot;Net zoveel tijd als nu&quot; &quot;Minder tijd&quot; &quot;Meer tijd&quot; ... #&gt; $ int139n : num -5 3 -5 -5 -5 3 1 1 3 -5 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:8] &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Niet van toepassing (niet noemen)&quot; &quot;Net zoveel tijd als nu&quot; &quot;Minder tijd&quot; &quot;Meer tijd&quot; ... #&gt; $ int140n : num -5 3 3 3 1 3 1 1 3 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:8] &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Niet van toepassing (niet noemen)&quot; &quot;of net zoveel tijd als nu?&quot; &quot;minder tijd,&quot; &quot;meer tijd,&quot; ... #&gt; $ int141n : num 1 3 1 3 1 3 1 1 3 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:8] &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Niet van toepassing (niet noemen)&quot; &quot;Net zoveel tijd als nu&quot; &quot;Minder tijd&quot; &quot;Meer tijd&quot; ... #&gt; $ v401 : num 2 2 2 2 3 2 3 2 2 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;of zeer slecht?&quot; &quot;slecht,&quot; &quot;gaat wel,&quot; &quot;goed,&quot; ... #&gt; $ var1343 : num 3 3 3 3 2 3 3 3 3 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Nee&quot; &quot;Soms&quot; &quot;Ja&quot; &quot;Weigert&quot; ... #&gt; $ var648 : num 3 1 3 4 3 3 3 4 3 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;of niet zo tevreden?&quot; &quot;tamelijk tevreden,&quot; &quot;tevreden,&quot; &quot;zeer tevreden,&quot; ... #&gt; $ var149 : num 2 1 -3 3 2 2 2 2 1 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:8] &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen mening&quot; &quot;Niet tevreden&quot; &quot;Tamelijk tevreden&quot; &quot;Tevreden&quot; ... #&gt; $ var058 : num 1 1 1 1 1 2 2 1 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:6] &quot;2&quot; &quot;1&quot; &quot;-2&quot; &quot;-3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Nee&quot; &quot;Ja&quot; &quot;Weigert&quot; &quot;Weet niet&quot; ... #&gt; $ var059 : num 1 1 1 2 1 1 1 1 1 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:6] &quot;2&quot; &quot;1&quot; &quot;-2&quot; &quot;-3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Nee&quot; &quot;Ja&quot; &quot;Weigert&quot; &quot;Weet niet&quot; ... #&gt; $ var064 : num 1 1 2 1 2 2 2 1 2 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:6] &quot;2&quot; &quot;1&quot; &quot;-2&quot; &quot;-3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Onvoldoende&quot; &quot;Voldoende&quot; &quot;Weigert&quot; &quot;Weet niet&quot; ... #&gt; $ var365 : num 1 1 2 1 1 1 1 2 2 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:6] &quot;2&quot; &quot;1&quot; &quot;-2&quot; &quot;-3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Nee&quot; &quot;Ja&quot; &quot;Weigert&quot; &quot;Weet niet&quot; ... #&gt; $ var065 : num 2 1 1 2 -3 1 2 2 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:6] &quot;2&quot; &quot;1&quot; &quot;-2&quot; &quot;-3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Welvaart houdt aan&quot; &quot;Voorziet crisis&quot; &quot;Weigert&quot; &quot;Weet niet&quot; ... #&gt; $ var092 : num 2 -3 2 4 4 4 4 3 4 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen mening&quot; &quot;of gaat achteruit?&quot; &quot;blijft ongeveer gelijk,&quot; &quot;gedeeltelijk vooruit gedeeltelijk achteruit,&quot; ... #&gt; $ var096 : num 3 3 2 3 3 1 2 2 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:10] &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen mening&quot; &quot;Veel minder&quot; &quot;Een beetje minder&quot; &quot;Laten zoals nu&quot; ... #&gt; $ int054 : num 4 -3 3 3 1 1 2 3 2 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:8] &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Helemaal geen tegenstelling&quot; &quot;Niet zo groot&quot; &quot;Groot&quot; &quot;Zeer groot&quot; ... #&gt; $ int055 : num 3 3 3 2 -3 1 2 3 3 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:8] &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Helemaal geen tegenstelling&quot; &quot;Niet zo groot&quot; &quot;Groot&quot; &quot;Zeer groot&quot; ... #&gt; $ int056 : num 2 2 3 3 2 1 1 3 2 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:8] &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Helemaal geen tegenstelling&quot; &quot;Niet zo groot&quot; &quot;Groot&quot; &quot;Zeer groot&quot; ... #&gt; $ int057 : num 2 3 2 2 1 2 3 3 2 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:8] &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Helemaal geen tegenstelling&quot; &quot;Niet zo groot&quot; &quot;Groot&quot; &quot;Zeer groot&quot; ... #&gt; $ int058 : num 3 3 2 2 -3 1 2 3 4 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:8] &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Helemaal geen tegenstelling&quot; &quot;Niet zo groot&quot; &quot;Groot&quot; &quot;Zeer groot&quot; ... #&gt; $ int059 : num 2 3 -3 3 3 1 1 2 4 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:8] &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Helemaal geen tegenstelling&quot; &quot;Niet zo groot&quot; &quot;Groot&quot; &quot;Zeer groot&quot; ... #&gt; $ int059a : num 3 2 2 2 3 1 2 2 3 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:8] &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Helemaal geen tegenstelling&quot; &quot;Niet zo groot&quot; &quot;Groot&quot; &quot;Zeer groot&quot; ... #&gt; $ var571 : num 2 3 3 2 2 2 2 3 2 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Dalen&quot; &quot;Gelijk blijven&quot; &quot;Stijgen&quot; &quot;Weigert&quot; ... #&gt; $ var572 : num -5 1 3 -5 -5 -5 -5 3 -5 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Een klein beetje&quot; &quot;Enigszins&quot; &quot;Sterk&quot; &quot;Weigert&quot; ... #&gt; $ var573 : num 1 2 3 2 1 1 1 1 2 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Dalen&quot; &quot;Gelijk blijven&quot; &quot;Stijgen&quot; &quot;Weigert&quot; ... #&gt; $ var574 : num 3 -5 3 -5 2 3 3 2 -5 -5 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Een klein beetje&quot; &quot;Enigszins&quot; &quot;Sterk&quot; &quot;Weigert&quot; ... #&gt; $ var576 : num 5 2 3 4 5 2 5 4 4 5 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:10] &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen mening&quot; &quot;of sterk mee oneens?&quot; &quot;mee oneens,&quot; &quot;noch mee eens, noch mee oneens,&quot; ... #&gt; $ var153 : num 3 1 1 1 1 1 1 1 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen oordeel&quot; &quot;Ontevreden&quot; &quot;Tevreden&quot; &quot;Weigert&quot; ... #&gt; $ var154 : num 3 2 2 2 3 2 3 2 3 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Onvoldoende&quot; &quot;Voldoende&quot; &quot;Te goed (niet noemen)&quot; &quot;Weigert&quot; ... #&gt; $ var155 : num 2 2 2 3 3 3 3 -3 2 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Onvoldoende&quot; &quot;Voldoende&quot; &quot;Te goed (niet noemen)&quot; &quot;Weigert&quot; ... #&gt; $ var156 : num 3 2 2 2 2 -3 3 2 2 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Onvoldoende&quot; &quot;Voldoende&quot; &quot;Te goed (niet noemen)&quot; &quot;Weigert&quot; ... #&gt; $ var157 : num 2 -3 2 2 2 3 3 -3 2 -3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Onvoldoende&quot; &quot;Voldoende&quot; &quot;Te goed (niet noemen)&quot; &quot;Weigert&quot; ... #&gt; $ var157a : num 3 -3 2 2 3 3 3 2 2 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Onvoldoende&quot; &quot;Voldoende&quot; &quot;Te goed (niet noemen)&quot; &quot;Weigert&quot; ... #&gt; $ var154a : num -3 -3 2 2 2 2 2 2 2 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Onvoldoende&quot; &quot;Voldoende&quot; &quot;Te goed (niet noemen)&quot; &quot;Weigert&quot; ... #&gt; $ var164 : num 2 2 2 1 1 1 1 2 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:8] &quot;4&quot; &quot;3&quot; &quot;2&quot; &quot;1&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen mening (niet noemen)&quot; &quot;Te klein&quot; &quot;Ongeveer juist&quot; &quot;Te groot&quot; ... #&gt; $ var165 : num 3 3 3 4 5 5 4 4 5 4 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:10] &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen oordeel (niet noemen)&quot; &quot;Veel kleiner&quot; &quot;Een beetje kleiner&quot; &quot;Blijven zoals nu&quot; ... #&gt; $ var166 : num 3 3 4 3 5 5 4 3 4 4 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:10] &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen oordeel (niet noemen)&quot; &quot;Veel kleiner&quot; &quot;Een beetje kleiner&quot; &quot;Blijven zoals nu&quot; ... #&gt; $ var179 : num 1 1 2 1 1 1 1 1 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:6] &quot;2&quot; &quot;1&quot; &quot;-2&quot; &quot;-3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Nee&quot; &quot;Ja&quot; &quot;Weigert&quot; &quot;Weet niet&quot; ... #&gt; $ var180 : num 1 1 1 2 1 2 2 1 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:6] &quot;2&quot; &quot;1&quot; &quot;-2&quot; &quot;-3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Nee&quot; &quot;Ja&quot; &quot;Weigert&quot; &quot;Weet niet&quot; ... #&gt; $ var184 : num 1 1 2 1 2 1 1 1 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:6] &quot;2&quot; &quot;1&quot; &quot;-2&quot; &quot;-3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Nee&quot; &quot;Ja&quot; &quot;Weigert&quot; &quot;Weet niet&quot; ... #&gt; $ var185 : num 2 1 1 1 2 1 1 1 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:6] &quot;2&quot; &quot;1&quot; &quot;-2&quot; &quot;-3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Nee&quot; &quot;Ja&quot; &quot;Weigert&quot; &quot;Weet niet&quot; ... #&gt; $ var198a : num 2 1 1 1 1 1 2 2 2 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:6] &quot;2&quot; &quot;1&quot; &quot;-2&quot; &quot;-3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Nee&quot; &quot;Ja&quot; &quot;Weigert&quot; &quot;Weet niet&quot; ... #&gt; $ var198 : num -5 3 5 3 1 1 -5 -5 -5 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:11] &quot;7&quot; &quot;6&quot; &quot;5&quot; &quot;4&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;Ander geloof&quot; &quot;Boeddhistisch&quot; &quot;Islamitisch&quot; &quot;Hindoe&quot; ... #&gt; $ var201a : num 2 1 1 2 2 1 2 2 2 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:6] &quot;2&quot; &quot;1&quot; &quot;-2&quot; &quot;-3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Nee&quot; &quot;Ja&quot; &quot;Weigert&quot; &quot;Weet niet&quot; ... #&gt; $ var201b : num -5 2 4 -5 -5 1 -5 -5 -5 -5 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:10] &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Ander kerkgenootschap of levensbeschouwelijke groepering&quot; &quot;Boeddhistisch&quot; &quot;Islamitisch&quot; &quot;Hindoe&quot; ... #&gt; $ var204 : num 5 4 5 4 5 4 5 5 3 5 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;of nooit?&quot; &quot;minder dan eenmaal per maand,&quot; &quot;eens per maand,&quot; &quot;eens per 2 weken,&quot; ... #&gt; $ int257 : num 7 3 3 4 6 2 3 5 3 7 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:11] &quot;7&quot; &quot;6&quot; &quot;5&quot; &quot;4&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;Buitengewoon ongelovig&quot; &quot;Erg ongelovig&quot; &quot;Enigszins ongelovig&quot; &quot;Noch gelovig, noch ongelovig&quot; ... #&gt; $ var211 : num 3 2 3 3 3 1 3 2 2 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Nee&quot; &quot;Gedeeltelijk&quot; &quot;Ja&quot; &quot;Weigert&quot; ... #&gt; $ var223 : num 1 2 1 3 1 1 1 3 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Hangt ervan af&quot; &quot;Moeten niet los van elkaar staan&quot; &quot;Moeten los van elkaar staan&quot; &quot;Weigert&quot; ... #&gt; $ var1320 : num 1 2 2 2 2 2 2 3 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;of sterk mee oneens?&quot; &quot;enigszins mee oneens,&quot; &quot;niet mee eens, niet mee oneens,&quot; &quot;enigszins mee eens,&quot; ... #&gt; $ var1321 : num 1 4 2 2 4 2 4 4 2 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;of sterk mee oneens?&quot; &quot;enigszins mee oneens,&quot; &quot;niet mee eens, niet mee oneens,&quot; &quot;enigszins mee eens,&quot; ... #&gt; $ var1322 : num 3 3 2 2 3 1 1 2 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;of helemaal niet voor u?&quot; &quot;gedeeltelijk voor u,&quot; &quot;helemaal voor u,&quot; &quot;Weigert&quot; ... #&gt; $ var1323 : num 3 3 2 3 3 2 3 3 3 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;of zelden of nooit?&quot; &quot;soms,&quot; &quot;vaak,&quot; &quot;Weigert&quot; ... #&gt; $ var1324 : num 3 3 2 3 3 3 3 3 3 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Zelden of nooit?&quot; &quot;Soms,&quot; &quot;Vaak&quot; &quot;Weigert&quot; ... #&gt; $ var1325 : num 3 3 3 3 3 2 3 3 3 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;of zelden of nooit?&quot; &quot;soms,&quot; &quot;vaak,&quot; &quot;Weigert&quot; ... #&gt; $ var1326 : num 3 3 3 3 3 3 3 3 3 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Zelden of nooit?&quot; &quot;Soms,&quot; &quot;Vaak&quot; &quot;Weigert&quot; ... #&gt; $ var1327 : num 3 3 2 3 3 2 3 3 2 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;of zelden of nooit?&quot; &quot;soms,&quot; &quot;vaak,&quot; &quot;Weigert&quot; ... #&gt; $ var1328 : num 3 3 3 3 3 3 3 3 3 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:7] &quot;3&quot; &quot;2&quot; &quot;1&quot; &quot;-2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Zelden of nooit?&quot; &quot;Soms,&quot; &quot;Vaak&quot; &quot;Weigert&quot; ... #&gt; $ var229 : num 3 3 3 8 3 3 3 2 3 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:12] &quot;8&quot; &quot;7&quot; &quot;6&quot; &quot;5&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:12] &quot;Veel vrienden en kennissen&quot; &quot;Prettig werk&quot; &quot;Een sterk geloof&quot; &quot;Een goed huwelijksleven&quot; ... #&gt; $ int218 : num 2 2 3 3 2 4 3 3 4 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Helemaal geen vertrouwen&quot; &quot;Zeer weinig vertrouwen&quot; &quot;Enig vertrouwen&quot; &quot;Veel vertrouwen&quot; ... #&gt; $ int219 : num 2 2 3 3 3 2 2 2 3 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Helemaal geen vertrouwen&quot; &quot;Zeer weinig vertrouwen&quot; &quot;Enig vertrouwen&quot; &quot;Veel vertrouwen&quot; ... #&gt; $ int221 : num 5 2 5 3 3 2 3 3 2 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Helemaal geen vertrouwen&quot; &quot;Zeer weinig vertrouwen&quot; &quot;Enig vertrouwen&quot; &quot;Veel vertrouwen&quot; ... #&gt; $ int222 : num 2 2 3 2 4 3 3 2 4 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Helemaal geen vertrouwen&quot; &quot;Zeer weinig vertrouwen&quot; &quot;Enig vertrouwen&quot; &quot;Veel vertrouwen&quot; ... #&gt; $ int223 : num 3 3 1 2 3 2 3 2 1 4 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Helemaal geen vertrouwen&quot; &quot;Zeer weinig vertrouwen&quot; &quot;Enig vertrouwen&quot; &quot;Veel vertrouwen&quot; ... #&gt; $ int710 : num 4 2 1 3 3 2 3 3 2 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Helemaal geen vertrouwen&quot; &quot;Zeer weinig vertrouwen&quot; &quot;Enig vertrouwen&quot; &quot;Veel vertrouwen&quot; ... #&gt; $ int711 : num 4 2 3 2 3 5 3 2 1 4 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Helemaal geen vertrouwen&quot; &quot;Zeer weinig vertrouwen&quot; &quot;Enig vertrouwen&quot; &quot;Veel vertrouwen&quot; ... #&gt; $ int712 : num 2 3 3 3 3 2 4 2 4 4 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Helemaal geen vertrouwen&quot; &quot;Zeer weinig vertrouwen&quot; &quot;Enig vertrouwen&quot; &quot;Veel vertrouwen&quot; ... #&gt; $ int713 : num 3 3 -3 3 2 4 4 2 4 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Helemaal geen vertrouwen&quot; &quot;Zeer weinig vertrouwen&quot; &quot;Enig vertrouwen&quot; &quot;Veel vertrouwen&quot; ... #&gt; $ int714 : num 3 2 -3 3 3 3 4 3 4 4 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Helemaal geen vertrouwen&quot; &quot;Zeer weinig vertrouwen&quot; &quot;Enig vertrouwen&quot; &quot;Veel vertrouwen&quot; ... #&gt; $ int715 : num 3 2 5 2 2 3 5 3 3 4 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Helemaal geen vertrouwen&quot; &quot;Zeer weinig vertrouwen&quot; &quot;Enig vertrouwen&quot; &quot;Veel vertrouwen&quot; ... #&gt; $ int716 : num 5 3 3 3 2 2 3 3 3 3 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:9] &quot;5&quot; &quot;4&quot; &quot;3&quot; &quot;2&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Helemaal geen vertrouwen&quot; &quot;Zeer weinig vertrouwen&quot; &quot;Enig vertrouwen&quot; &quot;Veel vertrouwen&quot; ... #&gt; $ var433 : num 2 2 3 3 3 1 5 2 3 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:10] &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen mening&quot; &quot;Helemaal niet mee eens&quot; &quot;Eigenlijk niet mee eens&quot; &quot;Noch mee eens, noch mee oneens&quot; ... #&gt; $ var439 : num 2 1 5 4 -3 1 1 4 1 5 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:10] &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen mening&quot; &quot;Helemaal niet mee eens&quot; &quot;Eigenlijk niet mee eens&quot; &quot;Noch mee eens, noch mee oneens&quot; ... #&gt; $ var1329 : num 2 4 2 4 2 1 2 1 1 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:10] &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen mening&quot; &quot;Helemaal niet mee eens&quot; &quot;Eigenlijk niet mee eens&quot; &quot;Noch mee eens, noch mee oneens&quot; ... #&gt; $ var1330 : num 5 5 4 3 5 1 1 4 1 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:10] &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen mening&quot; &quot;Helemaal niet mee eens&quot; &quot;Eigenlijk niet mee eens&quot; &quot;Noch mee eens, noch mee oneens&quot; ... #&gt; $ var445 : num 4 5 2 2 4 4 5 2 3 2 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:10] &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen mening&quot; &quot;Helemaal niet mee eens&quot; &quot;Eigenlijk niet mee eens&quot; &quot;Noch mee eens, noch mee oneens&quot; ... #&gt; $ var446 : num 3 5 1 4 2 1 2 3 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:10] &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen mening&quot; &quot;Helemaal niet mee eens&quot; &quot;Eigenlijk niet mee eens&quot; &quot;Noch mee eens, noch mee oneens&quot; ... #&gt; $ var447 : num 3 5 1 4 2 1 4 3 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:10] &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen mening&quot; &quot;Helemaal niet mee eens&quot; &quot;Eigenlijk niet mee eens&quot; &quot;Noch mee eens, noch mee oneens&quot; ... #&gt; $ var451 : num 5 5 4 4 4 1 4 5 5 4 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:10] &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen mening&quot; &quot;Helemaal niet mee eens&quot; &quot;Eigenlijk niet mee eens&quot; &quot;Noch mee eens, noch mee oneens&quot; ... #&gt; $ var452 : num 2 3 2 2 2 1 4 3 1 1 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:10] &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen mening&quot; &quot;Helemaal niet mee eens&quot; &quot;Eigenlijk niet mee eens&quot; &quot;Noch mee eens, noch mee oneens&quot; ... #&gt; $ var1316 : num 5 5 4 4 5 1 2 2 1 5 ... #&gt; ..- attr(*, &quot;value.labels&quot;)= Named chr [1:10] &quot;6&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen mening&quot; &quot;Helemaal niet mee eens&quot; &quot;Eigenlijk niet mee eens&quot; &quot;Noch mee eens, noch mee oneens&quot; ... #&gt; [list output truncated] #&gt; - attr(*, &quot;variable.labels&quot;)= Named chr [1:278] &quot;WE_ID&quot; &quot;veilignummer&quot; &quot;Leeftijd op 1-jan-2009&quot; &quot;Geslacht hhpersoon (GBA)&quot; ... #&gt; ..- attr(*, &quot;names&quot;)= chr [1:278] &quot;we_id&quot; &quot;veilignr&quot; &quot;lft1&quot; &quot;geslacht&quot; ... #&gt; - attr(*, &quot;codepage&quot;)= int 1252 str(cv08_haven) #&gt; tibble [1,963 x 278] (S3: tbl_df/tbl/data.frame) #&gt; $ we_id : dbl+lbl [1:1963] 36775330, 36775340, 36775420, 36775440, 36775450, 36775460, 36775480, 367... #&gt; ..@ label : chr &quot;WE_ID&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:2] 1e+10 1e+10 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Refusal&quot; &quot;Don&#39;t Know&quot; #&gt; $ veilignr: num [1:1963] 8.11e+08 8.11e+08 8.11e+08 8.11e+08 8.11e+08 ... #&gt; ..- attr(*, &quot;label&quot;)= chr &quot;veilignummer&quot; #&gt; ..- attr(*, &quot;format.spss&quot;)= chr &quot;F10.0&quot; #&gt; ..- attr(*, &quot;display_width&quot;)= int 12 #&gt; $ lft1 : dbl+lbl [1:1963] 51, 39, 16, 30, 57, 49, 62, 34, 59, 41, 25, 43, 74, 17, 23, 32, 51, 66, 6... #&gt; ..@ label : chr &quot;Leeftijd op 1-jan-2009&quot; #&gt; ..@ format.spss : chr &quot;F8.0&quot; #&gt; ..@ display_width: int 10 #&gt; ..@ labels : Named num 99 #&gt; .. ..- attr(*, &quot;names&quot;)= chr &quot;Onbekend&quot; #&gt; $ geslacht: chr+lbl [1:1963] M, V, V, M, M, V, M, V, M, M, M, V, V, M, M, V, M, M, V, M, V, M, V, M, V... #&gt; ..@ label : chr &quot;Geslacht hhpersoon (GBA)&quot; #&gt; ..@ format.spss : chr &quot;A1&quot; #&gt; ..@ display_width: int 10 #&gt; ..@ labels : Named chr [1:3] &quot;9&quot; &quot;M&quot; &quot;V&quot; #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:3] &quot;Onbekend&quot; &quot;Man&quot; &quot;Vrouw&quot; #&gt; $ allochtn: dbl+lbl [1:1963] 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0... #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:4] 0 1 2 9 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;geen allochtoon&quot; &quot;allochtoon&quot; &quot;onbekend&quot; &quot;Onbekend&quot; #&gt; $ lft01 : dbl+lbl [1:1963] 50, 38, 15, 29, 56, 48, 61, 33, 58, 40, 24, 42, 73, 16, 22, 31, 50, 65, 6... #&gt; ..@ label : chr &quot;Leeftijd OP op 1 jan. v.h. onderzoekjaar&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:5] 0 1 2 99 125 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;&lt; één jaar&quot; &quot;één jaar&quot; &quot;twee jaar&quot; &quot;Onbekend&quot; ... #&gt; $ lftop : dbl+lbl [1:1963] 51, 39, 16, 29, 57, 49, 62, 34, 59, 41, 25, 43, 74, 17, 22, 32, 51, 66, 6... #&gt; ..@ label : chr &quot;Leeftijd OP op datum interview&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:5] 0 1 2 99 125 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;&lt; één jaar&quot; &quot;één jaar&quot; &quot;twee jaar&quot; &quot;Onbekend&quot; ... #&gt; $ gewicht : num [1:1963] 8423 6244 13434 8997 8423 ... #&gt; ..- attr(*, &quot;label&quot;)= chr &quot;Persoonsgewicht eindres30&quot; #&gt; ..- attr(*, &quot;format.spss&quot;)= chr &quot;F8.2&quot; #&gt; ..- attr(*, &quot;display_width&quot;)= int 10 #&gt; $ var006n : dbl+lbl [1:1963] 6, 8, 3, 8, 6, 2, 2, 5, 5, 1, 1, 3, 2, 1, 1, 5, 6, 3, 1, 5, 6, 2, 5, 6, 1... #&gt; ..@ label : chr &quot;Voltooid opleidingsniveau (uitgebreid) OP, 12-14 jarigen niet standaard op bas.ondw.&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:11] -3 -1 1 2 3 4 5 6 7 8 ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;onbekend&quot; &quot;OP &lt; 12 jr of volgt actueel bas.ondw.&quot; &quot;basisonderwijs&quot; &quot;vmbo&quot; ... #&gt; $ v040 : dbl+lbl [1:1963] 2, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 1, 2... #&gt; ..@ label : chr &quot;Betaald werk?&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:6] -6 -5 -3 -2 1 2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var723 : dbl+lbl [1:1963] -5, 45, -5, 20, -5, -5, 40, 32, 30, 20, 38, 30, -5, -5, 18, 20, 40, -5, -... #&gt; ..@ label : chr &quot;Uren werk per week&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:4] -6 -5 -3 -2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; #&gt; $ var723a : dbl+lbl [1:1963] -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, -... #&gt; ..@ label : chr &quot;Categorie: uren werk per week&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ v202n : dbl+lbl [1:1963] 4, 1, 7, 1, 3, 2, 1, 1, 1, 1, 1, 1, 5, 7, 1, 1, 1, 5, 2, 1, 2, 1, 5, 1, 5... #&gt; ..@ label : chr &quot;positie werkkring (nieuw)&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] 1 2 3 4 5 6 7 8 10 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;werkt &gt;12 uur&quot; &quot;eigen huishouden&quot; &quot;werkloos&quot; &quot;arbeidsongeschikt&quot; ... #&gt; $ var1061a: dbl+lbl [1:1963] 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2... #&gt; ..@ label : chr &quot;(16) Verricht u vrijwilligerwerk&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:6] -6 -5 -3 -2 1 2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var1061b: dbl+lbl [1:1963] 23, -5, -5, -5, -5, -5, -5, -5, -5, -5, -5, 3, 5, -5, -5, 1, -5, -5, -... #&gt; ..@ label : chr &quot;(16) Hoeveel uur per week vrijwilligerwerk?&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:4] -6 -5 -3 -2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; #&gt; $ var1062a: dbl+lbl [1:1963] 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2... #&gt; ..@ label : chr &quot;(17) Kosteloos hulp aan zieke of gehandicapte familieleden, kennissen of buren?&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:6] -6 -5 -3 -2 1 2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var1062b: dbl+lbl [1:1963] -5, -5, -5, -5, -5, -5, -5, -5, 3, -5, -5, 2, 5, -5, -5, 1, -5, -5, -... #&gt; ..@ label : chr &quot;(17) Hoeveel uur per week kosteloos hulp?&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:4] -6 -5 -3 -2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; #&gt; $ int137n : dbl+lbl [1:1963] -5, 3, -5, 1, -5, 1, 2, 3, 3, 2, 1, 3, -5, -5, 1, 2, 2, 1, -... #&gt; ..@ label : chr &quot;S003 Gewenste tijd betaald werk&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int138n : dbl+lbl [1:1963] 3, 3, 3, 3, 2, 3, 3, 2, 3, 3, 3, 2, 3, -5, 3, 2, 1, 3, ... #&gt; ..@ label : chr &quot;S003 Gewenste tijd huishoudelijk werk&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int139n : dbl+lbl [1:1963] -5, 3, -5, -5, -5, 3, 1, 1, 3, -5, 3, 1, 3, -5, -5, 1, 1, 1, ... #&gt; ..@ label : chr &quot;S003 Gewenste tijd gezin&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int140n : dbl+lbl [1:1963] -5, 3, 3, 3, 1, 3, 1, 1, 3, 3, 3, 1, 3, -5, 1, 1, 3, 3, ... #&gt; ..@ label : chr &quot;S003 Gewenste tijd vrienden&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int141n : dbl+lbl [1:1963] 1, 3, 1, 3, 1, 3, 1, 1, 3, 1, 3, 3, 3, 3, 3, 1, 1, -5, ... #&gt; ..@ label : chr &quot;S003 Gewenste tijd vrijetijds-activiteit&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ v401 : dbl+lbl [1:1963] 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 3, 3, 1, 2, 3, 1, 2, 1... #&gt; ..@ label : chr &quot;Niveau gezondheid&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var1343 : dbl+lbl [1:1963] 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3... #&gt; ..@ label : chr &quot;(199) Ik voel me van andere mensen gesoleerd.&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var648 : dbl+lbl [1:1963] 3, 1, 3, 4, 3, 3, 3, 4, 3, 3, 1, 2, 3, 3, 3, 2, 3, 5, 3, 3, 3, 3, 2, 1, 3... #&gt; ..@ label : chr &quot;Tevredenheid leven&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var149 : dbl+lbl [1:1963] 2, 1, -3, 3, 2, 2, 2, 2, 1, 2, 1, 2, 2, -3, 2, 1, 2, 3, ... #&gt; ..@ label : chr &quot;Tevredenheid inkomen&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var058 : dbl+lbl [1:1963] 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, ... #&gt; ..@ label : chr &quot;Welvarendheid Nederland&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:6] -6 -5 -3 -2 1 2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var059 : dbl+lbl [1:1963] 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1... #&gt; ..@ label : chr &quot;Welvarendheid in eigen huishouden&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:6] -6 -5 -3 -2 1 2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var064 : dbl+lbl [1:1963] 1, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, ... #&gt; ..@ label : chr &quot;Inzet regering vergroten uw welvaart&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:6] -6 -5 -3 -2 1 2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var365 : dbl+lbl [1:1963] 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, ... #&gt; ..@ label : chr &quot;financieel een onbekommerde oude dag&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:6] -6 -5 -3 -2 1 2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var065 : dbl+lbl [1:1963] 2, 1, 1, 2, -3, 1, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, ... #&gt; ..@ label : chr &quot;Verwachting crisis met veel werklozen&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:6] -6 -5 -3 -2 1 2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var092 : dbl+lbl [1:1963] 2, -3, 2, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 4, 4, 4, 4, 4, ... #&gt; ..@ label : chr &quot;Ontwikkeling opvattingen gedrag en zeden&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var096 : dbl+lbl [1:1963] 3, 3, 2, 3, 3, 1, 2, 2, 1, 1, 2, 4, 3, 1, 2, 3, 2, 2, 1, 1, 2, 1, 3, 2, 1... #&gt; ..@ label : chr &quot;Niveau geld voor openbare voorzieningen&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:10] -6 -5 -3 -2 1 2 3 4 5 6 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int054 : dbl+lbl [1:1963] 4, -3, 3, 3, 1, 1, 2, 3, 2, 1, 1, 1, 2, 2, 2, 2, 3, 1, ... #&gt; ..@ label : chr &quot;Niveau tegenstelling arm en rijk&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int055 : dbl+lbl [1:1963] 3, 3, 3, 2, -3, 1, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, ... #&gt; ..@ label : chr &quot;Tegenstelling arbeidersklasse en middenklasse&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int056 : dbl+lbl [1:1963] 2, 2, 3, 3, 2, 1, 1, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 1, ... #&gt; ..@ label : chr &quot;Tegenstelling werklozen en werkenden&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int057 : dbl+lbl [1:1963] 2, 3, 2, 2, 1, 2, 3, 3, 2, 1, 1, 1, 2, 2, 2, 3, 2, 1, ... #&gt; ..@ label : chr &quot;Tegenstelling werkgevers en werknemers&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int058 : dbl+lbl [1:1963] 3, 3, 2, 2, -3, 1, 2, 3, 4, 2, -3, 1, 2, 3, 2, -3, 3, -3, -... #&gt; ..@ label : chr &quot;Tegenstelling platteland en stadsmensen&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int059 : dbl+lbl [1:1963] 2, 3, -3, 3, 3, 1, 1, 2, 4, 2, 1, 1, 2, 2, 2, 2, 2, 2, ... #&gt; ..@ label : chr &quot;Tegenstelling jongeren en ouderen&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int059a : dbl+lbl [1:1963] 3, 2, 2, 2, 3, 1, 2, 2, 3, 2, -3, 1, 2, 2, 2, 2, 2, -3, ... #&gt; ..@ label : chr &quot;Tegenstelling allochtonen en autochtonen&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var571 : dbl+lbl [1:1963] 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 3, 2, 3, 3, 3, 3, 1, 3, ... #&gt; ..@ label : chr &quot;Verwachting toekomst sociale uitkeringen&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var572 : dbl+lbl [1:1963] -5, 1, 3, -5, -5, -5, -5, 3, -5, 1, 2, -5, 1, 2, 3, 1, 2, 2, -... #&gt; ..@ label : chr &quot;Verwachting niveau sociale uitkeringen&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var573 : dbl+lbl [1:1963] 1, 2, 3, 2, 1, 1, 1, 1, 2, 2, 2, 1, 3, 3, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1... #&gt; ..@ label : chr &quot;Niveau uitkeringen in huidige economie 1&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var574 : dbl+lbl [1:1963] 3, -5, 3, -5, 2, 3, 3, 2, -5, -5, -5, 1, 3, 2, -5, -5, -5, 3, ... #&gt; ..@ label : chr &quot;Niveau uitkeringen in huidige economie 2&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var576 : dbl+lbl [1:1963] 5, 2, 3, 4, 5, 2, 5, 4, 4, 5, 4, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 2, 2, 2, 4... #&gt; ..@ label : chr &quot;Mening toekomst minder sociale zekerheid&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:10] -6 -5 -3 -2 1 2 3 4 5 6 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var153 : dbl+lbl [1:1963] 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2, 1, 3, 1, 1, 1, 2... #&gt; ..@ label : chr &quot;Tevredenheid sociale voorzieningen&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var154 : dbl+lbl [1:1963] 3, 2, 2, 2, 3, 2, 3, 2, 3, 3, -3, 3, 2, 2, -3, 2, 3, 3, ... #&gt; ..@ label : chr &quot;Niveau Algemene Ouderdomswet (AOW)&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var155 : dbl+lbl [1:1963] 2, 2, 2, 3, 3, 3, 3, -3, 2, 2, -3, 3, 3, 3, 2, 3, 2, 3, -... #&gt; ..@ label : chr &quot;Oordeel Wet Werk en Bijstand&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var156 : dbl+lbl [1:1963] 3, 2, 2, 2, 2, -3, 3, 2, 2, 2, -3, 1, 3, 2, -3, 2, 2, 3, ... #&gt; ..@ label : chr &quot;Oordeel Werkloosheidswet (WW).&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var157 : dbl+lbl [1:1963] 2, -3, 2, 2, 2, 3, 3, -3, 2, -3, -3, 3, 3, 3, -3, -3, 3, 3, ... #&gt; ..@ label : chr &quot;Oordeel Algemene Nabestaandenwet (ANW).&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var157a : dbl+lbl [1:1963] 3, -3, 2, 2, 3, 3, 3, 2, 2, 2, 3, 1, 3, 3, -3, 3, 2, 3, ... #&gt; ..@ label : chr &quot;Oordeel Arbeidsongeschiktheidswet.&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var154a : dbl+lbl [1:1963] -3, -3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 3, 2, -3, 2, 2, -3, -... #&gt; ..@ label : chr &quot;Oordeel Ziektewet.&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var164 : dbl+lbl [1:1963] 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1... #&gt; ..@ label : chr &quot;Mate verschil tussen inkomens in NL&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var165 : dbl+lbl [1:1963] 3, 3, 3, 4, 5, 5, 4, 4, 5, 4, 4, 5, 5, 1, 4, 2, 4, 5, 1, 4, 5, 4, 4, 3, 5... #&gt; ..@ label : chr &quot;Wens vergroten verschil inkomens&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:10] -6 -5 -3 -2 1 2 3 4 5 6 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var166 : dbl+lbl [1:1963] 3, 3, 4, 3, 5, 5, 4, 3, 4, 4, 3, 4, 5, 3, 3, 3, 4, 3, ... #&gt; ..@ label : chr &quot;Wens vergroten verschil bezit&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:10] -6 -5 -3 -2 1 2 3 4 5 6 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var179 : dbl+lbl [1:1963] 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... #&gt; ..@ label : chr &quot;M050 Vrij:om te demonstreren&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:6] -6 -5 -3 -2 1 2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var180 : dbl+lbl [1:1963] 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2... #&gt; ..@ label : chr &quot;M051 Vrij:openlijk kritiek koningshuis&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:6] -6 -5 -3 -2 1 2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var184 : dbl+lbl [1:1963] 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 2, 2, 2, 1, 1... #&gt; ..@ label : chr &quot;M054 Vrij:openb schrijven wat men wil&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:6] -6 -5 -3 -2 1 2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var185 : dbl+lbl [1:1963] 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1... #&gt; ..@ label : chr &quot;M054 Vrij:openb zeggen wat men wil&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:6] -6 -5 -3 -2 1 2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var198a : dbl+lbl [1:1963] 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 2, 1... #&gt; ..@ label : chr &quot;Opgevoed met bepaald geloof&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:6] -6 -5 -3 -2 1 2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var198 : dbl+lbl [1:1963] -5, 3, 5, 3, 1, 1, -5, -5, -5, 3, 1, -5, 3, 2, -5, 2, 1, 2, ... #&gt; ..@ label : chr &quot;Geloof opgevoed&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:11] -6 -5 -3 -2 1 2 3 4 5 6 ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var201a : dbl+lbl [1:1963] 2, 1, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2... #&gt; ..@ label : chr &quot;Rekent zich tot kerkgenootschap&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:6] -6 -5 -3 -2 1 2 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:6] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var201b : dbl+lbl [1:1963] -5, 2, 4, -5, -5, 1, -5, -5, -5, -5, 1, -5, 2, 2, -5, 6, -5, -5, -... #&gt; ..@ label : chr &quot;Welk kerkgenootschap is dat?&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:10] -6 -5 -3 -2 1 2 3 4 5 6 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var204 : dbl+lbl [1:1963] 5, 4, 5, 4, 5, 4, 5, 5, 3, 5, 4, 5, 1, 1, 5, 1, 5, 5, 5, 5, 1, 5, 5, 5, 5... #&gt; ..@ label : chr &quot;Aantal bezoeken kerk afgelopen half jaar&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int257 : dbl+lbl [1:1963] 7, 3, 3, 4, 6, 2, 3, 5, 3, 7, 4, 7, 2, 2, 3, 2, 3, 2, 4, 7, 2, 4, 6, 7, 4... #&gt; ..@ label : chr &quot;Mate gelovigheid&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:11] -6 -5 -3 -2 1 2 3 4 5 6 ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var211 : dbl+lbl [1:1963] 3, 2, 3, 3, 3, 1, 3, 2, 2, 3, -3, 3, 1, 1, 3, 1, 1, 3, ... #&gt; ..@ label : chr &quot;M069 Ziet bijbel als het woord van God&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var223 : dbl+lbl [1:1963] 1, 2, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1... #&gt; ..@ label : chr &quot;M068 Politiek los van godsdienst&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var1320 : dbl+lbl [1:1963] 1, 2, 2, 2, 2, 2, 2, 3, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, ... #&gt; ..@ label : chr &quot;(54) Zin van leven innerlijke ervaring en ontwikkeling eigen vermogens.&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var1321 : dbl+lbl [1:1963] 1, 4, 2, 2, 4, 2, 4, 4, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 4, 2, 5, 2, 2, 4, 1... #&gt; ..@ label : chr &quot;(55) Bij beslissingen afgaan op intutie en gevoel.&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var1322 : dbl+lbl [1:1963] 3, 3, 2, 2, 3, 1, 1, 2, 1, 1, 2, 3, 2, 2, 3, 2, 3, 1, ... #&gt; ..@ label : chr &quot;(56) Religie zoek ik zelf bijeen&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var1323 : dbl+lbl [1:1963] 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3... #&gt; ..@ label : chr &quot;(57) Praten of mailen over spiritualiteit&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var1324 : dbl+lbl [1:1963] 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3... #&gt; ..@ label : chr &quot;(58) Meedoen gespreksgroep over spiritueel onderwerp?&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var1325 : dbl+lbl [1:1963] 3, 3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3... #&gt; ..@ label : chr &quot;(59) Op internet kijken naar informatie over spiritualiteit&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var1326 : dbl+lbl [1:1963] 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3... #&gt; ..@ label : chr &quot;(60) Bezoek beurs etc over spirituele onderwerpen?&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var1327 : dbl+lbl [1:1963] 3, 3, 2, 3, 3, 2, 3, 3, 2, 3, 3, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3... #&gt; ..@ label : chr &quot;(61) Tijdschriften of boeken lezen over spirituele onderwerpen&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var1328 : dbl+lbl [1:1963] 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3... #&gt; ..@ label : chr &quot;(62) Deelnemen aan cursus, gericht op spiritualiteit?&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:7] -6 -5 -3 -2 1 2 3 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var229 : dbl+lbl [1:1963] 3, 3, 3, 8, 3, 3, 3, 2, 3, 3, 3, 5, 3, 2, 7, 6, 2, 3, 3, 3, 6, 3, 3, 2, 8... #&gt; ..@ label : chr &quot;M064 Allerbelangrijkste in het leven&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:12] -6 -5 -3 -2 1 2 3 4 5 6 ... #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:12] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int218 : dbl+lbl [1:1963] 2, 2, 3, 3, 2, 4, 3, 3, 4, 3, 3, 2, 2, 3, 4, 2, 4, 4, 5, 4, 3, 3, 3, 4, 2... #&gt; ..@ label : chr &quot;Mate vertrouwen in regering&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int219 : dbl+lbl [1:1963] 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 3, 2, 3, 4, -... #&gt; ..@ label : chr &quot;Mate vertrouwen in bedrijfsleven&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int221 : dbl+lbl [1:1963] 5, 2, 5, 3, 3, 2, 3, 3, 2, 3, 3, 5, 2, 2, 4, 3, 3, -3, -... #&gt; ..@ label : chr &quot;Mate vertrouwen kerken/reli organisaties&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int222 : dbl+lbl [1:1963] 2, 2, 3, 2, 4, 3, 3, 2, 4, 3, 2, 4, 2, 2, 4, 3, 2, 4, ... #&gt; ..@ label : chr &quot;Mate vertrouwen in rechtspraak&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int223 : dbl+lbl [1:1963] 3, 3, 1, 2, 3, 2, 3, 2, 1, 4, 2, 1, 2, 2, 3, 3, 3, 3, -... #&gt; ..@ label : chr &quot;Mate vertrouwen in onderwijs&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int710 : dbl+lbl [1:1963] 4, 2, 1, 3, 3, 2, 3, 3, 2, 3, 2, 1, 2, 2, 3, 3, 2, 3, 5, 2, 3, 2, 3, 2, 2... #&gt; ..@ label : chr &quot;Mate vertrouwen in gezondheidszorg&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int711 : dbl+lbl [1:1963] 4, 2, 3, 2, 3, 5, 3, 2, 1, 4, 3, 5, 2, 2, 3, 4, 3, 4, ... #&gt; ..@ label : chr &quot;Mate vertrouwen in kranten&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int712 : dbl+lbl [1:1963] 2, 3, 3, 3, 3, 2, 4, 2, 4, 4, 3, 2, 2, 2, 3, 3, 3, 4, 5, 3, 3, 2, 3, 2, 2... #&gt; ..@ label : chr &quot;Mate vertrouwen in politie&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int713 : dbl+lbl [1:1963] 3, 3, -3, 3, 2, 4, 4, 2, 4, 3, 3, 3, 2, 2, 4, 3, 3, 4, ... #&gt; ..@ label : chr &quot;Mate vertrouwen in Tweede Kamer&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int714 : dbl+lbl [1:1963] 3, 2, -3, 3, 3, 3, 4, 3, 4, 4, 3, 5, 2, 2, 3, 2, 3, 5, ... #&gt; ..@ label : chr &quot;Mate vertrouwen in ambtenaren&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int715 : dbl+lbl [1:1963] 3, 2, 5, 2, 2, 3, 5, 3, 3, 4, -3, 2, 2, 2, 3, 3, 2, -3, ... #&gt; ..@ label : chr &quot;Mate vertrouwen in Europese Unie&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ int716 : dbl+lbl [1:1963] 5, 3, 3, 3, 2, 2, 3, 3, 3, 3, 3, 2, 2, 3, 2, 4, 3, 3, ... #&gt; ..@ label : chr &quot;Mate vertrouwen in vakbonden&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:9] -6 -5 -3 -2 1 2 3 4 5 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var433 : dbl+lbl [1:1963] 2, 2, 3, 3, 3, 1, 5, 2, 3, 2, 3, 5, 2, 4, 2, 2, 3, 5, 5, 5, 4, 3, 3, 5, 1... #&gt; ..@ label : chr &quot;Misdadiger niet straffen maar veranderen&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:10] -6 -5 -3 -2 1 2 3 4 5 6 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var439 : dbl+lbl [1:1963] 2, 1, 5, 4, -3, 1, 1, 4, 1, 5, 3, 1, 2, 4, 3, 4, 2, 1, ... #&gt; ..@ label : chr &quot;Minder regels, meer sterke leiders&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:10] -6 -5 -3 -2 1 2 3 4 5 6 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var1329 : dbl+lbl [1:1963] 2, 4, 2, 4, 2, 1, 2, 1, 1, 2, 3, 1, 2, 4, 2, 2, 1, 1, 1, 3, 2, 2, 1, 5, 2... #&gt; ..@ label : chr &quot;De vrijheid van meningsuiting mag niet zover gaan dat mensen worden gekwetst in hun religieuze gevoelens.&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:10] -6 -5 -3 -2 1 2 3 4 5 6 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var1330 : dbl+lbl [1:1963] 5, 5, 4, 3, 5, 1, 1, 4, 1, 2, 3, 5, 2, 4, 3, 3, 2, -3, ... #&gt; ..@ label : chr &quot;(79) Schaam me een Nederlander te zijn.&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:10] -6 -5 -3 -2 1 2 3 4 5 6 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var445 : dbl+lbl [1:1963] 4, 5, 2, 2, 4, 4, 5, 2, 3, 2, 5, 5, 3, 4, 2, 2, 3, 2, 5, 5, 4, 3, 4, 5, 5... #&gt; ..@ label : chr &quot;Seks misdaad niet straffen maar genezen&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:10] -6 -5 -3 -2 1 2 3 4 5 6 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var446 : dbl+lbl [1:1963] 3, 5, 1, 4, 2, 1, 2, 3, 1, 1, 2, 1, 3, 4, 2, 5, 4, 1, 1, 3, 4, 2, 1, 5, 3... #&gt; ..@ label : chr &quot;Onzekerheid wat goed/verkeerd&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:10] -6 -5 -3 -2 1 2 3 4 5 6 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var447 : dbl+lbl [1:1963] 3, 5, 1, 4, 2, 1, 4, 3, 1, 1, 3, 1, 2, 4, 4, 5, 4, 1, 1, 3, 4, 2, 2, 5, 2... #&gt; ..@ label : chr &quot;Steeds anders onduidelijk goed/slecht&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:10] -6 -5 -3 -2 1 2 3 4 5 6 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var451 : dbl+lbl [1:1963] 5, 5, 4, 4, 4, 1, 4, 5, 5, 4, 3, 5, 2, 4, 4, 3, 5, 2, ... #&gt; ..@ label : chr &quot;Geweld om ideaal te verwezenlijken&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:10] -6 -5 -3 -2 1 2 3 4 5 6 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var452 : dbl+lbl [1:1963] 2, 3, 2, 2, 2, 1, 4, 3, 1, 1, 5, 3, 2, 2, 2, 2, 5, 1, 4, 3, 2, 3, 2, 5, 2... #&gt; ..@ label : chr &quot;Betekenis leven dmv ideaal of taak&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:10] -6 -5 -3 -2 1 2 3 4 5 6 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; $ var1316 : dbl+lbl [1:1963] 5, 5, 4, 4, 5, 1, 2, 2, 1, 5, 2, 4, 3, 2, 3, 4, 5, -3, ... #&gt; ..@ label : chr &quot;Geld overheid om sportevenementen binnen te halen&quot; #&gt; ..@ format.spss : chr &quot;F10.0&quot; #&gt; ..@ display_width: int 12 #&gt; ..@ labels : Named num [1:10] -6 -5 -3 -2 1 2 3 4 5 6 #&gt; .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; [list output truncated] The function str() asks for the structure of an object in your environment. You will see that the original data is stored differently in the three datasets. The different objects (cv08, cv08_nolab and cv08_haven) have a different structure. cv08 and cv08_nolab are data.frame objects, the haven::read_spss function produces a tibble. Let us have quick look at the structure of some variables. The take home message is that information is stored differently and that different information is stored,depending on the package used to read the data. To access a variable in a dataset use datasetname$variablename. Add to your cheat sheet under operators/symbols: $ str(cv08$lftop) #a factor #&gt; Factor w/ 81 levels &quot;&lt; één jaar&quot;,&quot;één jaar&quot;,..: 40 28 4 18 46 38 51 23 48 30 ... str(cv08_nolab$lftop) # a numeric variable #&gt; num [1:1963] 51 39 16 29 57 49 62 34 59 41 ... #&gt; - attr(*, &quot;value.labels&quot;)= Named chr [1:5] &quot;125&quot; &quot;99&quot; &quot;2&quot; &quot;1&quot; ... #&gt; ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;125 jaar&quot; &quot;Onbekend&quot; &quot;twee jaar&quot; &quot;één jaar&quot; ... str(cv08_haven$lftop) # a &#39;dbl+lbl&#39; this stands for doubles, or real numbers, which are labeled #&gt; dbl+lbl [1:1963] 51, 39, 16, 29, 57, 49, 62, 34, 59, 41, 25, 43, 74, 17, 22, 32, 51, 66, 64, 2... #&gt; @ label : chr &quot;Leeftijd OP op datum interview&quot; #&gt; @ format.spss : chr &quot;F10.0&quot; #&gt; @ display_width: int 12 #&gt; @ labels : Named num [1:5] 0 1 2 99 125 #&gt; ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;&lt; één jaar&quot; &quot;één jaar&quot; &quot;twee jaar&quot; &quot;Onbekend&quot; ... # next to the data itself, attributes are stored attributes(cv08$lftop) #&gt; $levels #&gt; [1] &quot;&lt; één jaar&quot; &quot;één jaar&quot; &quot;125 jaar&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; #&gt; [8] &quot;twee jaar&quot; &quot;20&quot; &quot;21&quot; &quot;22&quot; &quot;23&quot; &quot;24&quot; &quot;25&quot; #&gt; [15] &quot;26&quot; &quot;27&quot; &quot;28&quot; &quot;29&quot; &quot;30&quot; &quot;31&quot; &quot;32&quot; #&gt; [22] &quot;33&quot; &quot;34&quot; &quot;35&quot; &quot;36&quot; &quot;37&quot; &quot;38&quot; &quot;39&quot; #&gt; [29] &quot;40&quot; &quot;41&quot; &quot;42&quot; &quot;43&quot; &quot;44&quot; &quot;45&quot; &quot;46&quot; #&gt; [36] &quot;47&quot; &quot;48&quot; &quot;49&quot; &quot;50&quot; &quot;51&quot; &quot;52&quot; &quot;53&quot; #&gt; [43] &quot;54&quot; &quot;55&quot; &quot;56&quot; &quot;57&quot; &quot;58&quot; &quot;59&quot; &quot;60&quot; #&gt; [50] &quot;61&quot; &quot;62&quot; &quot;63&quot; &quot;64&quot; &quot;65&quot; &quot;66&quot; &quot;67&quot; #&gt; [57] &quot;68&quot; &quot;69&quot; &quot;70&quot; &quot;71&quot; &quot;72&quot; &quot;73&quot; &quot;74&quot; #&gt; [64] &quot;75&quot; &quot;76&quot; &quot;77&quot; &quot;78&quot; &quot;79&quot; &quot;80&quot; &quot;81&quot; #&gt; [71] &quot;82&quot; &quot;83&quot; &quot;84&quot; &quot;85&quot; &quot;86&quot; &quot;87&quot; &quot;88&quot; #&gt; [78] &quot;89&quot; &quot;90&quot; &quot;91&quot; &quot;Onbekend&quot; #&gt; #&gt; $class #&gt; [1] &quot;factor&quot; attributes(cv08_nolab$lftop) #&gt; $value.labels #&gt; 125 jaar Onbekend twee jaar één jaar &lt; één jaar #&gt; &quot;125&quot; &quot;99&quot; &quot;2&quot; &quot;1&quot; &quot;0&quot; attributes(cv08_haven$lftop) #&gt; $label #&gt; [1] &quot;Leeftijd OP op datum interview&quot; #&gt; #&gt; $format.spss #&gt; [1] &quot;F10.0&quot; #&gt; #&gt; $display_width #&gt; [1] 12 #&gt; #&gt; $class #&gt; [1] &quot;haven_labelled&quot; &quot;vctrs_vctr&quot; &quot;double&quot; #&gt; #&gt; $labels #&gt; &lt; één jaar één jaar twee jaar Onbekend 125 jaar #&gt; 0 1 2 99 125 # to access specific attributes attr(cv08_haven$lftop, &quot;labels&quot;) #&gt; &lt; één jaar één jaar twee jaar Onbekend 125 jaar #&gt; 0 1 2 99 125 summary(cv08$lftop) #&gt; &lt; één jaar één jaar 125 jaar 16 17 18 19 twee jaar 20 #&gt; 0 0 0 40 37 39 30 0 30 #&gt; 21 22 23 24 25 26 27 28 29 #&gt; 25 25 38 26 22 18 23 29 30 #&gt; 30 31 32 33 34 35 36 37 38 #&gt; 22 28 23 23 24 38 35 37 34 #&gt; 39 40 41 42 43 44 45 46 47 #&gt; 48 45 34 36 39 43 38 41 32 #&gt; 48 49 50 51 52 53 54 55 56 #&gt; 41 45 29 29 43 32 25 27 27 #&gt; 57 58 59 60 61 62 63 64 65 #&gt; 30 44 34 33 36 40 29 27 30 #&gt; 66 67 68 69 70 71 72 73 74 #&gt; 19 24 24 24 23 21 13 15 26 #&gt; 75 76 77 78 79 80 81 82 83 #&gt; 10 14 17 13 13 10 7 10 10 #&gt; 84 85 86 87 88 89 90 91 Onbekend #&gt; 7 6 3 8 2 3 1 3 4 summary(cv08_nolab$lftop) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 16.00 33.00 46.00 46.78 61.00 99.00 summary(cv08_haven$lftop) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 16.00 33.00 46.00 46.78 61.00 99.00 table(cv08_haven$lftop, useNA = &quot;always&quot;) #&gt; #&gt; 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #&gt; 40 37 39 30 30 25 25 38 26 22 18 23 29 30 22 28 23 23 24 38 #&gt; 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #&gt; 35 37 34 48 45 34 36 39 43 38 41 32 41 45 29 29 43 32 25 27 #&gt; 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 #&gt; 27 30 44 34 33 36 40 29 27 30 19 24 24 24 23 21 13 15 26 10 #&gt; 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 99 &lt;NA&gt; #&gt; 14 17 13 13 10 7 10 10 7 6 3 8 2 3 1 3 4 0 You can also have a look at your data. Try out the following commands. names(cv08_haven) summary(cv08_haven) head(cv08_haven) # fix(cv08_haven) #will produce an error fix(cv08) View(cv08_haven) Add to your cheat sheet under functions: str(), summary(), attributes(), attr(), table(), names(), head(), fix(), View() A.7 Define missings Okay, lets start playing around with our dataset. We are going to have a look at specific variables, define missings, recode some values, etc. I will focus on the dataset created by the haven package. A.7.1 R Base Lets use age as example. This variable is called lftop in CV. First have a look at this variable. str(cv08_haven$lftop) summary(cv08_haven$lftop) attr(cv08_haven$lftop, &quot;labels&quot;) table(cv08_haven$lftop, useNA = &quot;always&quot;) #&gt; dbl+lbl [1:1963] 51, 39, 16, 29, 57, 49, 62, 34, 59, 41, 25, 43, 74, 17, 22, 32, 51, 66, 64, 2... #&gt; @ label : chr &quot;Leeftijd OP op datum interview&quot; #&gt; @ format.spss : chr &quot;F10.0&quot; #&gt; @ display_width: int 12 #&gt; @ labels : Named num [1:5] 0 1 2 99 125 #&gt; ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;&lt; één jaar&quot; &quot;één jaar&quot; &quot;twee jaar&quot; &quot;Onbekend&quot; ... #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 16.00 33.00 46.00 46.78 61.00 99.00 #&gt; &lt; één jaar één jaar twee jaar Onbekend 125 jaar #&gt; 0 1 2 99 125 #&gt; #&gt; 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #&gt; 40 37 39 30 30 25 25 38 26 22 18 23 29 30 22 28 23 23 24 38 #&gt; 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #&gt; 35 37 34 48 45 34 36 39 43 38 41 32 41 45 29 29 43 32 25 27 #&gt; 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 #&gt; 27 30 44 34 33 36 40 29 27 30 19 24 24 24 23 21 13 15 26 10 #&gt; 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 99 &lt;NA&gt; #&gt; 14 17 13 13 10 7 10 10 7 6 3 8 2 3 1 3 4 0 We have category onbekend, which should be a missing. Lets copy the original variable in a new one, and attach it to the dataset. Thus not: lftop_new &lt;- cv08$lftop but: cv08$lftop_new &lt;- cv08$lftop You probably already noticed that to assign values to a new object we use &lt;-. What we now want to do is to replace those values of our new variable cv08$lftop_new which have the values Onbekend. We thus have to use subsetting. cv08$lftop_new[cv08$lftop_new == &quot;Onbekend&quot;] &lt;- NA Dont forget, if you want to understand the code work inside out. Update your cheat sheet! Note that == is a logical operator. What are the other logical operators in R? Note that [] is used to subset elements from an object (e.g. dataframe/vector/matrix) Note that NA is used in R to define missing values. It means Not Applicable. So did our recode work? table(cv08$lftop_new, useNA = &quot;always&quot;) #&gt; #&gt; &lt; één jaar één jaar 125 jaar 16 17 18 19 twee jaar 20 #&gt; 0 0 0 40 37 39 30 0 30 #&gt; 21 22 23 24 25 26 27 28 29 #&gt; 25 25 38 26 22 18 23 29 30 #&gt; 30 31 32 33 34 35 36 37 38 #&gt; 22 28 23 23 24 38 35 37 34 #&gt; 39 40 41 42 43 44 45 46 47 #&gt; 48 45 34 36 39 43 38 41 32 #&gt; 48 49 50 51 52 53 54 55 56 #&gt; 41 45 29 29 43 32 25 27 27 #&gt; 57 58 59 60 61 62 63 64 65 #&gt; 30 44 34 33 36 40 29 27 30 #&gt; 66 67 68 69 70 71 72 73 74 #&gt; 19 24 24 24 23 21 13 15 26 #&gt; 75 76 77 78 79 80 81 82 83 #&gt; 10 14 17 13 13 10 7 10 10 #&gt; 84 85 86 87 88 89 90 91 Onbekend #&gt; 7 6 3 8 2 3 1 3 0 #&gt; &lt;NA&gt; #&gt; 4 levels(cv08$lftop_new) #&gt; [1] &quot;&lt; één jaar&quot; &quot;één jaar&quot; &quot;125 jaar&quot; &quot;16&quot; &quot;17&quot; &quot;18&quot; &quot;19&quot; #&gt; [8] &quot;twee jaar&quot; &quot;20&quot; &quot;21&quot; &quot;22&quot; &quot;23&quot; &quot;24&quot; &quot;25&quot; #&gt; [15] &quot;26&quot; &quot;27&quot; &quot;28&quot; &quot;29&quot; &quot;30&quot; &quot;31&quot; &quot;32&quot; #&gt; [22] &quot;33&quot; &quot;34&quot; &quot;35&quot; &quot;36&quot; &quot;37&quot; &quot;38&quot; &quot;39&quot; #&gt; [29] &quot;40&quot; &quot;41&quot; &quot;42&quot; &quot;43&quot; &quot;44&quot; &quot;45&quot; &quot;46&quot; #&gt; [36] &quot;47&quot; &quot;48&quot; &quot;49&quot; &quot;50&quot; &quot;51&quot; &quot;52&quot; &quot;53&quot; #&gt; [43] &quot;54&quot; &quot;55&quot; &quot;56&quot; &quot;57&quot; &quot;58&quot; &quot;59&quot; &quot;60&quot; #&gt; [50] &quot;61&quot; &quot;62&quot; &quot;63&quot; &quot;64&quot; &quot;65&quot; &quot;66&quot; &quot;67&quot; #&gt; [57] &quot;68&quot; &quot;69&quot; &quot;70&quot; &quot;71&quot; &quot;72&quot; &quot;73&quot; &quot;74&quot; #&gt; [64] &quot;75&quot; &quot;76&quot; &quot;77&quot; &quot;78&quot; &quot;79&quot; &quot;80&quot; &quot;81&quot; #&gt; [71] &quot;82&quot; &quot;83&quot; &quot;84&quot; &quot;85&quot; &quot;86&quot; &quot;87&quot; &quot;88&quot; #&gt; [78] &quot;89&quot; &quot;90&quot; &quot;91&quot; &quot;Onbekend&quot; But we want age as numeric variable not as a factor (categorical). str(cv08$lftop_new) #&gt; Factor w/ 81 levels &quot;&lt; één jaar&quot;,&quot;één jaar&quot;,..: 40 28 4 18 46 38 51 23 48 30 ... cv08$agen &lt;- as.numeric(as.character(cv08$lftop_new)) #how clumsy. we first convert the factor to a string and then to a numeric variable. table(cv08$agen, useNA = &quot;always&quot;) #&gt; #&gt; 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #&gt; 40 37 39 30 30 25 25 38 26 22 18 23 29 30 22 28 23 23 24 38 #&gt; 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #&gt; 35 37 34 48 45 34 36 39 43 38 41 32 41 45 29 29 43 32 25 27 #&gt; 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 #&gt; 27 30 44 34 33 36 40 29 27 30 19 24 24 24 23 21 13 15 26 10 #&gt; 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 &lt;NA&gt; #&gt; 14 17 13 13 10 7 10 10 7 6 3 8 2 3 1 3 4 str(cv08$agen) #&gt; num [1:1963] 51 39 16 29 57 49 62 34 59 41 ... Hint: R is case sensitive. Just try to avoid capitals in your variable names. There are people who have set up a whole list of rules how to name and label stuff. Interesting? You can have a look here. I will use all_lower_case_underscore_seperated. Add to your cheat sheet as.character() and as.numeric(). A.7.2 Tidy Copy the variable We will mutate the original dataset by adding a variable. cv08_haven &lt;- mutate(cv08_haven, lftop_new = lftop) Replace missings. Be aware that the value 99 is the onbekend category. cv08_haven$lftop_new &lt;- na_if(cv08_haven$lftop_new, 99) Normally you would combine these two steps into one cv08_haven &lt;- mutate(cv08_haven, lftop_new = na_if(lftop, 99)) check table(cv08_haven$lftop_new, useNA = &quot;always&quot;) #&gt; #&gt; 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 #&gt; 40 37 39 30 30 25 25 38 26 22 18 23 29 30 22 28 23 23 24 38 #&gt; 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 #&gt; 35 37 34 48 45 34 36 39 43 38 41 32 41 45 29 29 43 32 25 27 #&gt; 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 #&gt; 27 30 44 34 33 36 40 29 27 30 19 24 24 24 23 21 13 15 26 10 #&gt; 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 &lt;NA&gt; #&gt; 14 17 13 13 10 7 10 10 7 6 3 8 2 3 1 3 4 An advantage of the tidy way is that it is more intuitive and requires less subsetting. A disadvantage is that you need to know more specific functions. Hint: In your cheat sheet, make a distinction between all the stuff that belongs to R Base and all specific functions, operators, etc. that are part of Tidyverse. How do you know it is part of Tidyverse? Well, if you dont load tidyverse, the code will not work. A.8 Recoding variables So, we defined a missing value for age. As a second example let us recode the variable education. This one is called var006n in CV08. Lets create a new variable educ3 with three levels: 1. primary 2. secondary 3. tertiary A.8.1 R Base levels(cv08$var006n) #&gt; [1] &quot;onbekend&quot; &quot;OP &lt; 12 jr of volgt actueel bas.ondw.&quot; #&gt; [3] &quot;basisonderwijs&quot; &quot;vmbo&quot; #&gt; [5] &quot;mavo&quot; &quot;havo/vwo&quot; #&gt; [7] &quot;mbo&quot; &quot;hbo&quot; #&gt; [9] &quot;wo&quot; &quot;wo_duplicated_8&quot; #&gt; [11] &quot;Onbekend&quot; table(cv08$var006n, useNA = &quot;always&quot;) #&gt; #&gt; onbekend OP &lt; 12 jr of volgt actueel bas.ondw. #&gt; 5 0 #&gt; basisonderwijs vmbo #&gt; 380 287 #&gt; mavo havo/vwo #&gt; 137 106 #&gt; mbo hbo #&gt; 543 339 #&gt; wo wo_duplicated_8 #&gt; 0 166 #&gt; Onbekend &lt;NA&gt; #&gt; 0 0 # lets make it a numeric var first cv08$educn &lt;- as.numeric(cv08$var006n) # check table(cv08$educn, useNA = &quot;always&quot;) #&gt; #&gt; 1 3 4 5 6 7 8 10 &lt;NA&gt; #&gt; 5 380 287 137 106 543 339 166 0 # start with an empty variable cv08$educ3 &lt;- NA # fill category by category cv08$educ3[cv08$educn == 2 | cv08$educn == 3] &lt;- 1 cv08$educ3[cv08$educn &gt; 3 &amp; cv08$educn &lt; 8] &lt;- 2 cv08$educ3[cv08$educn &gt; 7 &amp; cv08$educn &lt; 11] &lt;- 3 # check table(cv08$educ3, useNA = &quot;always&quot;) #&gt; #&gt; 1 2 3 &lt;NA&gt; #&gt; 380 1073 505 5 prop.table(table(cv08$educ3, useNA = &quot;always&quot;)) #&gt; #&gt; 1 2 3 &lt;NA&gt; #&gt; 0.193581253 0.546612328 0.257259297 0.002547122 # now educ3 is a numeric variable, we want it as factor cv08$educ3 &lt;- as.factor(cv08$educ3) table(cv08$educ3, useNA = &quot;always&quot;) #&gt; #&gt; 1 2 3 &lt;NA&gt; #&gt; 380 1073 505 5 levels(cv08$educ3) &lt;- c(&quot;primary&quot;, &quot;secondary&quot;, &quot;tertiary&quot;) table(cv08$educ3, useNA = &quot;always&quot;) #&gt; #&gt; primary secondary tertiary &lt;NA&gt; #&gt; 380 1073 505 5 Is this really the first time we encountered the function c()? Well, make sure it is somewhere at the top of your cheat sheet. A.8.2 Tidy And now the fun starts. Tidyverse includes a dplyr::recode function, but this function does not work on labelled variables imported via the haven package. Luckily, there is a package that extends the original function, labelled. # install.packages(&#39;labelled&#39;) require(labelled) #to be able to use the recode function on haven labelled variables # inspect variable str(cv08_haven$var006n) #&gt; dbl+lbl [1:1963] 6, 8, 3, 8, 6, 2, 2, 5, 5, 1, 1, 3, 2, 1, 1, 5, 6, 3, 1, 5, 6, 2, 5, 6, 1, 3,... #&gt; @ label : chr &quot;Voltooid opleidingsniveau (uitgebreid) OP, 12-14 jarigen niet standaard op bas.ondw.&quot; #&gt; @ format.spss : chr &quot;F10.0&quot; #&gt; @ display_width: int 12 #&gt; @ labels : Named num [1:11] -3 -1 1 2 3 4 5 6 7 8 ... #&gt; ..- attr(*, &quot;names&quot;)= chr [1:11] &quot;onbekend&quot; &quot;OP &lt; 12 jr of volgt actueel bas.ondw.&quot; &quot;basisonderwijs&quot; &quot;vmbo&quot; ... attr(cv08_haven$var006n, &quot;labels&quot;) #&gt; onbekend OP &lt; 12 jr of volgt actueel bas.ondw. #&gt; -3e+00 -1e+00 #&gt; basisonderwijs vmbo #&gt; 1e+00 2e+00 #&gt; mavo havo/vwo #&gt; 3e+00 4e+00 #&gt; mbo hbo #&gt; 5e+00 6e+00 #&gt; wo wo #&gt; 7e+00 8e+00 #&gt; Onbekend #&gt; 1e+10 table(cv08_haven$var006n, useNA = &quot;always&quot;) #&gt; #&gt; -3 1 2 3 4 5 6 8 &lt;NA&gt; #&gt; 5 380 287 137 106 543 339 166 0 # recode values, all missings as one value cv08_haven &lt;- mutate(cv08_haven, educ3 = recode(var006n, `-3` = -9, `-1` = 1, `1` = 1, `2` = 2, `3` = 2, `4` = 2, `5` = 2, `6` = 3, `7` = 3, `8` = 3, `10` = -9), .keep_value_labels = FALSE) # replace missing values with NA. cv08_haven &lt;- mutate(cv08_haven, educ3 = na_if(educ3, -9)) # make educ3 a factor cv08_haven &lt;- mutate(cv08_haven, educ3 = factor(educ3, levels = c(1, 2, 3), labels = c(&quot;primary&quot;, &quot;secondary&quot;, &quot;tertiary&quot;))) # check table(cv08_haven$educ3, useNA = &quot;always&quot;) #&gt; #&gt; primary secondary tertiary &lt;NA&gt; #&gt; 380 1073 505 5 Personally, I think this is all quite complicated. But I guess this is a matter of taste. And advantage of the Tidy way is that you could use the %&gt;%, the piping operator. Now, your code does not read from the inside out but from left to right. For many people this is more intuitive. The output of the function on the left is transported to the (first argument of the) function on the right. Thus, in the example below, you see that in the second and third call to mutate I dont have to tell the function which dataset I am using. cv08_haven &lt;- mutate(cv08_haven, educ3 = recode(var006n, `-3` = -9, `-1` = 1, `1` = 1, `2` = 2, `3` = 2, `4` = 2, `5` = 2, `6` = 3, `7` = 3, `8` = 3, `10` = -9), .keep_value_labels = FALSE) %&gt;% mutate(educ3 = na_if(educ3, -9)) %&gt;% mutate(educ3 = factor(educ3, levels = c(1, 2, 3), labels = c(&quot;primary&quot;, &quot;secondary&quot;, &quot;tertiary&quot;))) Perhaps an even tidier way would be: cv08_haven &lt;- cv08_haven %&gt;% mutate(educ3 = recode(var006n, `-3` = -9, `-1` = 1, `1` = 1, `2` = 2, `3` = 2, `4` = 2, `5` = 2, `6` = 3, `7` = 3, `8` = 3, `10` = -9), .keep_value_labels = FALSE) %&gt;% mutate(educ3 = na_if(educ3, -9)) %&gt;% mutate(educ3 = factor(educ3, levels = c(1, 2, 3), labels = c(&quot;primary&quot;, &quot;secondary&quot;, &quot;tertiary&quot;))) In the example above, it still may feel a little clumsy to have to make a call to the same mutate function three times. Well, this is indeed not necessary. Thus, the most tidy way is: cv08_haven &lt;- cv08_haven %&gt;% mutate(educ3 = recode(var006n, `-3` = -9, `-1` = 1, `1` = 1, `2` = 2, `3` = 2, `4` = 2, `5` = 2, `6` = 3, `7` = 3, `8` = 3, `10` = -9, .keep_value_labels = FALSE), educ3 = na_if(educ3, -9), educ3 = factor(educ3, levels = c(1, 2, 3), labels = c(&quot;primary&quot;, &quot;secondary&quot;, &quot;tertiary&quot;))) A.9 Means and counting specific values A.9.1 R Base Next step. Lets calculate a mean. We will use three questions in CV on polarization. This does not make any theoretical sense of course. Step 1: have a look at the vars summary(cv08$int055) summary(cv08$int056) summary(cv08$int057) #&gt; Geen opgave N.v.t. Weet niet #&gt; 0 0 85 #&gt; Weigert Zeer groot Groot #&gt; 0 57 551 #&gt; Niet zo groot Helemaal geen tegenstelling #&gt; 1213 57 #&gt; Geen opgave N.v.t. Weet niet #&gt; 0 0 118 #&gt; Weigert Zeer groot Groot #&gt; 0 258 987 #&gt; Niet zo groot Helemaal geen tegenstelling #&gt; 571 29 #&gt; Geen opgave N.v.t. Weet niet #&gt; 0 0 145 #&gt; Weigert Zeer groot Groot #&gt; 0 213 803 #&gt; Niet zo groot Helemaal geen tegenstelling #&gt; 756 46 Step 2: make numeric cv08$int055n &lt;- as.numeric(cv08$int055) table(cv08$int055n, useNA = &quot;always&quot;) cv08$int056n &lt;- as.numeric(cv08$int056) table(cv08$int056n, useNA = &quot;always&quot;) cv08$int057n &lt;- as.numeric(cv08$int057) table(cv08$int057n, useNA = &quot;always&quot;) #&gt; #&gt; 3 5 6 7 8 &lt;NA&gt; #&gt; 85 57 551 1213 57 0 #&gt; #&gt; 3 5 6 7 8 &lt;NA&gt; #&gt; 118 258 987 571 29 0 #&gt; #&gt; 3 5 6 7 8 &lt;NA&gt; #&gt; 145 213 803 756 46 0 Step 3: define missings and recode cv08$int055n[cv08$int055n &lt; 5] &lt;- NA cv08$int055n &lt;- cv08$int055n - 4 cv08$int056n[cv08$int056n &lt; 5] &lt;- NA cv08$int056n &lt;- cv08$int056n - 4 cv08$int057n[cv08$int057n &lt; 5] &lt;- NA cv08$int057n &lt;- cv08$int057n - 4 Step 4: calculate means. # How does the function mean work in R? mean(cv08$int055n) #whoops mean(cv08$int055n, na.rm = TRUE) #works. but not what we want. mean(c(cv08$int055n, cv08$int056n, cv08$int057n), na.rm = T) #works but not what we want. #&gt; [1] NA #&gt; [1] 2.676251 #&gt; [1] 2.410756 This is not what we want. What we want is to calculate a mean for each row/respondent. This will do the trick: testmeans &lt;- rowMeans(cbind(cv08$int055n, cv08$int056n, cv08$int057n), na.rm = T) head(testmeans) #yes! #&gt; [1] 2.333333 2.666667 2.666667 2.333333 1.500000 1.333333 What we really want is a mean but only if there is a maximum of 1 NA in the three variables. # lets first count how many missings we have for each respondent nmis &lt;- rowSums(is.na(cbind(cv08$int055n, cv08$int056n, cv08$int057n))) # ?is.na ?rowSums testmeans &lt;- ifelse(nmis &lt; 2, testmeans, NA) # add the calculated means to our dataset cv08$int_mean &lt;- testmeans # Bonus: count specific values so now we have this, it is easy to find how many times respondents # answered &#39;zeer groot&#39;, that is &#39;1&#39; timesZG &lt;- rowSums(cbind(cv08$int055n, cv08$int056n, # cv08$int057n)==1, na.rm=T) You need to add a lot of very powerful functions to your cheat sheet: mean(), rowMeans(), rowSums, cbind(), is.na(), ifelse(). Did you also notice that the logicals FALSE and TRUE can be summed? (FALSE equals 0 and TRUE equals 1). A.9.2 Tidy Here it goes in one big code chunk. # Step 1: have a look at the vars str(cv08_haven$int055) attr(cv08_haven$int055, &quot;labels&quot;) summary(cv08_haven$int055) summary(cv08_haven$int056) summary(cv08_haven$int057) table(cv08_haven$int055n, useNA = &quot;always&quot;) table(cv08_haven$int056, useNA = &quot;always&quot;) table(cv08_haven$int057, useNA = &quot;always&quot;) # Step 2: define missings and recode cv08_haven &lt;- mutate(cv08_haven, int055n = recode(int055, `-6` = -9, `-5` = -9, `-3` = -9, `-2` = -9, `1` = 4, `2` = 3, `3` = 2, `4` = 1), .keep_value_labels = FALSE) %&gt;% mutate(int055n = na_if(int055n, -9)) %&gt;% mutate(int055n = labelled(int055n, c(`Helemaal geen tegenstelling` = 1, `Niet zo groot` = 2, Groot = 3, `Zeer groot` = 4))) cv08_haven &lt;- mutate(cv08_haven, int056n = recode(int056, `-6` = -9, `-5` = -9, `-3` = -9, `-2` = -9, `1` = 4, `2` = 3, `3` = 2, `4` = 1), .keep_value_labels = FALSE) %&gt;% mutate(int056n = na_if(int056n, -9)) %&gt;% mutate(int056n = labelled(int056n, c(`Helemaal geen tegenstelling` = 1, `Niet zo groot` = 2, Groot = 3, `Zeer groot` = 4))) cv08_haven &lt;- mutate(cv08_haven, int057n = recode(int057, `-6` = -9, `-5` = -9, `-3` = -9, `-2` = -9, `1` = 4, `2` = 3, `3` = 2, `4` = 1), .keep_value_labels = FALSE) %&gt;% mutate(int057n = na_if(int057n, -9)) %&gt;% mutate(int057n = labelled(int057n, c(`Helemaal geen tegenstelling` = 1, `Niet zo groot` = 2, Groot = 3, `Zeer groot` = 4))) # Step 3: calculate means. option 1 cv08_haven &lt;- cv08_haven %&gt;% rowwise() %&gt;% mutate(int_mean = mean(c(int055n, int056n, int057n), na.rm = TRUE)) ## option 2 cv08_haven &lt;- cv08_haven %&gt;% mutate(int_mean = rowMeans(cbind(int055n, int056n, int057n), na.rm = TRUE)) # what we really want is a mean but only if there is a maximum of 1 NA in the three variables cv08_haven &lt;- cv08_haven %&gt;% mutate(int_mean_temp = rowMeans(cbind(int055n, int056n, int057n), na.rm = TRUE), nmis = rowSums(is.na(cbind(int055n, int056n, int057n))), int_mean = ifelse(nmis &lt; 2, int_mean_temp, NA)) %&gt;% select(-int_mean_temp, -nmis) #&gt; dbl+lbl [1:1963] 3, 3, 3, 2, -3, 1, 2, 3, 3, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, ... #&gt; @ label : chr &quot;Tegenstelling arbeidersklasse en middenklasse&quot; #&gt; @ format.spss : chr &quot;F10.0&quot; #&gt; @ display_width: int 12 #&gt; @ labels : Named num [1:8] -6 -5 -3 -2 1 2 3 4 #&gt; ..- attr(*, &quot;names&quot;)= chr [1:8] &quot;Geen opgave&quot; &quot;N.v.t.&quot; &quot;Weet niet&quot; &quot;Weigert&quot; ... #&gt; Geen opgave N.v.t. Weet niet #&gt; -6 -5 -3 #&gt; Weigert Zeer groot Groot #&gt; -2 1 2 #&gt; Niet zo groot Helemaal geen tegenstelling #&gt; 3 4 #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -3.00 2.00 3.00 2.43 3.00 4.00 #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -3.000 2.000 2.000 1.888 3.000 4.000 #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -3.000 2.000 2.000 1.954 3.000 4.000 #&gt; #&gt; &lt;NA&gt; #&gt; 0 #&gt; #&gt; -3 1 2 3 4 &lt;NA&gt; #&gt; 118 258 987 571 29 0 #&gt; #&gt; -3 1 2 3 4 &lt;NA&gt; #&gt; 145 213 803 756 46 0 So what are you adding to your cheat sheet? rowwise(), select(). A.10 Merging data files What you need to know 1: Panel or stacked cross-sections? What you need to know 2: If panel, do you want data in long or wide format? We need to follow these steps: Step1: select variables Step2: make consistent Step3: perform the actual merging. Make sure to include necessary identifier variables. Step4: check your results!! A.10.1 R Base A.10.1.1 Step1: select variables # step 1: selecting the variables you want to keep. for this tutorial only 6 variables: id, age, # sex, educ, health, region (not that R is case sensitive) cv08_sel &lt;- cv08[, c(&quot;we_id&quot;, &quot;lftop&quot;, &quot;geslacht&quot;, &quot;var006n&quot;, &quot;v401&quot;, &quot;landd&quot;)] cv10_sel &lt;- cv10[, c(&quot;Sleutel&quot;, &quot;var002&quot;, &quot;var001&quot;, &quot;Vltoplop&quot;, &quot;V401&quot;, &quot;Landd&quot;)] Note that to select rows, you need to set an expression before the , [row,] and to select columns, after the , [,col]. Thus with dataset[i,j] you will select row i and column j. You have learned to subset dataframes by using indici (e.g. dataset[,1:3]), logical expressions (e.g. dataset[var1&gt;1,]), and names (e.g. dataset[,\"varname\"]). A.10.1.2 Step2: make consistent # step 2: making the variables similar across individual datasets step 2a: making names the same names(cv08_sel) &lt;- names(cv10_sel) &lt;- c(&quot;id&quot;, &quot;age&quot;, &quot;sex&quot;, &quot;educ&quot;, &quot;health&quot;, &quot;region&quot;) # step 2b: making levels and labels consistent summary(cv08_sel) summary(cv10_sel) # they look very consistent already. but check carefully. # we don&#39;t want id to be a factor but numeric. Note that we don&#39;t want the factor level values as # numbers but the actual labels as numbers. # id cv08_sel$id &lt;- as.numeric(as.character(cv08_sel$id)) cv10_sel$id &lt;- as.numeric(as.character(cv10_sel$id)) # age cv08_sel$age &lt;- as.numeric(as.character(cv08_sel$age)) cv10_sel$age &lt;- as.numeric(as.character(cv10_sel$age)) # sex men levels(cv08_sel$sex) levels(cv10_sel$sex) table(cv08_sel$sex, useNA = &quot;always&quot;) table(cv10_sel$sex, useNA = &quot;always&quot;) # lets make it a numeric var first cv08_sel$sexn &lt;- as.numeric(cv08_sel$sex) table(cv08_sel$sexn) # recode into dummy cv08_sel$men &lt;- ifelse(cv08_sel$sexn == 2, 1, 0) cv08_sel$men &lt;- ifelse(cv08_sel$sexn == 1, NA, cv08_sel$men) # check table(cv08_sel$men, useNA = &quot;always&quot;) # lets make it a numeric var first cv10_sel$sexn &lt;- as.numeric(cv10_sel$sex) table(cv10_sel$sexn) # recode into dummy cv10_sel$men &lt;- ifelse(cv10_sel$sexn == 2, 1, 0) # check table(cv10_sel$men, useNA = &quot;always&quot;) # educ educ3 levels(cv08_sel$educ) levels(cv10_sel$educ) table(cv08_sel$educ, useNA = &quot;always&quot;) table(cv10_sel$educ, useNA = &quot;always&quot;) # lets make it a numeric var first cv08_sel$educn &lt;- as.numeric(cv08_sel$educ) table(cv08_sel$educn) # recode into 3cats: 1 primair, 2 secundair, 3 is tertiair cv08_sel$educ3 &lt;- NA cv08_sel$educ3[cv08_sel$educn == 2 | cv08_sel$educn == 3] &lt;- 1 cv08_sel$educ3[cv08_sel$educn &gt; 3 &amp; cv08_sel$educn &lt; 8] &lt;- 2 cv08_sel$educ3[cv08_sel$educn &gt; 7 &amp; cv08_sel$educn &lt; 11] &lt;- 3 # check table(cv08_sel$educ3, useNA = &quot;always&quot;) prop.table(table(cv08_sel$educ3, useNA = &quot;always&quot;)) # lets make it a numeric var first cv10_sel$educn &lt;- as.numeric(cv10_sel$educ) table(cv10_sel$educn) # recode into 3cats: 1 primair, 2 secundari, 3 is tertiair cv10_sel$educ3 &lt;- NA cv10_sel$educ3[cv10_sel$educn &lt; 3] &lt;- 1 #correct? cv10_sel$educ3[cv10_sel$educn &gt; 2 &amp; cv10_sel$educn &lt; 6] &lt;- 2 cv10_sel$educ3[cv10_sel$educn == 6] &lt;- 3 # check table(cv10_sel$educ3, useNA = &quot;always&quot;) prop.table(table(cv10_sel$educ3, useNA = &quot;always&quot;)) A.10.1.3 Step3: merge # lets add a wave variable cv08_sel$wave &lt;- 2008 cv10_sel$wave &lt;- 2010 # let make a fake ID, we will use this later when we pretend CV is panel data. cv08_sel$id2 &lt;- rank(cv08_sel$id) cv10_sel$id2 &lt;- rank(cv10_sel$id) # simply place one dataset under the other thus row bind (rbind) check first if same vars in both # datasets. perhaps clean up first. cv08_sel &lt;- cv08_sel[, c(&quot;id&quot;, &quot;id2&quot;, &quot;age&quot;, &quot;men&quot;, &quot;educ3&quot;, &quot;health&quot;, &quot;region&quot;, &quot;wave&quot;)] cv10_sel &lt;- cv10_sel[, c(&quot;id&quot;, &quot;id2&quot;, &quot;age&quot;, &quot;men&quot;, &quot;educ3&quot;, &quot;health&quot;, &quot;region&quot;, &quot;wave&quot;)] summary(cv08_sel) #&gt; id id2 age men educ3 #&gt; Min. :36775330 Min. : 1.0 Min. :16.00 Min. :0.00 Min. :1.000 #&gt; 1st Qu.:37604540 1st Qu.: 491.5 1st Qu.:33.00 1st Qu.:0.00 1st Qu.:2.000 #&gt; Median :38724230 Median : 982.0 Median :46.00 Median :1.00 Median :2.000 #&gt; Mean :38830177 Mean : 982.0 Mean :46.67 Mean :0.51 Mean :2.064 #&gt; 3rd Qu.:40598965 3rd Qu.:1472.5 3rd Qu.:60.00 3rd Qu.:1.00 3rd Qu.:3.000 #&gt; Max. :41199300 Max. :1963.0 Max. :91.00 Max. :1.00 Max. :3.000 #&gt; NA&#39;s :4 NA&#39;s :10 NA&#39;s :5 #&gt; health region wave #&gt; goed, :1060 Postcode (nog) onbekend: 0 Min. :2008 #&gt; zeer goed, : 504 Noord-Nederland :220 1st Qu.:2008 #&gt; gaat wel, : 320 Oost-Nederland :416 Median :2008 #&gt; slecht, : 67 West-Nederland :853 Mean :2008 #&gt; of zeer slecht?: 12 Zuid-Nederland :474 3rd Qu.:2008 #&gt; Geen opgave : 0 Max. :2008 #&gt; (Other) : 0 summary(cv10_sel) #&gt; id id2 age men educ3 #&gt; Min. :20131231 Min. : 1.0 Min. :16.00 Min. :0.000 Min. :1.000 #&gt; 1st Qu.:20131965 1st Qu.: 734.8 1st Qu.:34.00 1st Qu.:0.000 1st Qu.:2.000 #&gt; Median :20132698 Median :1468.5 Median :48.00 Median :0.000 Median :2.000 #&gt; Mean :20132698 Mean :1468.5 Mean :48.27 Mean :0.485 Mean :2.106 #&gt; 3rd Qu.:20133432 3rd Qu.:2202.2 3rd Qu.:63.00 3rd Qu.:1.000 3rd Qu.:3.000 #&gt; Max. :20134166 Max. :2936.0 Max. :96.00 Max. :1.000 Max. :3.000 #&gt; NA&#39;s :3 #&gt; health region wave #&gt; goed, :1592 Noord-Nederland: 306 Min. :2010 #&gt; zeer goed, : 776 Oost-Nederland : 678 1st Qu.:2010 #&gt; gaat wel, : 466 West-Nederland :1263 Median :2010 #&gt; slecht, : 79 Zuid-Nederland : 689 Mean :2010 #&gt; of zeer slecht?: 22 3rd Qu.:2010 #&gt; Weigert : 1 Max. :2010 #&gt; (Other) : 0 cv_tot &lt;- rbind(cv08_sel, cv10_sel) A.10.1.4 Step4: check summary(cv_tot) #&gt; id id2 age men educ3 #&gt; Min. :20131231 Min. : 1 Min. :16.00 Min. :0.000 Min. :1.000 #&gt; 1st Qu.:20132456 1st Qu.: 613 1st Qu.:33.00 1st Qu.:0.000 1st Qu.:2.000 #&gt; Median :20133680 Median :1225 Median :47.00 Median :0.000 Median :2.000 #&gt; Mean :27624666 Mean :1274 Mean :47.63 Mean :0.495 Mean :2.089 #&gt; 3rd Qu.:37978375 3rd Qu.:1838 3rd Qu.:62.00 3rd Qu.:1.000 3rd Qu.:3.000 #&gt; Max. :41199300 Max. :2936 Max. :96.00 Max. :1.000 Max. :3.000 #&gt; NA&#39;s :4 NA&#39;s :10 NA&#39;s :8 #&gt; health region wave #&gt; goed, :2652 Postcode (nog) onbekend: 0 Min. :2008 #&gt; zeer goed, :1280 Noord-Nederland : 526 1st Qu.:2008 #&gt; gaat wel, : 786 Oost-Nederland :1094 Median :2010 #&gt; slecht, : 146 West-Nederland :2116 Mean :2009 #&gt; of zeer slecht?: 34 Zuid-Nederland :1163 3rd Qu.:2010 #&gt; Weigert : 1 Max. :2010 #&gt; (Other) : 0 head(cv_tot) #&gt; id id2 age men educ3 health region wave #&gt; 1 36775330 1 51 1 3 goed, West-Nederland 2008 #&gt; 2 36775340 2 39 0 3 goed, West-Nederland 2008 #&gt; 3 36775420 3 16 0 2 goed, West-Nederland 2008 #&gt; 4 36775440 4 29 1 3 goed, West-Nederland 2008 #&gt; 5 36775450 5 57 1 3 gaat wel, West-Nederland 2008 #&gt; 6 36775460 6 49 0 2 goed, West-Nederland 2008 Okay, lets pretend it was panel data. cv_tot would then be a panel dataset in long format. But oftentimes, you want a panel dataset in wide format. If you dont know the difference between long and wide format, check the differences between cv_tot and cv_tot_panel after step3b. A.10.1.5 Step3b: merge # lets make a panel dataset in wide format cv_tot_panel &lt;- merge(cv08_sel, cv10_sel, all = TRUE, by = &quot;id2&quot;) head(cv_tot_panel) #&gt; id2 id.x age.x men.x educ3.x health.x region.x wave.x id.y age.y men.y educ3.y #&gt; 1 1 36775330 51 1 3 goed, West-Nederland 2008 20131231 20 0 2 #&gt; 2 2 36775340 39 0 3 goed, West-Nederland 2008 20131232 29 0 3 #&gt; 3 3 36775420 16 0 2 goed, West-Nederland 2008 20131233 30 1 2 #&gt; 4 4 36775440 29 1 3 goed, West-Nederland 2008 20131234 64 1 2 #&gt; 5 5 36775450 57 1 3 gaat wel, West-Nederland 2008 20131235 45 1 1 #&gt; 6 6 36775460 49 0 2 goed, West-Nederland 2008 20131236 80 0 2 #&gt; health.y region.y wave.y #&gt; 1 goed, West-Nederland 2010 #&gt; 2 zeer goed, West-Nederland 2010 #&gt; 3 goed, West-Nederland 2010 #&gt; 4 goed, West-Nederland 2010 #&gt; 5 goed, West-Nederland 2010 #&gt; 6 goed, West-Nederland 2010 # rename variables. and when necessary merge again with third wave. not very efficient but it # works. # many people prefer the reshape function. (i like doing it myself but here it goes) cv_tot_panel &lt;- reshape(cv_tot, timevar = &quot;wave&quot;, idvar = &quot;id2&quot;, direction = &quot;wide&quot;) head(cv_tot_panel) #&gt; id2 id.2008 age.2008 men.2008 educ3.2008 health.2008 region.2008 id.2010 age.2010 men.2010 #&gt; 1 1 36775330 51 1 3 goed, West-Nederland 20131231 20 0 #&gt; 2 2 36775340 39 0 3 goed, West-Nederland 20131232 29 0 #&gt; 3 3 36775420 16 0 2 goed, West-Nederland 20131233 30 1 #&gt; 4 4 36775440 29 1 3 goed, West-Nederland 20131234 64 1 #&gt; 5 5 36775450 57 1 3 gaat wel, West-Nederland 20131235 45 1 #&gt; 6 6 36775460 49 0 2 goed, West-Nederland 20131236 80 0 #&gt; educ3.2010 health.2010 region.2010 #&gt; 1 2 goed, West-Nederland #&gt; 2 3 zeer goed, West-Nederland #&gt; 3 2 goed, West-Nederland #&gt; 4 2 goed, West-Nederland #&gt; 5 1 goed, West-Nederland #&gt; 6 2 goed, West-Nederland A.10.2 Tidy A.10.2.1 Step1: select variables # step 1: selecting the variables you want to keep. for this tutorial only 6 variables: id, age, # sex, educ, health, region (not that R is case sensitive) cv08_sel &lt;- cv08_haven %&gt;% select(c(&quot;we_id&quot;, &quot;lftop&quot;, &quot;geslacht&quot;, &quot;var006n&quot;, &quot;v401&quot;, &quot;landd&quot;)) cv10_sel &lt;- cv10_haven %&gt;% select(c(&quot;Sleutel&quot;, &quot;var002&quot;, &quot;var001&quot;, &quot;Vltoplop&quot;, &quot;V401&quot;, &quot;Landd&quot;)) A.10.2.2 Step2: make consistent # step 2: making the variables similar across individual datasets step 2a: making names the same names(cv08_sel) &lt;- names(cv10_sel) &lt;- c(&quot;id&quot;, &quot;age&quot;, &quot;sex&quot;, &quot;educ&quot;, &quot;health&quot;, &quot;region&quot;) # step 2b: making levels and labels consistent str(cv08_sel) str(cv10_sel) # summary(cv08_sel) summary(cv10_sel) # they look very consistent already. but check carefully. id is okay # age: replace &#39;onbekend&#39; cv08_sel &lt;- cv08_sel %&gt;% mutate(age = na_if(age, 99)) cv10_sel &lt;- cv10_sel %&gt;% mutate(age = na_if(age, 99)) # sex: men cv08_sel &lt;- cv08_sel %&gt;% mutate(men = recode(sex, `9` = -9, M = 1, V = 0, .keep_value_labels = FALSE), men = na_if(men, -9), men = labelled(men, c(man = 1, vrouw = 0))) cv10_sel &lt;- cv10_sel %&gt;% mutate(men = recode(sex, `2` = 0, .keep_value_labels = FALSE), men = labelled(men, c(man = 1, vrouw = 0))) # educ educ3 attr(cv08_sel$educ, &quot;labels&quot;) attr(cv10_sel$educ, &quot;labels&quot;) cv08_sel &lt;- cv08_sel %&gt;% mutate(educ3 = recode(educ, `-3` = -9, `-1` = 1, `1` = 1, `2` = 2, `3` = 2, `4` = 2, `5` = 2, `6` = 3, `7` = 3, `8` = 3, `10` = -9, .keep_value_labels = FALSE), educ3 = na_if(educ3, -9), educ3 = factor(educ3, levels = c(1, 2, 3), labels = c(&quot;primary&quot;, &quot;secondary&quot;, &quot;tertiary&quot;))) cv10_sel &lt;- cv10_sel %&gt;% mutate(educ3 = recode(educ, `-1` = 1, `1` = 1, `2` = 2, `3` = 2, `4` = 2, `5` = 3, `10` = -9, .keep_value_labels = FALSE), educ3 = na_if(educ3, -9), educ3 = factor(educ3, levels = c(1, 2, 3), labels = c(&quot;primary&quot;, &quot;secondary&quot;, &quot;tertiary&quot;))) A.10.2.3 Step3: merge # lets add a wave variable cv08_sel$wave &lt;- 2008 cv10_sel$wave &lt;- 2010 # let make a fake ID, we will use this later when we pretend CV is panel data. cv08_sel$id2 &lt;- rank(cv08_sel$id) cv10_sel$id2 &lt;- rank(cv10_sel$id) # simply place one dataset under the other thus row bind (rbind) check first if same vars in both # datasets. perhaps clean up first. cv08_sel &lt;- cv08_sel %&gt;% select(c(&quot;id&quot;, &quot;id2&quot;, &quot;age&quot;, &quot;men&quot;, &quot;educ3&quot;, &quot;health&quot;, &quot;region&quot;, &quot;wave&quot;)) cv10_sel &lt;- cv10_sel %&gt;% select(c(&quot;id&quot;, &quot;id2&quot;, &quot;age&quot;, &quot;men&quot;, &quot;educ3&quot;, &quot;health&quot;, &quot;region&quot;, &quot;wave&quot;)) cv_tot_tidy &lt;- cv08_sel %&gt;% add_row(cv10_sel) A.10.2.4 Step4: check summary(cv_tot_tidy) #&gt; id id2 age men educ3 #&gt; Min. :20131231 Min. : 1 Min. :16.00 Min. :0.000 primary : 839 #&gt; 1st Qu.:20132456 1st Qu.: 613 1st Qu.:33.00 1st Qu.:0.000 secondary:2777 #&gt; Median :20133680 Median :1225 Median :47.00 Median :1.000 tertiary :1275 #&gt; Mean :27624666 Mean :1274 Mean :47.63 Mean :0.513 NA&#39;s : 8 #&gt; 3rd Qu.:37978375 3rd Qu.:1838 3rd Qu.:62.00 3rd Qu.:1.000 #&gt; Max. :41199300 Max. :2936 Max. :96.00 Max. :1.000 #&gt; NA&#39;s :4 NA&#39;s :10 #&gt; health region wave #&gt; Min. :-2.000 Min. :1.000 Min. :2008 #&gt; 1st Qu.: 1.000 1st Qu.:2.000 1st Qu.:2008 #&gt; Median : 2.000 Median :3.000 Median :2010 #&gt; Mean : 1.979 Mean :2.799 Mean :2009 #&gt; 3rd Qu.: 2.000 3rd Qu.:3.000 3rd Qu.:2010 #&gt; Max. : 5.000 Max. :4.000 Max. :2010 #&gt; head(cv_tot_tidy) #&gt; # A tibble: 6 x 8 #&gt; # Rowwise: #&gt; id id2 age men educ3 health region wave #&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;fct&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt; &lt;dbl&gt; #&gt; 1 36775330 1 51 1 [man] tertiary 2 [goed,] 3 [West-Nederland] 2008 #&gt; 2 36775340 2 39 0 [vrouw] tertiary 2 [goed,] 3 [West-Nederland] 2008 #&gt; 3 36775420 3 16 0 [vrouw] secondary 2 [goed,] 3 [West-Nederland] 2008 #&gt; 4 36775440 4 29 1 [man] tertiary 2 [goed,] 3 [West-Nederland] 2008 #&gt; 5 36775450 5 57 1 [man] tertiary 3 [gaat wel,] 3 [West-Nederland] 2008 #&gt; 6 36775460 6 49 0 [vrouw] secondary 2 [goed,] 3 [West-Nederland] 2008 Okay, lets pretend it was panel data cv_tot would then be a panel dataset in long format. But oftentimes, you want a panel dataset in wide format. If you dont know the difference between long and wide format, check the differences between cv_tot and cv_tot_panel after step 3b. A.10.2.5 Step3b: merge # lets make a panel dataset in wide format cv_tot_panel_tidy &lt;- full_join(cv08_sel, cv10_sel, by = &quot;id2&quot;, suffix = c(&quot;.2008&quot;, &quot;.2010&quot;)) A.11 Aggregate data Lets suppose you want to add the mean age of each region as contextual variable to your data. A.11.1 R Base # step 1. construct dataset with aggregate info age_region &lt;- aggregate(cv_tot$age, by = list(cv_tot$region), FUN = mean) head(age_region) #&gt; Group.1 x #&gt; 1 Noord-Nederland 47.77567 #&gt; 2 Oost-Nederland 48.04113 #&gt; 3 West-Nederland NA #&gt; 4 Zuid-Nederland NA Ai, we have missings in age. Luckily the aggregate function can deal with missings. # step 1. construct dataset with aggregate info age_region &lt;- aggregate(cv_tot$age, by = list(cv_tot$region), FUN = mean, na.rm = TRUE) head(age_region) # lets correct variable names names(age_region) &lt;- c(&quot;region&quot;, &quot;age_region&quot;) age_region # step 2. match to dataset cv_total &lt;- merge(cv_tot, age_region, by = &quot;region&quot;, all.x = TRUE) head(cv_total) tail(cv_total) #&gt; Group.1 x #&gt; 1 Noord-Nederland 47.77567 #&gt; 2 Oost-Nederland 48.04113 #&gt; 3 West-Nederland 46.88416 #&gt; 4 Zuid-Nederland 48.52500 #&gt; region age_region #&gt; 1 Noord-Nederland 47.77567 #&gt; 2 Oost-Nederland 48.04113 #&gt; 3 West-Nederland 46.88416 #&gt; 4 Zuid-Nederland 48.52500 #&gt; region id id2 age men educ3 health wave age_region #&gt; 1 Noord-Nederland 40604110 1728 30 0 3 zeer goed, 2008 47.77567 #&gt; 2 Noord-Nederland 37975380 610 41 0 2 goed, 2008 47.77567 #&gt; 3 Noord-Nederland 40604300 1741 23 0 2 goed, 2008 47.77567 #&gt; 4 Noord-Nederland 38722490 890 49 1 2 gaat wel, 2008 47.77567 #&gt; 5 Noord-Nederland 20131654 424 60 0 1 goed, 2010 47.77567 #&gt; 6 Noord-Nederland 40604100 1727 18 1 2 goed, 2008 47.77567 #&gt; region id id2 age men educ3 health wave age_region #&gt; 4894 Zuid-Nederland 39568320 1429 78 1 2 goed, 2008 48.525 #&gt; 4895 Zuid-Nederland 20133708 2478 62 0 2 goed, 2010 48.525 #&gt; 4896 Zuid-Nederland 20134032 2802 49 1 2 zeer goed, 2010 48.525 #&gt; 4897 Zuid-Nederland 20132436 1206 52 1 1 slecht, 2010 48.525 #&gt; 4898 Zuid-Nederland 20131923 693 46 1 2 goed, 2010 48.525 #&gt; 4899 Zuid-Nederland 20134031 2801 50 1 2 slecht, 2010 48.525 # You can also define your own functions and use these. fmean_narm &lt;- function(x) { mean(x, na.rm = T) } age_region_test &lt;- aggregate(cv_tot$age, by = list(cv_tot$region), FUN = fmean_narm) head(age_region_test) #&gt; Group.1 x #&gt; 1 Noord-Nederland 47.77567 #&gt; 2 Oost-Nederland 48.04113 #&gt; 3 West-Nederland 46.88416 #&gt; 4 Zuid-Nederland 48.52500 A.11.2 Tidy # step 1. convert the dataset to an aggregate/grouped version using the &#39;group_by&#39; function from # the Dplyr package (part of the tidyverse), which will enable you to perform aggregate-level, or # &#39;by group,&#39; operations. age_region &lt;- group_by(cv_tot_tidy, region) # step 2. use mutate to create and append the mean age by region to the original data frame (don&#39;t # forget to remove NA values). age_region &lt;- mutate(age_region, mean = mean(age, na.rm = TRUE)) # step 3. you can link both commands using the pipe operator %&gt;% to keep your code concise (and # readable if you&#39;re writing a script). cv_total_tidy &lt;- cv_tot_tidy %&gt;% group_by(region) %&gt;% mutate(age_region = mean(age, na.rm = TRUE)) %&gt;% ungroup() #because group_by() returns a grouped tibble (a tibble specific class with a group attribute), it&#39;s good practice to close the pipe-chain with ungroup() to avoid an errors down the line. A.12 Missing values A.12.1 R Base Suppose you want to estimate the following model: model1 &lt;- lm(as.numeric(health) ~ men + age + as.factor(educ3) + as.factor(region), data = cv_total) summary(model1) #&gt; #&gt; Call: #&gt; lm(formula = as.numeric(health) ~ men + age + as.factor(educ3) + #&gt; as.factor(region), data = cv_total) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.8504 -0.5991 0.0100 0.3289 3.2407 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 5.6427632 0.0515902 109.377 &lt; 2e-16 *** #&gt; men -0.0155079 0.0211182 -0.734 0.463 #&gt; age 0.0115217 0.0005858 19.667 &lt; 2e-16 *** #&gt; as.factor(educ3)2 -0.1831809 0.0293149 -6.249 4.49e-10 *** #&gt; as.factor(educ3)3 -0.3248725 0.0330650 -9.825 &lt; 2e-16 *** #&gt; as.factor(region)Oost-Nederland -0.0535139 0.0391705 -1.366 0.172 #&gt; as.factor(region)West-Nederland -0.0152383 0.0360003 -0.423 0.672 #&gt; as.factor(region)Zuid-Nederland 0.0140803 0.0388009 0.363 0.717 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.7371 on 4869 degrees of freedom #&gt; (22 observations deleted due to missingness) #&gt; Multiple R-squared: 0.09928, Adjusted R-squared: 0.09799 #&gt; F-statistic: 76.67 on 7 and 4869 DF, p-value: &lt; 2.2e-16 You see that 22 cases are deleted due to missingness but what happened with your health variable? cv_total$health[cv_total$health == &quot;Weigert&quot;] &lt;- NA cv_total$healthn &lt;- as.numeric(cv_total$health) - 4 table(cv_total$health, useNA = &quot;always&quot;) table(cv_total$healthn, useNA = &quot;always&quot;) #&gt; #&gt; Geen opgave N.v.t. Weet niet Weigert zeer goed, goed, #&gt; 0 0 0 0 1280 2652 #&gt; gaat wel, slecht, of zeer slecht? &lt;NA&gt; #&gt; 786 146 34 1 #&gt; #&gt; 1 2 3 4 5 &lt;NA&gt; #&gt; 1280 2652 786 146 34 1 Of course we have several options: listwise deletion. Only use when very few missings replace missing values with intuitive values or add missing as a separate category. impute missing values. A bit complicated but the best option. A.12.1.1 Option 1: listwise deletion # step 1. define all missings summary(cv_total) #&gt; region id id2 age men #&gt; Postcode (nog) onbekend: 0 Min. :20131231 Min. : 1 Min. :16.00 Min. :0.000 #&gt; Noord-Nederland : 526 1st Qu.:20132456 1st Qu.: 613 1st Qu.:33.00 1st Qu.:0.000 #&gt; Oost-Nederland :1094 Median :20133680 Median :1225 Median :47.00 Median :0.000 #&gt; West-Nederland :2116 Mean :27624666 Mean :1274 Mean :47.63 Mean :0.495 #&gt; Zuid-Nederland :1163 3rd Qu.:37978375 3rd Qu.:1838 3rd Qu.:62.00 3rd Qu.:1.000 #&gt; Max. :41199300 Max. :2936 Max. :96.00 Max. :1.000 #&gt; NA&#39;s :4 NA&#39;s :10 #&gt; educ3 health wave age_region healthn #&gt; Min. :1.000 goed, :2652 Min. :2008 Min. :46.88 Min. :1.00 #&gt; 1st Qu.:2.000 zeer goed, :1280 1st Qu.:2008 1st Qu.:46.88 1st Qu.:1.00 #&gt; Median :2.000 gaat wel, : 786 Median :2010 Median :47.78 Median :2.00 #&gt; Mean :2.089 slecht, : 146 Mean :2009 Mean :47.63 Mean :1.98 #&gt; 3rd Qu.:3.000 of zeer slecht?: 34 3rd Qu.:2010 3rd Qu.:48.04 3rd Qu.:2.00 #&gt; Max. :3.000 (Other) : 0 Max. :2010 Max. :48.52 Max. :5.00 #&gt; NA&#39;s :8 NA&#39;s : 1 NA&#39;s :1 model2 &lt;- lm(as.numeric(healthn) ~ men + age + as.factor(educ3) + as.factor(region), data = cv_total) summary(model2) #&gt; #&gt; Call: #&gt; lm(formula = as.numeric(healthn) ~ men + age + as.factor(educ3) + #&gt; as.factor(region), data = cv_total) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.6261 -0.6001 0.0094 0.3288 3.2415 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.6429046 0.0515620 31.863 &lt; 2e-16 *** #&gt; men -0.0162517 0.0211087 -0.770 0.441 #&gt; age 0.0115192 0.0005855 19.673 &lt; 2e-16 *** #&gt; as.factor(educ3)2 -0.1831872 0.0292989 -6.252 4.39e-10 *** #&gt; as.factor(educ3)3 -0.3233545 0.0330525 -9.783 &lt; 2e-16 *** #&gt; as.factor(region)Oost-Nederland -0.0535499 0.0391492 -1.368 0.171 #&gt; as.factor(region)West-Nederland -0.0153441 0.0359807 -0.426 0.670 #&gt; as.factor(region)Zuid-Nederland 0.0157006 0.0387850 0.405 0.686 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.7367 on 4868 degrees of freedom #&gt; (23 observations deleted due to missingness) #&gt; Multiple R-squared: 0.09925, Adjusted R-squared: 0.09795 #&gt; F-statistic: 76.63 on 7 and 4868 DF, p-value: &lt; 2.2e-16 You see 23 cases deleted due to missingness A.12.1.2 Option 2: replacing missing values. Dont replace missings on dependent variable. For categorical variables add category missing. For continues/metric variables replace missing with mean value. cv_total$men2 &lt;- ifelse(is.na(cv_total$men), 2, cv_total$men) summary(cv_total$men2) cv_total$educ3b &lt;- ifelse(is.na(cv_total$educ3), 4, cv_total$educ3) summary(cv_total$educ3b) cv_total$age2 &lt;- ifelse(is.na(cv_total$age), mean(cv_total$age, na.rm = TRUE), cv_total$age) # And lets make a dummy that indicates for whom we replaced missing values. cv_total$age_mis &lt;- ifelse(is.na(cv_total$age), 1, 0) summary(cv_total$age2) table(cv_total$age_mis) # pay attention, now use categorical variable men2 model3 &lt;- lm(healthn ~ as.factor(men2) + age2 + age_mis + as.factor(educ3b) + as.factor(region), data = cv_total) summary(model3) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 0.0000 0.0000 0.0000 0.4981 1.0000 2.0000 #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 1.000 2.000 2.000 2.092 3.000 4.000 #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 16.00 33.50 47.00 47.63 62.00 96.00 #&gt; #&gt; 0 1 #&gt; 4895 4 #&gt; #&gt; Call: #&gt; lm(formula = healthn ~ as.factor(men2) + age2 + age_mis + as.factor(educ3b) + #&gt; as.factor(region), data = cv_total) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.6200 -0.6013 0.0103 0.3268 3.2422 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.6430251 0.0516038 31.839 &lt; 2e-16 *** #&gt; as.factor(men2)1 -0.0186500 0.0211416 -0.882 0.378 #&gt; as.factor(men2)2 -0.2534807 0.2341966 -1.082 0.279 #&gt; age2 0.0114647 0.0005852 19.592 &lt; 2e-16 *** #&gt; age_mis 0.1067769 0.3699339 0.289 0.773 #&gt; as.factor(educ3b)2 -0.1786212 0.0293068 -6.095 1.18e-09 *** #&gt; as.factor(educ3b)3 -0.3193546 0.0330785 -9.654 &lt; 2e-16 *** #&gt; as.factor(educ3b)4 -0.4032117 0.2624656 -1.536 0.125 #&gt; as.factor(region)Oost-Nederland -0.0548586 0.0391963 -1.400 0.162 #&gt; as.factor(region)West-Nederland -0.0146274 0.0360252 -0.406 0.685 #&gt; as.factor(region)Zuid-Nederland 0.0157363 0.0388302 0.405 0.685 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.7384 on 4887 degrees of freedom #&gt; (1 observation deleted due to missingness) #&gt; Multiple R-squared: 0.09797, Adjusted R-squared: 0.09612 #&gt; F-statistic: 53.08 on 10 and 4887 DF, p-value: &lt; 2.2e-16 A.12.1.3 Option 3: impute missing values We will use the R package van Buuren and Groothuis-Oudshoorn (2011). For theory please see: * https://stefvanbuuren.name/Winnipeg * https://stefvanbuuren.name/Winnipeg/Lectures/Winnipeg.pdf * For great reading see: https://bookdown.org/mwheymans/bookmi/ Read the literature, lectures and have a look at all vignettes of the package mice (here). This is not basic stuff! # lets start with the original dataset, that is without replacement of the missings cv_total &lt;- cv_total[, c(&quot;id&quot;, &quot;id2&quot;, &quot;age&quot;, &quot;men&quot;, &quot;educ3&quot;, &quot;health&quot;, &quot;region&quot;, &quot;wave&quot;)] # define all missings only for health needs to be redefined cv_total$health[cv_total$health == &quot;Weigert&quot;] &lt;- NA cv_total$health &lt;- as.numeric(cv_total$health) - 4 # multiple imputation take into account measurement level of variables cv_total$men &lt;- as.factor(cv_total$men) cv_total$educ3 &lt;- as.factor(cv_total$educ3) # check pattern md.pattern(cv_total) # we do not have real patterns. thus MCAR. This is only seldom the case!! # lets impute names(cv_total) # We do not want to use id id2 and wave to predict the other variables thus we need to tell mice in # the predictorMatrix argument. Easiest way is to first estimate and then correct. imp &lt;- mice(data = cv_total, method = c(&quot;&quot;, &quot;&quot;, &quot;cart&quot;, &quot;logreg&quot;, &quot;polr&quot;, &quot;cart&quot;, &quot;&quot;, &quot;&quot;)) attributes(imp) pred &lt;- imp$pred pred[, &quot;id&quot;] &lt;- 0 pred[, &quot;id2&quot;] &lt;- 0 pred[, &quot;wave&quot;] &lt;- 0 pred # let us also use a seed, so we have the same data in class. imp &lt;- mice(data = cv_total, method = c(&quot;&quot;, &quot;&quot;, &quot;cart&quot;, &quot;logreg&quot;, &quot;polr&quot;, &quot;cart&quot;, &quot;&quot;, &quot;&quot;), pred = pred, seed = 45622) summary(cv_total) summary(complete(imp)) plot(imp) # in real life: check convergence, check plausible values. see vignette 2 of mice package # and fit model on imputed dataset model_imp &lt;- with(imp, lm(as.numeric(health) ~ men + age + educ3 + region)) pool_model_imp &lt;- pool(model_imp) summary(pool_model_imp) #&gt; id id2 region wave health age educ3 men #&gt; 4876 1 1 1 1 1 1 1 1 0 #&gt; 10 1 1 1 1 1 1 1 0 1 #&gt; 8 1 1 1 1 1 1 0 1 1 #&gt; 4 1 1 1 1 1 0 1 1 1 #&gt; 1 1 1 1 1 0 1 1 1 1 #&gt; 0 0 0 0 1 4 8 10 23 #&gt; [1] &quot;id&quot; &quot;id2&quot; &quot;age&quot; &quot;men&quot; &quot;educ3&quot; &quot;health&quot; &quot;region&quot; &quot;wave&quot; #&gt; #&gt; iter imp variable #&gt; 1 1 age men educ3 health #&gt; 1 2 age men educ3 health #&gt; 1 3 age men educ3 health #&gt; 1 4 age men educ3 health #&gt; 1 5 age men educ3 health #&gt; 2 1 age men educ3 health #&gt; 2 2 age men educ3 health #&gt; 2 3 age men educ3 health #&gt; 2 4 age men educ3 health #&gt; 2 5 age men educ3 health #&gt; 3 1 age men educ3 health #&gt; 3 2 age men educ3 health #&gt; 3 3 age men educ3 health #&gt; 3 4 age men educ3 health #&gt; 3 5 age men educ3 health #&gt; 4 1 age men educ3 health #&gt; 4 2 age men educ3 health #&gt; 4 3 age men educ3 health #&gt; 4 4 age men educ3 health #&gt; 4 5 age men educ3 health #&gt; 5 1 age men educ3 health #&gt; 5 2 age men educ3 health #&gt; 5 3 age men educ3 health #&gt; 5 4 age men educ3 health #&gt; 5 5 age men educ3 health #&gt; $names #&gt; [1] &quot;data&quot; &quot;imp&quot; &quot;m&quot; &quot;where&quot; &quot;blocks&quot; #&gt; [6] &quot;call&quot; &quot;nmis&quot; &quot;method&quot; &quot;predictorMatrix&quot; &quot;visitSequence&quot; #&gt; [11] &quot;formulas&quot; &quot;post&quot; &quot;blots&quot; &quot;ignore&quot; &quot;seed&quot; #&gt; [16] &quot;iteration&quot; &quot;lastSeedValue&quot; &quot;chainMean&quot; &quot;chainVar&quot; &quot;loggedEvents&quot; #&gt; [21] &quot;version&quot; &quot;date&quot; #&gt; #&gt; $class #&gt; [1] &quot;mids&quot; #&gt; #&gt; id id2 age men educ3 health region wave #&gt; id 0 0 1 1 1 1 1 0 #&gt; id2 0 0 1 1 1 1 1 0 #&gt; age 0 0 0 1 1 1 1 0 #&gt; men 0 0 1 0 1 1 1 0 #&gt; educ3 0 0 1 1 0 1 1 0 #&gt; health 0 0 1 1 1 0 1 0 #&gt; region 0 0 1 1 1 1 0 0 #&gt; wave 0 0 1 1 1 1 1 0 #&gt; #&gt; iter imp variable #&gt; 1 1 age men educ3 health #&gt; 1 2 age men educ3 health #&gt; 1 3 age men educ3 health #&gt; 1 4 age men educ3 health #&gt; 1 5 age men educ3 health #&gt; 2 1 age men educ3 health #&gt; 2 2 age men educ3 health #&gt; 2 3 age men educ3 health #&gt; 2 4 age men educ3 health #&gt; 2 5 age men educ3 health #&gt; 3 1 age men educ3 health #&gt; 3 2 age men educ3 health #&gt; 3 3 age men educ3 health #&gt; 3 4 age men educ3 health #&gt; 3 5 age men educ3 health #&gt; 4 1 age men educ3 health #&gt; 4 2 age men educ3 health #&gt; 4 3 age men educ3 health #&gt; 4 4 age men educ3 health #&gt; 4 5 age men educ3 health #&gt; 5 1 age men educ3 health #&gt; 5 2 age men educ3 health #&gt; 5 3 age men educ3 health #&gt; 5 4 age men educ3 health #&gt; 5 5 age men educ3 health #&gt; id id2 age men educ3 health #&gt; Min. :20131231 Min. : 1 Min. :16.00 0 :2469 1 : 839 Min. :1.00 #&gt; 1st Qu.:20132456 1st Qu.: 613 1st Qu.:33.00 1 :2420 2 :2777 1st Qu.:1.00 #&gt; Median :20133680 Median :1225 Median :47.00 NA&#39;s: 10 3 :1275 Median :2.00 #&gt; Mean :27624666 Mean :1274 Mean :47.63 NA&#39;s: 8 Mean :1.98 #&gt; 3rd Qu.:37978375 3rd Qu.:1838 3rd Qu.:62.00 3rd Qu.:2.00 #&gt; Max. :41199300 Max. :2936 Max. :96.00 Max. :5.00 #&gt; NA&#39;s :4 NA&#39;s :1 #&gt; region wave #&gt; Postcode (nog) onbekend: 0 Min. :2008 #&gt; Noord-Nederland : 526 1st Qu.:2008 #&gt; Oost-Nederland :1094 Median :2010 #&gt; West-Nederland :2116 Mean :2009 #&gt; Zuid-Nederland :1163 3rd Qu.:2010 #&gt; Max. :2010 #&gt; #&gt; id id2 age men educ3 health #&gt; Min. :20131231 Min. : 1 Min. :16.00 0:2471 1: 842 Min. :1.000 #&gt; 1st Qu.:20132456 1st Qu.: 613 1st Qu.:33.00 1:2428 2:2779 1st Qu.:1.000 #&gt; Median :20133680 Median :1225 Median :47.00 3:1278 Median :2.000 #&gt; Mean :27624666 Mean :1274 Mean :47.62 Mean :1.979 #&gt; 3rd Qu.:37978375 3rd Qu.:1838 3rd Qu.:62.00 3rd Qu.:2.000 #&gt; Max. :41199300 Max. :2936 Max. :96.00 Max. :5.000 #&gt; region wave #&gt; Postcode (nog) onbekend: 0 Min. :2008 #&gt; Noord-Nederland : 526 1st Qu.:2008 #&gt; Oost-Nederland :1094 Median :2010 #&gt; West-Nederland :2116 Mean :2009 #&gt; Zuid-Nederland :1163 3rd Qu.:2010 #&gt; Max. :2010 #&gt; term estimate std.error statistic df p.value #&gt; 1 (Intercept) 1.64227489 0.0515381739 31.8652131 4883.812 0.000000e+00 #&gt; 2 men1 -0.01821954 0.0211574583 -0.8611405 4683.712 3.892048e-01 #&gt; 3 age 0.01146351 0.0005846149 19.6086530 4854.560 0.000000e+00 #&gt; 4 educ32 -0.17813172 0.0292524289 -6.0894676 4881.477 1.219647e-09 #&gt; 5 educ33 -0.31977903 0.0330290911 -9.6817388 4859.881 0.000000e+00 #&gt; 6 regionOost-Nederland -0.05554060 0.0391698388 -1.4179430 4888.390 1.562711e-01 #&gt; 7 regionWest-Nederland -0.01515412 0.0360044223 -0.4208962 4888.453 6.738494e-01 #&gt; 8 regionZuid-Nederland 0.01582036 0.0387973170 0.4077694 4888.453 6.834609e-01 A.12.2 Tidy Suppose you want to estimate the following model: model1 &lt;- cv_total_tidy %&gt;% mutate(healthn = as.numeric(health), across(c(educ3, region), as.factor)) %&gt;% lm(formula = healthn ~ men + age + educ3 + region) %&gt;% summary() # the across function from the Dplyr package is similar to the apply function (if MARGIN = 2) from # base R. it performs the same operation on multiple columns (or, if you have groups, for each # combination of columns and groups). # lm is not a &#39;pipe friendly&#39; function because the data is provided in the second argument, while # the pipe operator reads the data from the first unnamed(!) argument that follows the %&gt;%. by # naming the first argument, the pipe will take data from the second argument, which is (in this # case) the correct data argument. model1 #&gt; #&gt; Call: #&gt; lm(formula = healthn ~ men + age + educ3 + region, data = .) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -3.8116 -0.6047 0.0148 0.3307 3.1987 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 1.6653869 0.0514844 32.347 &lt; 2e-16 *** #&gt; men -0.0645195 0.0211756 -3.047 0.00232 ** #&gt; age 0.0114940 0.0005866 19.594 &lt; 2e-16 *** #&gt; educ3secondary -0.1804942 0.0293612 -6.147 8.51e-10 *** #&gt; educ3tertiary -0.3212901 0.0331382 -9.695 &lt; 2e-16 *** #&gt; region2 -0.0513877 0.0392212 -1.310 0.19019 #&gt; region3 -0.0138878 0.0360443 -0.385 0.70003 #&gt; region4 0.0148341 0.0388569 0.382 0.70265 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.738 on 4869 degrees of freedom #&gt; (22 observations deleted due to missingness) #&gt; Multiple R-squared: 0.1007, Adjusted R-squared: 0.09939 #&gt; F-statistic: 77.87 on 7 and 4869 DF, p-value: &lt; 2.2e-16 Whoops, tidy also includes th weigert observation for the health variable. Let us correct. # step 1. define all missings cv_total_tidy &lt;- cv_total_tidy %&gt;% mutate(health = na_if(health, -2), healthn = as.numeric(health) - 4) A.12.2.1 Option 1: listwise deletion model2 &lt;- cv_total_tidy %&gt;% mutate(across(c(educ3, region), as.factor)) %&gt;% lm(formula = healthn ~ men + age + educ3 + region) %&gt;% summary() model2 #&gt; #&gt; Call: #&gt; lm(formula = healthn ~ men + age + educ3 + region, data = .) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.6468 -0.6067 0.0159 0.3293 3.2001 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -2.335748 0.051349 -45.488 &lt; 2e-16 *** #&gt; men -0.063146 0.021121 -2.990 0.00281 ** #&gt; age 0.011489 0.000585 19.638 &lt; 2e-16 *** #&gt; educ3secondary -0.180538 0.029284 -6.165 7.61e-10 *** #&gt; educ3tertiary -0.318222 0.033056 -9.627 &lt; 2e-16 *** #&gt; region2 -0.051467 0.039117 -1.316 0.18833 #&gt; region3 -0.014097 0.035949 -0.392 0.69497 #&gt; region4 0.018132 0.038759 0.468 0.63994 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.7361 on 4868 degrees of freedom #&gt; (23 observations deleted due to missingness) #&gt; Multiple R-squared: 0.1008, Adjusted R-squared: 0.0995 #&gt; F-statistic: 77.95 on 7 and 4868 DF, p-value: &lt; 2.2e-16 A.12.2.2 Option 2: replacing missing values. Besides the ifelse() command, you can use replace_na from the Tidyr package (part of the tidyverse), which, you might have guessed from the name, enables you to replace NAs with specified values. However, using replace_na will return an error if the variable is a factor and requires conversion to numeric-type values. Well still use ifelse() to create the dummy age_mis. # with the first mutate argument you avoid repeating as.numeric() for each variable. cv_total_tidy2 &lt;- cv_total_tidy %&gt;% mutate(across(everything(), as.numeric), men2 = replace_na(men, 2), educ3b = replace_na(educ3, 4), age2 = replace_na(age, mean(age, na.rm = TRUE)), age_mis = ifelse(is.na(age), 1, 0)) # note: the following two commands are equivalent: mutate(across(everything(), as.numeric)) # mutate_all(as.numeric) the next two as well: mutate_at(c(&#39;men2&#39;, &#39;educ3b&#39;, &#39;region&#39;), as.factor) # mutate(across(c(men2, educ3b, region), as.factor)) # pay attention, now use categorical variable men2. model3 &lt;- cv_total_tidy2 %&gt;% mutate_at(c(&quot;men2&quot;, &quot;educ3b&quot;, &quot;region&quot;), as.factor) %&gt;% lm(formula = healthn ~ men2 + age2 + age_mis + educ3b + region) %&gt;% summary() model3 #&gt; #&gt; Call: #&gt; lm(formula = healthn ~ men2 + age2 + age_mis + educ3b + region, #&gt; data = .) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.6410 -0.6085 0.0151 0.3288 3.1980 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -2.3353427 0.0513786 -45.454 &lt; 2e-16 *** #&gt; men21 -0.0663976 0.0211427 -3.140 0.0017 ** #&gt; men22 -0.2774981 0.2339830 -1.186 0.2357 #&gt; age2 0.0114325 0.0005846 19.554 &lt; 2e-16 *** #&gt; age_mis 0.1075306 0.3695905 0.291 0.7711 #&gt; educ3b2 -0.1758631 0.0292885 -6.005 2.06e-09 *** #&gt; educ3b3 -0.3139584 0.0330785 -9.491 &lt; 2e-16 *** #&gt; educ3b4 -0.3938024 0.2620934 -1.503 0.1330 #&gt; region2 -0.0526057 0.0391619 -1.343 0.1792 #&gt; region3 -0.0132167 0.0359911 -0.367 0.7135 #&gt; region4 0.0183776 0.0388022 0.474 0.6358 #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.7377 on 4887 degrees of freedom #&gt; (1 observation deleted due to missingness) #&gt; Multiple R-squared: 0.09964, Adjusted R-squared: 0.0978 #&gt; F-statistic: 54.08 on 10 and 4887 DF, p-value: &lt; 2.2e-16 A.12.2.3 Option 3: impute missing values pred &lt;- cv_total_tidy %&gt;% select(c(&quot;id&quot;, &quot;id2&quot;, &quot;age&quot;, &quot;men&quot;, &quot;educ3&quot;, &quot;health&quot;, &quot;region&quot;, &quot;wave&quot;)) %&gt;% #start with the original variables. mutate_all(as.numeric) %&gt;% mutate_at(c(&quot;men&quot;, &quot;educ3&quot;), as.factor) %&gt;% mice(method = c(&quot;&quot;, &quot;&quot;, &quot;cart&quot;, &quot;logreg&quot;, &quot;polr&quot;, &quot;cart&quot;, &quot;&quot;, &quot;&quot;)) %&gt;% unclass() %&gt;% #The object returned from the mice function is of class &#39;mids,&#39; which dplyr&#39;s functions cannot handle. The unclass() function from base R solve this issue as it removes the class attribute and, in this case, returns a list that dplyr can handle. pluck(&quot;predictorMatrix&quot;) %&gt;% #the pluck() function from the Purrr package (part of the tidyverse) is the user-friendly equivalent of `[[` in base R: `[[`(&quot;predictorMatrix&quot;), which enables you to extract a single element from the list we just created using unclass(). as.data.frame() %&gt;% #Because the mutate function (and its variations) are not build for lists, we need to coerce into a data frame (or Tibble) mutate_at(c(&quot;id&quot;, &quot;id2&quot;, &quot;age&quot;), funs(recode(1, 0))) %&gt;% #To &#39;remove&#39; the columns id id2 and wave as predictors, we recode all their 1s to 0s using mutate_at() and recode(). as.matrix() #impute missings (again but now on correct predictors) and fit model on imputed dataset model_imp &lt;- cv_total_tidy %&gt;% select(c(&quot;id&quot;,&quot;id2&quot;,&quot;age&quot;,&quot;men&quot;,&quot;educ3&quot;,&quot;health&quot;,&quot;region&quot;,&quot;wave&quot;)) %&gt;% mutate_all(as.numeric) %&gt;% mutate_at(c(&quot;men&quot;, &quot;educ3&quot;), as.factor) %&gt;% mice(method = c(&quot;&quot;, &quot;&quot;, &quot;cart&quot;, &quot;logreg&quot;, &quot;polr&quot;, &quot;cart&quot;, &quot;&quot;, &quot;&quot;), pred = pred, seed = 45622) %&gt;% with(lm(as.numeric(health) ~ men + age + educ3 + region)) %&gt;% pool() summary(model_imp) A.13 Very important stuff!! Below are some important topics you need to be familiar with if you want to enjoy R. The web is full of very good material/tutorials on these subjects, so I see no need to write them myself. However, if I have a strong opinion on something, I will let you know. A.13.1 if else if (condition) { # code executed when condition is TRUE } else { # code executed when condition is FALSE } Useful links: - r4ds - Advanced R A.13.2 loops Always define the object in which you are going to save the data beforehand and make sure it has the proper size. If you do not know the size beforehand, make use of a list. Please use proper indentation. Use comments. Be aware of missing values in the input. Thus not x &lt;- c(4,5,6,2,7,897,23, NA, 7) y &lt;- c(7,9,6,98,1,NA,3, NA, 7) output &lt;- NA for (i in seq_along(x)) { output[i] &lt;- x[i] + y[i] } if you want your output to be : [1] 11 14 12 100 8 897 26 NA 14. But: x &lt;- c(4, 5, 6, 2, 7, 897, 23, NA, 7) y &lt;- c(7, 9, 6, 98, 1, NA, 3, NA, 7) output &lt;- rep(NA, length(x)) #store data here for (i in seq_along(x)) { # I want to add x and y if both have valid values and otherwise just have the valid value. if (!is.na(x[i]) &amp; !is.na(y[i])) { output[i] &lt;- x[i] + y[i] #both have valid values. } else if (!is.na(x[i]) &amp; is.na(y[i])) { output[i] &lt;- x[i] #only x has valid value } else if (is.na(x[i]) &amp; !is.na(y[i])) { output[i] &lt;- y[i] #only y has valid value } else { output[i] &lt;- NA #no valid values } } Useful links: - r4ds - Advanced R (Just read the complete chapters please.) A.13.3 Lists A very useful object in R. Please try to use them asap. Useful links: - The basics: Quick R - All you need to know (just read the complete chapter please): r4ds - Advanced (just read the complete chapter please): Advanced R A.13.4 lapply/map and do.call Think of the desired output. Should it be a list or a vector? lapply: ?lapply do.call: ?do.call r4ds Advanced R A.13.5 functions Some personal opinions: start the name of your new function with an f and use snake_case: fnew_name Always use return: fnew_name(arg1) {output &lt;- arg1 + 1; return(output)} Use comments either above the line or at the same line: fnew_name(arg1) {#what am I doing in the next line; output &lt;- arg + 1; return(output) #what do I do in this line} return an informative error if something goes wrong: if (length(arg1)!=length(arg2)) stop(\"arguments have a different length\") Useful links: - The basics: Quick R - All you need to know: r4ds - Advanced: Advanced R Thank you for reading this tutorial!! A.14 Assignment Time to practice. Go back to your latest paper, assignment or theses in which you used a software package other than R to wrangle, describe, visualize and/or analyze the data. Translate your syntax into R. Be as consistent as possible with respect to using Base and Tidy. But that is way too difficult!! Okay, okay, well try to do exactly the same as in this tutorial (without copy pasting the code chunks) but on a different dataset. O, you are already a fluent R Base user? Well, translate your latest R Base script into Tidy. Not challenging enough? Start working with Rmarkdown and Git. Fork this repository. Make this tutorial better. Submit a pull request to me. Consider the following list: annoyinglist &lt;- list(v1=c(2,45,6, NA), v2=c(23536), v3=NA, v4=c(2346)) calculate the mean of all elements saved in the annoyinglist with a loop calculate the mean of each list-element (i.e. v1 v2 v3) with a loop Please do the two same things but know with using lapply, map and do.call Define your own function which does the two same things and just needs as input the list. References "],["variance.html", "B Interdependencies B.1 Variance and covariance B.2 Intraclass correlation", " B Interdependencies B.1 Variance and covariance Social scientists try to explain variance and covariance. It is therefore a good idea to learn by heart the formula for variance and covariance. The sample variance of a random continuous variable X, VAR(X), is as follows: \\[VAR(X)=s_{xx}^2 = s_x^2= \\frac{\\Sigma^n_{i=1}(X_i-\\overline{X})(X_i-\\overline{X})}{n-1}= \\frac{\\Sigma^n_{i=1}(X_i-\\overline{X})^2}{n-1}\\] The sample standard deviation is given by: \\[STD(X)=\\sqrt{s_x^2}=s_x\\] The sample covariance of two random continuous variables X and Y, COV(X,Y) is as follows: \\[COV(X,Y)=s_{xy}^2 = \\frac{\\Sigma^n_{i=1}(X_i-\\overline{X})(Y_i-\\overline{Y})}{n-1}\\] The sample correlation coefficient between two random continuous variables X and Y, COR(X,Y), is a covariance on standardized variables (\\(z_x=X_{sd}=(X-\\overline{X})/s_x\\)) and hence: \\[COR(X,Y)=r_{xy} = \\frac{s_{xy}^2}{s_x s_y}= \\frac{\\Sigma^n_i(X_i-\\overline{X})(Y_i-\\overline{Y})}{\\sqrt{\\Sigma^n_i(X_i-\\overline{X})^2}\\sqrt{\\Sigma^n_i(Y_i-\\overline{Y})^2}}\\] Just to be complete. The population equivalent of the covariance: \\[\\sigma _{xy}^2 = \\frac{\\Sigma^n_i(X_i - \\mu_x)(Y_i-\\mu_y)}{N},\\] with \\(\\mu\\) the population mean. And the correlation within the population is: \\[\\rho_{xy} = \\frac{\\sigma_{xy}^2}{\\sigma_x \\sigma_y}\\] B.1.1 Want to learn more?! I strongly recommend you to read the online book on probability by Pishro-Nik (2016)! B.2 Intraclass correlation B.2.1 Dyadic data Let us suppose we have dyadic data. For example on the political opinion of two marriage partners. We want to know if these data are interdependent. Run the code chunck below to simulate some data. require(MASS) set.seed(9864) #We set a seed. In this we the random numbers we will generate be the same and we thus end up with the same dataset. Please not that to be absolutely sure to get the same dataset, we need to run the same R version (and packages). # let us start with simulating the opinion of both partners. Sigma &lt;- matrix(c(10, 4, 4, 5), 2, 2) opinions &lt;- mvrnorm(n = 1000, mu = c(4, 5), Sigma) opinion_W &lt;- opinions[, 1] opinion_M &lt;- opinions[, 2] dyad_id &lt;- 1:1000 # and let&#39;s put everything together data &lt;- data.frame(dyad_id, opinion_W, opinion_M) # add some description to the data attr(data, &quot;description&quot;) &lt;- &quot;This is a simulated dataset to illustrate interdependencies of observations within dyads (i.e. heterosexual couples). The dataset is in wide-format: one row refers to one couple. Variables with \\&quot;_W\\&quot; refer to women,\\&quot;_M\\&quot; refer to men.&quot; # I don&#39;t think the variables need any further description. B.2.1.1 Describe data Lets have a look at our data. require(psych) head(data) str(data) summary(data) attr(data, &quot;description&quot;) describe(data) #&gt; dyad_id opinion_W opinion_M #&gt; 1 1 1.180285 3.651525 #&gt; 2 2 9.930618 7.117465 #&gt; 3 3 4.022491 2.205877 #&gt; 4 4 2.990720 7.485650 #&gt; 5 5 3.024059 8.292194 #&gt; 6 6 8.408048 4.720610 #&gt; &#39;data.frame&#39;: 1000 obs. of 3 variables: #&gt; $ dyad_id : int 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ opinion_W: num 1.18 9.93 4.02 2.99 3.02 ... #&gt; $ opinion_M: num 3.65 7.12 2.21 7.49 8.29 ... #&gt; - attr(*, &quot;description&quot;)= chr &quot;This is a simulated dataset to illustrate interdependencies of observations within dyads (i.e. heterosexual cou&quot;| __truncated__ #&gt; dyad_id opinion_W opinion_M #&gt; Min. : 1.0 Min. :-5.337 Min. :-2.992 #&gt; 1st Qu.: 250.8 1st Qu.: 2.141 1st Qu.: 3.517 #&gt; Median : 500.5 Median : 4.222 Median : 5.013 #&gt; Mean : 500.5 Mean : 4.201 Mean : 5.006 #&gt; 3rd Qu.: 750.2 3rd Qu.: 6.170 3rd Qu.: 6.545 #&gt; Max. :1000.0 Max. :14.476 Max. :11.670 #&gt; [1] &quot;This is a simulated dataset to illustrate interdependencies of observations within dyads (i.e. heterosexual couples). The dataset is in wide-format: one row refers to one couple. Variables with \\&quot;_W\\&quot; refer to women,\\&quot;_M\\&quot; refer to men.&quot; #&gt; vars n mean sd median trimmed mad min max range skew kurtosis se #&gt; dyad_id 1 1000 500.50 288.82 500.50 500.50 370.65 1.00 1000.00 999.00 0.00 -1.20 9.13 #&gt; opinion_W 2 1000 4.20 3.18 4.22 4.19 3.01 -5.34 14.48 19.81 0.05 0.01 0.10 #&gt; opinion_M 3 1000 5.01 2.27 5.01 5.02 2.25 -2.99 11.67 14.66 -0.06 -0.03 0.07 B.2.1.2 Interdependencies: correlation There are different (naive and less naive) ways to check for interdependence. For more background information see this page by David A. Kenny. Also check out paragraph 3.3 of the book by (T. A. Snijders and Bosker 1999). Lets us start with something that pops up in your mind immediatelya correlation. cov(data$opinion_M, data$opinion_W) #the covariance between the two variables. Have a look at the simulation. This is indeed what we have put into the data. #&gt; [1] 4.154203 cov(scale(data$opinion_M), scale(data$opinion_W)) #the covariance between the two standardized variables. That is the correlation. #&gt; [,1] #&gt; [1,] 0.5741921 cor.test(data$opinion_M, data$opinion_W) #See, same value. Now also with significance. #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: data$opinion_M and data$opinion_W #&gt; t = 22.156, df = 998, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.5311041 0.6143179 #&gt; sample estimates: #&gt; cor #&gt; 0.5741921 This would indicate a strong and significant correlation. Remember that our data is in wide format. A better way is to calculate the correlation on a long dataset. This method is called the double entry method. Why is this a better way? It takes into account that the variance and mean of the opinions of men and women may be different. The endresult will more closely resemble the ICC we will encounter later. var1 &lt;- c(data$opinion_M, data$opinion_W) var2 &lt;- c(data$opinion_W, data$opinion_M) cor.test(var1, var2) #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: var1 and var2 #&gt; t = 26.576, df = 1998, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.4779248 0.5427243 #&gt; sample estimates: #&gt; cor #&gt; 0.5110503 Lower, but still significant. It is even possible that to now see a negative (and significant) correlation. For example try to repeat the above with a dataset we would get after running the following simulation. require(MASS) set.seed(9864) #We set a seed. In this we the random numbers we will generate be the same and we thus end up with the same dataset. Please not that to be absolutely sure to get the same dataset, we need to run the same R version (and packages). # let us start with simulating the opinion of both partners. Sigma &lt;- matrix(c(10, 4, 4, 5), 2, 2) opinions &lt;- mvrnorm(n = 1000, mu = c(20, 25), Sigma) opinion_W &lt;- opinions[, 1] opinion_M &lt;- opinions[, 2] dyad_id &lt;- 1:1000 # and let&#39;s put everything together data &lt;- data.frame(dyad_id, opinion_W, opinion_M) # add some description to the data attr(data, &quot;description&quot;) &lt;- &quot;This is a simulated dataset to illustrate interdependencies of observations within dyads (i.e. heterosexual couples). The dataset is in wide-format: one row refers to one couple. Variables with \\&quot;_W\\&quot; refer to women,\\&quot;_M\\&quot; refer to men.&quot; # I don&#39;t think the variables need any further description. B.2.1.3 ICC The intraclass correlation is the correlation between two random subjects of the same cluster. There are many mathematical definitions of the ICC. Let us start with a definition from the ANOVA tradition: \\[ ICC = \\frac{(MS_B - MS_W)}{(MS_B + MS_W)} \\] where, \\[ MS_B = VAR(\\bar{X}_{dyad}) * 2 \\] and \\[ MS_W = \\sum(X_{ego} - X_{alter})^2 / (2* N_{dyads}) \\] Lets have a go! MSB &lt;- var((data$opinion_M + data$opinion_W)/2) * 2 MSW &lt;- (sum((data$opinion_M - data$opinion_W)^2))/(2 * length(data$opinion_W)) ICC_anova &lt;- (MSB - MSW)/(MSB + MSW) ICC_anova #&gt; [1] 0.5114198 Do you see that the ICC is very close to the correlation based on a dataset in long format (double entry method)? Thus in practice, the double entry method is very convenient to check for interdependencies if you are working on dyadic data. Most of you are probably more familiar with definitions of the ICC as provided within textbooks on multi-level analysis. Where the intraclass correlation - at least for continuous dependent variables - is defined as the between variance (i.e. the variance in dyad means) divided by the total variance (i.e. the sum of the between and within variance). There is only one problem, we need these variances present in the real population. In our data we only observe the variances present in our sample. The observed between variance needs to be corrected. Below I will show you how to do that. First make a dataset in longformat. # first make a dataset in longformat. dyadmean &lt;- (data$opinion_M + data$opinion_W)/2 data_long &lt;- rbind(data, data) data_long$partner_id &lt;- rep(1:2, each = 1000) data_long$dyad_id &lt;- rep(1:1000, times = 2) data_long$dyadmean &lt;- c(dyadmean, dyadmean) # lets the first dyad entry refer to the women and the second to the men data_long$opinion &lt;- ifelse(data_long$partner_id == 1, data_long$opinion_W, data_long$opinion_M) # also define the opinion of the partner data_long$opinion_P &lt;- ifelse(data_long$partner_id == 2, data_long$opinion_W, data_long$opinion_M) head(data_long) #&gt; dyad_id opinion_W opinion_M partner_id dyadmean opinion opinion_P #&gt; 1 1 1.180285 3.651525 1 2.415905 1.180285 3.651525 #&gt; 2 2 9.930618 7.117465 1 8.524041 9.930618 7.117465 #&gt; 3 3 4.022491 2.205877 1 3.114184 4.022491 2.205877 #&gt; 4 4 2.990720 7.485650 1 5.238185 2.990720 7.485650 #&gt; 5 5 3.024059 8.292194 1 5.658127 3.024059 8.292194 #&gt; 6 6 8.408048 4.720610 1 6.564329 8.408048 4.720610 With this dataset in longformat we can calculate the ICC. # first calculate the between variance of our sample. Note that this we only need observations of # the dyads once (thus N_dyads=1000) S_B &lt;- var(data_long$dyadmean[1:1000]) # within variance SW &lt;- sum((data_long$opinion - data_long$dyadmean)^2)/1000 #we divide by the number of dyads # We now need to correct the observed between variance to reflect the population between variance. S_B_E &lt;- S_B - SW/2 ICC_ML &lt;- S_B_E/(S_B_E + SW) ICC_ML #&gt; [1] 0.5114198 Of course exactly similar to the ICC_anova. But this procedure is of course quite cumbersome. It may be a lot easier to estimate an empty multi-level model which also spits out the ICC (after some tweaking). See below. require(nlme) # estimate empty model with ML mlme &lt;- lme(opinion ~ 1, data = data_long, random = list(~1 | dyad_id), ) summary(mlme) # Standard deviations are reported instead of variances. extract the variances. VarCorr(mlme) # the intercept variance is at the between-level. the residual variances are at the observation / # within-level. thus based on these numbers we may calculate the ICC ourselves. varests &lt;- as.numeric(VarCorr(mlme)[1:2]) varests ICC_MLb &lt;- varests[1]/sum(varests) ICC_MLb #&gt; Linear mixed-effects model fit by REML #&gt; Data: data_long #&gt; AIC BIC logLik #&gt; 9491.554 9508.355 -4742.777 #&gt; #&gt; Random effects: #&gt; Formula: ~1 | dyad_id #&gt; (Intercept) Residual #&gt; StdDev: 1.998493 1.953361 #&gt; #&gt; Fixed effects: opinion ~ 1 #&gt; Value Std.Error DF t-value p-value #&gt; (Intercept) 4.603242 0.07682307 1000 59.92004 0 #&gt; #&gt; Standardized Within-Group Residuals: #&gt; Min Q1 Med Q3 Max #&gt; -3.28830881 -0.51102249 0.01645221 0.57161594 2.75065667 #&gt; #&gt; Number of Observations: 2000 #&gt; Number of Groups: 1000 #&gt; dyad_id = pdLogChol(1) #&gt; Variance StdDev #&gt; (Intercept) 3.993974 1.998493 #&gt; Residual 3.815620 1.953361 #&gt; [1] 3.993974 3.815620 #&gt; [1] 0.5114189 In this course we will rely heavily on the Lavaan package. We can also calculate the ICC with Lavaan. require(&quot;lavaan&quot;) model &lt;- &quot; level: 1 opinion ~ 1 #regression model opinion ~~ opinion #variance level: 2 opinion ~ 1 opinion ~~ opinion &quot; fit &lt;- lavaan(model = model, data = data_long, cluster = &quot;dyad_id&quot;) summary(fit) #&gt; lavaan 0.6-9 ended normally after 7 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 3 #&gt; #&gt; Number of observations 2000 #&gt; Number of clusters [dyad_id] 1000 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 0.000 #&gt; Degrees of freedom 0 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; #&gt; Level 1 [within]: #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; opinion 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; opinion 3.816 0.171 22.361 0.000 #&gt; #&gt; #&gt; Level 2 [dyad_id]: #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; opinion 4.603 0.077 59.950 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; opinion 3.988 0.277 14.391 0.000 lavInspect(fit, &quot;icc&quot;) #&gt; opinion #&gt; 0.511 The take home message is that the two observations for each dyad are indeed interrelated. Is this a lot? Is this significant? B.2.2 Egonet / Socionet data The above procedure to calculate the ICC correlation can also be used for egonet data. Let us suppose we have egonet data. For example on the political opinions of you and your friends. We want to know if these data are interdependent. Run the code chunck below to simulate some data. require(MASS) set.seed(9864) #We set a seed. In this we the random numbers we will generate be the same and we thus end up with the same dataset. Please not that to be absolutely sure to get the same dataset, we need to run the same R version (and packages). # let us start with simulating the opinion of ego and its alters. Sigma &lt;- matrix(sample(c(1, 2, 3), 36, replace = T), 6, 6) Sigma[lower.tri(Sigma)] = t(Sigma)[lower.tri(Sigma)] diag(Sigma) &lt;- c(5, 4, 6, 3, 7, 6) # Sigma opinions &lt;- mvrnorm(n = 1000, mu = c(4, 4, 4, 4, 4, 4), Sigma) opinion_ego &lt;- opinions[, 1] opinion_alter1 &lt;- opinions[, 2] opinion_alter2 &lt;- opinions[, 3] opinion_alter3 &lt;- opinions[, 4] opinion_alter4 &lt;- opinions[, 5] opinion_alter5 &lt;- opinions[, 6] egonet_id &lt;- 1:1000 # and let&#39;s put everything together data &lt;- data.frame(egonet_id, opinion_alter1, opinion_alter2, opinion_alter3, opinion_alter4, opinion_alter5) # I don&#39;t think the variables need any further description. B.2.2.1 Describe data Lets have a look at our data. require(psych) head(data) str(data) summary(data) attr(data, &quot;description&quot;) describe(data) #&gt; egonet_id opinion_alter1 opinion_alter2 opinion_alter3 opinion_alter4 opinion_alter5 #&gt; 1 1 -0.1024247 2.473552 2.993974 0.1604888 1.7443645 #&gt; 2 2 6.4712148 2.689709 6.052834 5.7392350 7.1487502 #&gt; 3 3 4.0474525 1.715663 2.469049 2.7659637 0.8816233 #&gt; 4 4 1.4692597 4.354919 4.253400 0.3809896 4.0698368 #&gt; 5 5 1.8042561 3.262400 4.754952 0.4194922 4.5527566 #&gt; 6 6 5.4410875 6.103960 3.432771 3.7275621 2.0488686 #&gt; &#39;data.frame&#39;: 1000 obs. of 6 variables: #&gt; $ egonet_id : int 1 2 3 4 5 6 7 8 9 10 ... #&gt; $ opinion_alter1: num -0.102 6.471 4.047 1.469 1.804 ... #&gt; $ opinion_alter2: num 2.47 2.69 1.72 4.35 3.26 ... #&gt; $ opinion_alter3: num 2.99 6.05 2.47 4.25 4.75 ... #&gt; $ opinion_alter4: num 0.16 5.739 2.766 0.381 0.419 ... #&gt; $ opinion_alter5: num 1.744 7.149 0.882 4.07 4.553 ... #&gt; egonet_id opinion_alter1 opinion_alter2 opinion_alter3 opinion_alter4 #&gt; Min. : 1.0 Min. :-1.855 Min. :-5.919 Min. :-1.443 Min. :-4.722 #&gt; 1st Qu.: 250.8 1st Qu.: 2.713 1st Qu.: 2.515 1st Qu.: 3.014 1st Qu.: 2.179 #&gt; Median : 500.5 Median : 4.027 Median : 4.146 Median : 4.132 Median : 4.014 #&gt; Mean : 500.5 Mean : 4.058 Mean : 4.118 Mean : 4.131 Mean : 3.943 #&gt; 3rd Qu.: 750.2 3rd Qu.: 5.426 3rd Qu.: 5.765 3rd Qu.: 5.318 3rd Qu.: 5.732 #&gt; Max. :1000.0 Max. :10.244 Max. :12.382 Max. :10.869 Max. :11.874 #&gt; opinion_alter5 #&gt; Min. :-3.280 #&gt; 1st Qu.: 2.528 #&gt; Median : 4.044 #&gt; Mean : 4.100 #&gt; 3rd Qu.: 5.764 #&gt; Max. :13.241 #&gt; NULL #&gt; vars n mean sd median trimmed mad min max range skew kurtosis #&gt; egonet_id 1 1000 500.50 288.82 500.50 500.50 370.65 1.00 1000.00 999.00 0.00 -1.20 #&gt; opinion_alter1 2 1000 4.06 1.98 4.03 4.06 2.00 -1.86 10.24 12.10 0.03 -0.21 #&gt; opinion_alter2 3 1000 4.12 2.46 4.15 4.11 2.41 -5.92 12.38 18.30 -0.03 0.26 #&gt; opinion_alter3 4 1000 4.13 1.75 4.13 4.14 1.71 -1.44 10.87 12.31 -0.04 0.26 #&gt; opinion_alter4 5 1000 3.94 2.64 4.01 3.97 2.66 -4.72 11.87 16.60 -0.09 -0.09 #&gt; opinion_alter5 6 1000 4.10 2.44 4.04 4.09 2.43 -3.28 13.24 16.52 0.03 0.00 #&gt; se #&gt; egonet_id 9.13 #&gt; opinion_alter1 0.06 #&gt; opinion_alter2 0.08 #&gt; opinion_alter3 0.06 #&gt; opinion_alter4 0.08 #&gt; opinion_alter5 0.08 Reshape into long format. require(&quot;tidyverse&quot;) data_long &lt;- tidyr::pivot_longer(data = data, cols = everything()[-1], names_to = &quot;alter&quot;, values_to = &quot;opinion&quot;) head(data_long) #&gt; # A tibble: 6 x 3 #&gt; egonet_id alter opinion #&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 opinion_alter1 -0.102 #&gt; 2 1 opinion_alter2 2.47 #&gt; 3 1 opinion_alter3 2.99 #&gt; 4 1 opinion_alter4 0.160 #&gt; 5 1 opinion_alter5 1.74 #&gt; 6 2 opinion_alter1 6.47 B.2.2.2 ICC via ML require(nlme) # estimate empty model with ML mlme &lt;- lme(opinion ~ 1, data = data_long, random = list(~1 | egonet_id), ) summary(mlme) # Standard deviations are reported instead of variances. extract the variances. VarCorr(mlme) # the intercept variance is at the between-level. the residual variances are at the observation / # within-level. thus based on these numbers we may calculate the ICC ourselves. varests &lt;- as.numeric(VarCorr(mlme)[1:2]) varests ICC_MLb &lt;- varests[1]/sum(varests) ICC_MLb #&gt; Linear mixed-effects model fit by REML #&gt; Data: data_long #&gt; AIC BIC logLik #&gt; 21837.85 21857.4 -10915.93 #&gt; #&gt; Random effects: #&gt; Formula: ~1 | egonet_id #&gt; (Intercept) Residual #&gt; StdDev: 1.22191 1.921997 #&gt; #&gt; Fixed effects: opinion ~ 1 #&gt; Value Std.Error DF t-value p-value #&gt; (Intercept) 4.069793 0.04724277 4000 86.14638 0 #&gt; #&gt; Standardized Within-Group Residuals: #&gt; Min Q1 Med Q3 Max #&gt; -3.77787873 -0.59863578 0.01410604 0.59669184 4.14533775 #&gt; #&gt; Number of Observations: 5000 #&gt; Number of Groups: 1000 #&gt; egonet_id = pdLogChol(1) #&gt; Variance StdDev #&gt; (Intercept) 1.493065 1.221910 #&gt; Residual 3.694073 1.921997 #&gt; [1] 1.493065 3.694073 #&gt; [1] 0.2878398 B.2.2.3 ICC via SEM/lavaan require(&quot;lavaan&quot;) model &lt;- &quot; level: 1 opinion ~ 1 #regression model opinion ~~ opinion #variance level: 2 opinion ~ 1 opinion ~~ opinion &quot; fit &lt;- lavaan(model = model, data = data_long, cluster = &quot;egonet_id&quot;) summary(fit) #&gt; lavaan 0.6-9 ended normally after 11 iterations #&gt; #&gt; Estimator ML #&gt; Optimization method NLMINB #&gt; Number of model parameters 3 #&gt; #&gt; Number of observations 5000 #&gt; Number of clusters [egonet_id] 1000 #&gt; #&gt; Model Test User Model: #&gt; #&gt; Test statistic 0.000 #&gt; Degrees of freedom 0 #&gt; #&gt; Parameter Estimates: #&gt; #&gt; Standard errors Standard #&gt; Information Observed #&gt; Observed information based on Hessian #&gt; #&gt; #&gt; Level 1 [within]: #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; opinion 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; opinion 3.694 0.083 44.721 0.000 #&gt; #&gt; #&gt; Level 2 [egonet_id]: #&gt; #&gt; Intercepts: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; opinion 4.070 0.047 86.190 0.000 #&gt; #&gt; Variances: #&gt; Estimate Std.Err z-value P(&gt;|z|) #&gt; opinion 1.491 0.101 14.750 0.000 lavInspect(fit, &quot;icc&quot;) #&gt; opinion #&gt; 0.288 References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
