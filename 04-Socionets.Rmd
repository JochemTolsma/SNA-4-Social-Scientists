# (PART) Part 3 SOCIONETS  {-} 


# Theory  {#socionetst}


<style>

.button1 {
  background-color: #f44336; /* Red */ 
  border: none;
  color: white;
  padding: 15px 32px;
  text-align: center;
  text-decoration: none;
  display: inline-block;
  font-size: 16px;
  margin: 4px 2px;
  cursor: pointer;
}

.button1:hover {
  box-shadow: 0 12px 16px 0 rgba(0,0,0,0.24), 0 17px 50px 0 rgba(0,0,0,0.19);
}

.button1 {border-radius: 12px;}

.button1 {width: 100%;}
</style>


<script>
function myFunction() {

            var btn = document.getElementById("myButton");
            //to make it fancier
            if (btn.value == "Click to hide code") {
                btn.value = "Click to show code";
                btn.innerHTML = "Click to show code";
            }
            else {
                btn.value = "Click to hide code";
                btn.innerHTML = "Click to hide code";
            }
            //this is what you're looking for
            var x = document.getElementById("myDIV");
            if (x.style.display === "none") {
                x.style.display = "block";
            } else {
                x.style.display = "none";
            }
        }

</script>




```{r, echo=FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=100),tidy=TRUE, warning = FALSE, message = FALSE,comment = "#>" , attr.source = ".numberLines", cache=TRUE) 
options(width = 100)

```

```{r colorize, echo=FALSE}
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

```{r emotions, echo=FALSE, message=FALSE}
# install.packages("devtools")
#devtools::install_github("hadley/emo")
require(emo)
require(igraph)
#emo::ji_find("affection")
#emo::ji("notes")
#jis
```

```{r, echo=FALSE}
#https://bookdown.org/markhoff/social_network_analysis/bridges-holes-the-small-world-problem-and-simulation.html
simulate_caveman <- function(n = 25, clique_size = 5){
  require(igraph)
  # Groups are all the same size, so I check whether N is divisible by the size of groups
  if ( ((n%/%clique_size) * clique_size) != n){
    stop("n is not evenly divisible by clique_size")
  }
  
  groups = n/clique_size # this determines the number of groups
  
  el <- data.frame(PersonA = 1:n, Group = NA) # I create a dataframe which has people and the groups they are in
  # I treat it like a person to group edgelist
  
  group_vector = c()
  for (i in 1:groups){
    group_vector <- c(group_vector, rep(i, clique_size))
  }  

  el$Group <- group_vector
  
  inc <- table(el) # I use the table function to turn the person to group edgelist into an incidence matrix
  adj <- inc %*% t(inc) # And I use matrix multiplication with the transpose to turn the person to group incidence matrix
  # into a person to person adjacency matrix
  
  diag(adj) <- 0 
  
  g <- graph.adjacency(adj, mode = "undirected") # I graph this matrix

  group_connect <- seq(from = 1, to = n, by = clique_size) # I determine the points of connection using a sequence funciton
  
  for( i in 1:(length(group_connect)-1)){
    p1 <- group_connect[i] + 1
    p2 <- group_connect[i+1]
    g <- igraph::add.edges(g, c(p1,p2)) # And I connect the points of connection using add.edges
  }
    g <- igraph::add.edges(g, c(group_connect[1],(group_connect[groups]+1))) # finally I connect the ends of the structure so that it forms a circle

    return(g)    
}

set.seed(43635)
caveman_net <- simulate_caveman(n = 105, clique_size = 15) 

#par(mar = c(2,2,2,2))
#plot(caveman_net, layout = layout.kamada.kawai(caveman_net), vertex.size = 2, vertex.label = NA, vertex.color = "grey80")
caveman_net_rewired <-  rewire(caveman_net, keeping_degseq(niter = 180))

E(caveman_net_rewired)$color <- "grey80"
V(caveman_net_rewired)$color <- "red"
#layout = layout.kamada.kawai(caveman_net)
#plot.igraph(caveman_net_rewired, vertex.size = 4, vertex.label=NA)

#graph.density(caveman_net_rewired)

smallworld <- caveman_net_rewired
```



A complete, full, or sociocentric network is a network within a sampled context or foci of which we know all nodes and all connections between nodes.^[With 'we' I mean the observers, or the researchers. The social agents do not need to be aware or know all nodes and ties.] The boundaries of the network are thus a priori defined and the contexts in which nodes are present are the sampled units. 
We may for example sample a classroom, neighborhood, university or country and collect all relations between all nodes within this context. 


## Network structures 

We already discussed the following network structures: 

- (in/out) degree or density: \@ref(density)  <!---fix label--->
- reciprocity   \@ref(fig:dd)
- dyad configurations \@ref(network-structures-dyad) 
- triad configurations  \@ref(triad)
- degree centrality  \@ref(degreecentrality)
- closeness centrality \@ref(closenesscentrality)
- betweenness centrality \@ref(betweennesscentrality)  
- clustering / transitivity \@ref(clustering)

It became clear that these networks structures could be calculated for egonets but that these structures have analogous at the complete-network-level. 

Below we will discuss different network structures that are (more) unique for complete networks. 

### Path length

Average shortest path length is defined as the average number of steps along the shortest paths for all possible pairs of network nodes. Thus the length of a path is the number of edges the path contains. Some pairs of nodes (dyads) may not be connected at all. How to deal with those? Well we could calculate the average path length of all connected nodes. See for example the network below in Figure \@ref(fig:pl). 

 
```{r pl, fig.cap="small random graph", echo=FALSE}
set.seed(34575)
random_graph <- erdos.renyi.game(n = 4, p.or.m = .1, directed = TRUE) 
plot(random_graph)
```
This is a directed graph, thus node 3 is connected to node 4 (path length 2) but 4 is not connected to 3. What is the average path length? ...  
It is: `r round(igraph::average.path.length(random_graph),2)`. 

The average path length in Smallworld is very low considering the size (`r igraph::vcount(smallworld)`)  and density (`r round(igraph::edge_density(caveman_net_rewired),2)`), it is: `r round(igraph::average.path.length(caveman_net_rewired),2)`. 
Since path length excludes disconnected nodes, it does not necessarily tells us something about the 'degrees of separation'. Let us instead calculate for each path length the proportion of nodes each node can reach and take the average value of that. 

To do that, we will make use of the function `ego_size` in the `igraph` package. See code chunk below for an example. 


```{r ex}
mean((ego_size(random_graph, order=2, mode = "out") - 1 )/ vcount(random_graph))

```

Thus for the small random graph above, Figure \@ref(fig:pl), if we take a path length of two we find that nodes 1 to 4 node can reach `r (igraph::ego_size(random_graph, order=2, mode = "out") - 1 )/ igraph::vcount(random_graph)` of all other nodes, respectively. This makes for an average reach of `r mean((igraph::ego_size(random_graph, order=2, mode = "out") - 1 )/ igraph::vcount(random_graph))`.  

For Smallworld we find the following:  

- path length one: `r round(mean((igraph::ego_size(smallworld, order=1, mode = "out") - 1 )/ igraph::vcount(smallworld)),2)`%.  
- path length two: `r round(mean((igraph::ego_size(smallworld, order=2, mode = "out") - 1 )/ igraph::vcount(smallworld)),2)`%.  
- path length three: `r round(mean((igraph::ego_size(smallworld, order=3, mode = "out") - 1 )/ igraph::vcount(smallworld)),2)`%.  
 
This brings us to the six-degrees-of-separation phenomenon. This is the observation that for real societies and real worlds 100% of the population would be connected to 100% of the population via 6 other persons (making for a path length of seven). Phrased otherwise, with path length seven, the average reach would be 100%. 

The idea that all people are connected through just six degrees of separation is based on Stanley Milgram's famous 'The small world problem' paper [@milgram1967small]. Luckily, you don't need to read the paper. Just watch the movie: 

<iframe width="560" height="315" src="https://www.youtube.com/embed/JWr5hBNWBOI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

### cliques and communities  

**to do**

### Segregation  

**to do**

#### inter- and intra-group density {-}

**to do**

#### Coleman {-}

**to do**

#### Moran's I {-}

Our inter-/intra group density and Coleman's homophily measures describe the extent to which similar people are more likely to be connected. We now take a slightly different angle. We want to know if nodes who are closer to one another in the network are more a like. To which my students respond in unison with:  

<p class= "quote">
 "Hey, that sounds like some sort of correlation!" 
</p> 

 Yup, we need some spatial autocorrelation measure. Let us use Moran's I for this.    
We will start with a calculation of the correlation between the score of actor *i* and the (mean) score of the alters of *i* to whom *i* is connected directly.

Spatial autocorrelation measures are actually quite complex. A lot of build in functions in different packages of R are not very clear on all the defaults. With respect to Moran's I, its values are actually quite difficult to compare across different spatial/network settings. Results may depend heavily on whether or not you demean your variables of interest, the chosen neighborhood/weight matrix (and hence on distance decay functions and type of standardization of the weight matrix). Anyways, lets get started. 

Moran's I is given by: 

$$I= \gamma \Sigma_i\Sigma_jw_{ij}z_iz_j,$$  


where $w_{ij}$ is the weight matrix $z_i$ and $z_j$ are the scores in deviations from the mean. And, 

$$\gamma= S_0 * m_2 = \Sigma_i\Sigma_jw_{ij} * \frac{\Sigma_iz_i^2}{n},$$  

with: $S_0 = \Sigma_i\Sigma_jw_{ij}$ and  $m_2 =  \frac{\Sigma_iz_i^2}{n}$.

Thus $S_0$ is the sum of the weight matrix and $m_2$ is an estimate of the variance. For more information see [Anselin 1995](https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1538-4632.1995.tb00338.x).  


We need two packages, if we not want to define all functions ourselves: `sna` and `ape`.[^1]

[^1]: I quite frequently need to calculate Moran's I and related statistics in my work/hobby. I commonly use the functions in the R package `spdep`. 

Let us demonstrate the concept on the build-in dataset of RSiena, [s50](http://www.stats.ox.ac.uk/~snijders/siena/s50_data.htm). See also `?s50`. For now, we only need to know this data set offers information on friendship ties and alcohol consumption among pupils in a school. 

We want to answer the following question: 

**Are pupils who are close to each other in the network more likely to exhibit similar drinking behavior?** 

Or phrased otherwise:  

**Is the network segregated based on drinking behavior?**



First use `sna`. 
Of course we need to define 'close'. Let us give alters a weight of 1 if they are part of the 1.0 degree egocentric network and 0 otherwise. 

```{r}
require(RSiena)
library(network)
    friend.data.w1 <- s501
    friend.data.w2 <- s502
    friend.data.w3 <- s503
    drink <- s50a
    smoke <- s50s

    net1 <- network::as.network(friend.data.w1)
    net2 <- network::as.network(friend.data.w2)
    net3 <- network::as.network(friend.data.w3)

#nacf does not row standardize!    
snam1<- sna::nacf(net1, drink[, 1], type="moran", neighborhood.type='out', demean = TRUE)
snam1[2] #the first order matrix is stored in second list-element

```

Lets calculate the same thing with `ape`. 
```{r}
require(ape)
require(sna)
geodistances <- geodist(net1, count.paths = TRUE)
geodistances <- geodistances$gdist

#first define a nb based on distance 1. 
weights1 <- geodistances==1

#this function rowstandardizes by default
ape::Moran.I(drink[,1], scaled=FALSE, weight=weights1, na.rm=TRUE)
```
Close but not the same value!

Lets use 'my own' function and don't rowstandardize. 

```{r fMoran.I}
fMoran.I <- function (x, weight, scaled = FALSE, na.rm = FALSE, alternative = "two.sided", rowstandardize = TRUE) 
{
 if (rowstandardize) {
    if (dim(weight)[1] != dim(weight)[2]) 
        stop("'weight' must be a square matrix")
    n <- length(x)
    if (dim(weight)[1] != n) 
        stop("'weight' must have as many rows as observations in 'x'")
    ei <- -1/(n - 1)
    nas <- is.na(x)
    if (any(nas)) {
        if (na.rm) {
            x <- x[!nas]
            n <- length(x)
            weight <- weight[!nas, !nas]
        }
        else {
            warning("'x' has missing values: maybe you wanted to set na.rm = TRUE?")
            return(list(observed = NA, expected = ei, sd = NA, 
                p.value = NA))
        }
    }
    ROWSUM <- rowSums(weight)
    ROWSUM[ROWSUM == 0] <- 1
    weight <- weight/ROWSUM
    s <- sum(weight)
    m <- mean(x)
    y <- x - m
    cv <- sum(weight * y %o% y)
    v <- sum(y^2)
    obs <- (n/s) * (cv/v)
    if (scaled) {
        i.max <- (n/s) * (sd(rowSums(weight) * y)/sqrt(v/(n - 
            1)))
        obs <- obs/i.max
    }
    S1 <- 0.5 * sum((weight + t(weight))^2)
    S2 <- sum((apply(weight, 1, sum) + apply(weight, 2, sum))^2)
    s.sq <- s^2
    k <- (sum(y^4)/n)/(v/n)^2
    sdi <- sqrt((n * ((n^2 - 3 * n + 3) * S1 - n * S2 + 3 * s.sq) - 
        k * (n * (n - 1) * S1 - 2 * n * S2 + 6 * s.sq))/((n - 
        1) * (n - 2) * (n - 3) * s.sq) - 1/((n - 1)^2))
    alternative <- match.arg(alternative, c("two.sided", 
        "less", "greater"))
    pv <- pnorm(obs, mean = ei, sd = sdi)
    if (alternative == "two.sided") 
        pv <- if (obs <= ei) 
            2 * pv
        else 2 * (1 - pv)
    if (alternative == "greater") 
        pv <- 1 - pv
    list(observed = obs, expected = ei, sd = sdi, p.value = pv)
 } else {
    if (dim(weight)[1] != dim(weight)[2]) 
        stop("'weight' must be a square matrix")
    n <- length(x)
    if (dim(weight)[1] != n) 
        stop("'weight' must have as many rows as observations in 'x'")
    ei <- -1/(n - 1)
    nas <- is.na(x)
    if (any(nas)) {
        if (na.rm) {
            x <- x[!nas]
            n <- length(x)
            weight <- weight[!nas, !nas]
        }
        else {
            warning("'x' has missing values: maybe you wanted to set na.rm = TRUE?")
            return(list(observed = NA, expected = ei, sd = NA, 
                p.value = NA))
        }
    }
    #ROWSUM <- rowSums(weight)
    #ROWSUM[ROWSUM == 0] <- 1
    #weight <- weight/ROWSUM
    s <- sum(weight)
    m <- mean(x)
    y <- x - m
    cv <- sum(weight * y %o% y)
    v <- sum(y^2)
    obs <- (n/s) * (cv/v)
    if (scaled) {
        i.max <- (n/s) * (sd(rowSums(weight) * y)/sqrt(v/(n - 
            1)))
        obs <- obs/i.max
    }
    S1 <- 0.5 * sum((weight + t(weight))^2)
    S2 <- sum((apply(weight, 1, sum) + apply(weight, 2, sum))^2)
    s.sq <- s^2
    k <- (sum(y^4)/n)/(v/n)^2
    sdi <- sqrt((n * ((n^2 - 3 * n + 3) * S1 - n * S2 + 3 * s.sq) - 
        k * (n * (n - 1) * S1 - 2 * n * S2 + 6 * s.sq))/((n - 
        1) * (n - 2) * (n - 3) * s.sq) - 1/((n - 1)^2))
    alternative <- match.arg(alternative, c("two.sided", 
        "less", "greater"))
    pv <- pnorm(obs, mean = ei, sd = sdi)
    if (alternative == "two.sided") 
        pv <- if (obs <= ei) 
            2 * pv
        else 2 * (1 - pv)
    if (alternative == "greater") 
        pv <- 1 - pv
    list(observed = obs, expected = ei, sd = sdi, p.value = pv)
 }
 
 
}
```


```{r}

fMoran.I(drink[,1], scaled=FALSE, weight=weights1, na.rm=TRUE, rowstandardize=FALSE)
```

Same result as `nacf` function!  

What does rowstandardization mean??  

- **rowstandardize**: We assume that each node *i* is influenced equally by its neighbourhood regardless on how large it. You could compare this to the average alter effect in RSiena)  
- **not rowstandardize**: We assume that each alter *j* has the same influence on *i* (if at the same distance). You could compare this to the total alter effect in RSiena.  

To not standardize is default in `sna::nacf`, to standardize is default in `apa::Moran.I`. This bugs me. I thus made a small adaption to `apa::Moran.I` and now in the function `fMoran.I` you can choose if you want to rowstandardize or not.

But...  

> `r emo::ji("musical_note")`   `r emo::ji("musical_note")`  
> "What you really, really want  
> I wanna, (ha) I wanna, (ha)  
> I wanna, (ha) I wanna, (ha)  
> I wanna really, really, really   
> Wanna zigazig ah"  
>                       `r emo::ji("musical_note")`  `r emo::ji("musical_note")`  
> --- Spice Girls - Wannabe

What I really would like to see is a correlation between actor *i* and all the alters to whom it is connected (direct or indirectly) and where alters at a larger distances (longer shortest path lengths) are weighted less. 

* step 1: for each acter *i* determine the distances (shortest path lengths) to all other nodes in the network.  
* step 2: based on these distances decide on how we want to weigh. That is, determine a distance decay function.  
* step 3: decide whether or not we want to row-standardize.  


```{r}
#step 1: calculate distances
geodistances <- geodist(net1, count.paths = TRUE)
geodistances <- geodistances$gdist
#set the distance to yourself as Inf
diag(geodistances) <- Inf


#step 2: define a distance decay function. This one is pretty standard in the spatial autocorrelation literature but actually pretty arbitrary. 
weights2 <- exp(-geodistances)

#step 3: I dont want to rowstandardize. 
fMoran.I(drink[,1], scaled=FALSE, weight=weights2, na.rm=TRUE, rowstandardize = FALSE)

``` 
\
Conclusion: Yes pupils closer to one another are more a like! And 'closer' here means a shorter shortest path length. You also observe that the correlation is lower than we calculated previously. Apparently, we are a like to alters very close by (path length one) and less so (or even dissimilar) to alters further off.  

## Random graphs  

Now that we know how to count dyad and triad configurations, to calculate network properties and to determine the extent of segregation within networks, the follow-up question is: Is this a lot? Or even: Is this significant? Let try to tackle the latter question. What do we mean with significant? Probably something like: The chance, $p$, to observe our value for network characteristic (or statistic), $s^{net}$, is smaller than some arbitrary value, $\alpha$, would we have randomly picked a network from the general population of networks, $X$, to which our observed network, $x^o$ belongs.  
This leaves us with just two smaller problems. First, what is this population of networks to which our observed network belongs. Second, what is the distribution of values for network characteristic, $s^{net}$, in this population? 

Let us go back to Smallworld. We stated that in a small world network, despite having a low density and being relatively clustered, the relative average path length is small. What do we mean with relative? Well, in SNA it means that if we would make a random graph, the chance is very low (lower than say 5%) that this graph would have a higher degree of clustering and a shorter average path length. 

Let us define the population of graphs to which Smallworld belongs as the graphs that have the same size and density.^[You could also set an additional filter and only compare graphs which also have the same degree distribution as the actual observed graph (Smallworld in the example).] In `igraph` you can generate random graphs with the `erdos.renyi.game` function.  
Let us make 1000 random graphs with size 105 (just as in Smallworld) and with a density of `r round(igraph::graph.density(smallworld),2)` (just as in Smallworld). And let us make a histogram of all observed average degree of clustering and path lengths. 

```{r rgraphs, fig.cap="Comparing network statistics of Smallworld with random graphs"}
require(igraph)
dens <- round(graph.density(smallworld),2) #save density of smallworld
trial <- 1000 #set number of trials/sims
trialclus <- triallen <- rep(NA, trial) #define objects in which you are saving results


for ( i in 1:trial ){
  random_graph <- erdos.renyi.game(n = 105, p.or.m = dens, directed = FALSE) #make the random graph
  triallen[i] <- average.path.length(random_graph, unconnected=TRUE) #calculate average path length
  trialclus[i] <- transitivity(random_graph, isolates = c("NaN")) #calculate clustering
}


par(mfrow=c(1,2))
{hist(trialclus, xlim=c(0.1,.36), main="average clustering coefficient", xlab="", )
abline(v=transitivity(smallworld, isolates = c("NaN")), col="red", lwd=3)}

{hist(triallen, xlim=c(1.9,2.2), main="average path length", xlab="")
abline(v=average.path.length(smallworld, unconnected=TRUE), col="red", lwd=3)}
 




```

The values of Smallworld are plotted in red. Smallworld indeed has a very high degree of clustering. Indeed, it is significantly higher than could be expected, because if falls outside more than 99 per cent of the cases of random graphs.  
However, we also see that the average path length of Smallworld is significantly higher compared to random graphs. 
Well, is, or is not, Smallworld a smallworld?  
A formal definition of smallworldlyness is:  

$$ \sigma = \frac{C/C_r}{L/L_r}, $$ 

where C is the average clustering of the observed graph and $C_ r$ is the clustering of the random graph, L is the average shortest path length of the observed graph and $L_r$ is the average shortest path length of the random graph. With $\sigma > 1$  the network is said to be a small-world network. The smallworldness of Smallworld is ...

...well, try to calculate this for yourself. 

<button class=button1 onclick="myFunction()" id="myButton" value="Click To Open Instructions">Only click button after 5 minutes!</button>

<div style="display:none;" id="myDIV">


```{r, results="hold"}
C <- transitivity(smallworld, isolates = c("NaN"))
Cr <- mean(trialclus)
L <- average.path.length(smallworld, unconnected=TRUE)
Lr <- mean(triallen)
Sigma <- (C/Cr) / (L/Lr)
print("The smallworldness of Smallworld is:")
Sigma
```

</div>

## Causes  

## Consequences  

---  


# Methods  {#socionetsm}

## Causes  

## Consequences  


---  

# Data  

## Sampling  

## Ethical considerations  

## Measurement 

---  





